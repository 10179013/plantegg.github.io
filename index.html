<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1">






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2117/06/07/文章索引index/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2117/06/07/文章索引index/" itemprop="url">文章索引</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2117-06-07T18:30:03+08:00">
                2117-06-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2117/06/07/文章索引index/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2117/06/07/文章索引index/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="文章索引"><a href="#文章索引" class="headerlink" title="文章索引"></a>文章索引</h1><h2 id="精华文章推荐"><a href="#精华文章推荐" class="headerlink" title="精华文章推荐"></a>精华文章推荐</h2><h4 id="《Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》-从一个参数引起的rt抖动定位到OS锁等待再到CPU-Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大"><a href="#《Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》-从一个参数引起的rt抖动定位到OS锁等待再到CPU-Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大" class="headerlink" title="《Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》 从一个参数引起的rt抖动定位到OS锁等待再到CPU Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大"></a><a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/" target="_blank" rel="noopener">《Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》 从一个参数引起的rt抖动定位到OS锁等待再到CPU Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大</a></h4><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/d567449fe52725a9d0b9d4ec9baa372c.png" alt="image.png"></p>
<h4 id="10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。"><a href="#10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。" class="headerlink" title="10倍性能提升全过程 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。"></a><a href="https://plantegg.github.io/2018/01/23/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/" target="_blank" rel="noopener">10倍性能提升全过程</a> 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一个性能全栈工程师如何发现各种问题的。</h4><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/05703c168e63e96821ea9f921d83712b.png" alt="image.png"></p>
<h4 id="就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET"><a href="#就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET" class="headerlink" title="就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET"></a><a href="https://plantegg.github.io/2017/06/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/" target="_blank" rel="noopener">就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET</a></h4><p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2020/png/33359/1579241362064-807d8378-6c54-4a2c-a888-ff2337df817c.png" alt="image.png" style="zoom:80%;"></p>
<h4 id="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的"><a href="#就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的" class="headerlink" title="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的"></a><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/" target="_blank" rel="noopener">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></h4><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/e177d59ecb886daef5905ed80a84dfd2.png" alt></p>
<h4 id="就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。-同时可以跟讲这块的RFC1180比较一下，RFC1180-写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90-的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用"><a href="#就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。-同时可以跟讲这块的RFC1180比较一下，RFC1180-写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90-的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用" class="headerlink" title="就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。  同时可以跟讲这块的RFC1180比较一下，RFC1180 写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用"></a><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/" target="_blank" rel="noopener">就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。</a>  同时可以跟讲这块的<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="noopener">RFC1180</a>比较一下，RFC1180 写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用</h4><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/8f5d8518c1d92ed68d23218028e3cd11.png" alt></p>
<h4 id="从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》"><a href="#从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》" class="headerlink" title="从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》"></a><a href="https://plantegg.github.io/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/" target="_blank" rel="noopener">从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》</a></h4><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/94d55b926b5bb1573c4cab8353428712.png" alt></p>
<h4 id="LVS-20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。"><a href="#LVS-20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。" class="headerlink" title="LVS 20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。"></a><a href="https://plantegg.github.io/2019/07/19/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%9D%87%E8%A1%A1/" target="_blank" rel="noopener">LVS 20倍的负载不均衡，原来是内核的这个Bug</a>，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。</h4><h4 id="就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理"><a href="#就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理" class="headerlink" title="就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理"></a><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%8F%A1%E6%89%8B/" target="_blank" rel="noopener">就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理</a></h4><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/6d66dadecb72e11e3e5ab765c6c3ea2e.png" alt></p>
<h4 id="nslookup-OK-but-ping-fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来"><a href="#nslookup-OK-but-ping-fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来" class="headerlink" title="nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来"></a><a href="https://plantegg.github.io/2019/01/09/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82ping--nslookup-OK-but-ping-fail/" target="_blank" rel="noopener">nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来</a></h4><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/ca466bb6430f1149958ceb41b9ffe591.png" alt></p>
<h4 id="如何在工作中学习-一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？"><a href="#如何在工作中学习-一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？" class="headerlink" title="如何在工作中学习 一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？"></a><a href="https://plantegg.github.io/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">如何在工作中学习</a> 一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？</h4><h3 id="举三反一–从理论知识到实际问题的推导-坚决不让思路跑偏，如何从一个理论知识点推断可能的问题"><a href="#举三反一–从理论知识到实际问题的推导-坚决不让思路跑偏，如何从一个理论知识点推断可能的问题" class="headerlink" title="举三反一–从理论知识到实际问题的推导 坚决不让思路跑偏，如何从一个理论知识点推断可能的问题"></a><a href="https://plantegg.github.io/2020/11/02/举三反一--从理论知识到实际问题的推导/" target="_blank" rel="noopener">举三反一–从理论知识到实际问题的推导</a> 坚决不让思路跑偏，如何从一个理论知识点推断可能的问题</h3><h2 id="性能相关"><a href="#性能相关" class="headerlink" title="性能相关"></a>性能相关</h2><h4 id="就是要你懂TCP–半连接队列和全连接队列-偶发性的连接reset异常、重启服务后短时间的连接异常"><a href="#就是要你懂TCP–半连接队列和全连接队列-偶发性的连接reset异常、重启服务后短时间的连接异常" class="headerlink" title="就是要你懂TCP–半连接队列和全连接队列  偶发性的连接reset异常、重启服务后短时间的连接异常"></a><a href="https://plantegg.github.io/2017/06/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/" target="_blank" rel="noopener">就是要你懂TCP–半连接队列和全连接队列</a>  偶发性的连接reset异常、重启服务后短时间的连接异常</h4><h4 id="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的-发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响"><a href="#就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的-发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响" class="headerlink" title="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的  发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响"></a><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/" target="_blank" rel="noopener">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a>  发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响</h4><h4 id="就是要你懂TCP–性能优化大全"><a href="#就是要你懂TCP–性能优化大全" class="headerlink" title="就是要你懂TCP–性能优化大全"></a><a href="https://plantegg.github.io/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8/" target="_blank" rel="noopener">就是要你懂TCP–性能优化大全</a></h4><h4 id="就是要你懂TCP–TCP性能问题-Nagle算法和delay-ack"><a href="#就是要你懂TCP–TCP性能问题-Nagle算法和delay-ack" class="headerlink" title="就是要你懂TCP–TCP性能问题 Nagle算法和delay ack"></a><a href="https://plantegg.github.io/2018/06/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%9C%80%E7%BB%8F%E5%85%B8%E7%9A%84TCP%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">就是要你懂TCP–TCP性能问题</a> Nagle算法和delay ack</h4><h4 id="10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。"><a href="#10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。" class="headerlink" title="10倍性能提升全过程 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。"></a><a href="https://plantegg.github.io/2018/01/23/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/" target="_blank" rel="noopener">10倍性能提升全过程</a> 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。</h4><h2 id="网络相关基础知识"><a href="#网络相关基础知识" class="headerlink" title="网络相关基础知识"></a>网络相关基础知识</h2><h4 id="就是要你懂网络–一个网络包的旅程"><a href="#就是要你懂网络–一个网络包的旅程" class="headerlink" title="就是要你懂网络–一个网络包的旅程"></a><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/" target="_blank" rel="noopener">就是要你懂网络–一个网络包的旅程</a></h4><h4 id="通过案例来理解MSS、MTU等相关TCP概念"><a href="#通过案例来理解MSS、MTU等相关TCP概念" class="headerlink" title="通过案例来理解MSS、MTU等相关TCP概念"></a><a href="https://plantegg.github.io/2018/05/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E9%80%9A%E8%BF%87%E6%A1%88%E4%BE%8B%E6%9D%A5%E5%AD%A6%E4%B9%A0MSS%E3%80%81MTU/" target="_blank" rel="noopener">通过案例来理解MSS、MTU等相关TCP概念</a></h4><h4 id="就是要你懂TCP–握手和挥手"><a href="#就是要你懂TCP–握手和挥手" class="headerlink" title="就是要你懂TCP–握手和挥手"></a><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%8F%A1%E6%89%8B/" target="_blank" rel="noopener">就是要你懂TCP–握手和挥手</a></h4><h4 id="wireshark-dup-ack-issue-and-keepalive"><a href="#wireshark-dup-ack-issue-and-keepalive" class="headerlink" title="wireshark-dup-ack-issue and keepalive"></a><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--wireshark-dup-ack-issue/" target="_blank" rel="noopener">wireshark-dup-ack-issue and keepalive</a></h4><h4 id="一个没有遵守tcp规则导致的问题"><a href="#一个没有遵守tcp规则导致的问题" class="headerlink" title="一个没有遵守tcp规则导致的问题"></a><a href="https://plantegg.github.io/2018/11/26/%E4%B8%80%E4%B8%AA%E6%B2%A1%E6%9C%89%E9%81%B5%E5%AE%88tcp%E8%A7%84%E5%88%99%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">一个没有遵守tcp规则导致的问题</a></h4><h4 id="kubernetes-service-和-kube-proxy详解"><a href="#kubernetes-service-和-kube-proxy详解" class="headerlink" title="kubernetes service 和 kube-proxy详解"></a><a href="https://plantegg.github.io/2020/09/22/kubernetes service 和 kube-proxy详解/" target="_blank" rel="noopener">kubernetes service 和 kube-proxy详解</a></h4><h2 id="DNS相关"><a href="#DNS相关" class="headerlink" title="DNS相关"></a>DNS相关</h2><h4 id="就是要你懂DNS–一文搞懂域名解析相关问题"><a href="#就是要你懂DNS–一文搞懂域名解析相关问题" class="headerlink" title="就是要你懂DNS–一文搞懂域名解析相关问题"></a><a href="https://plantegg.github.io/2019/06/09/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82DNS--%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">就是要你懂DNS–一文搞懂域名解析相关问题</a></h4><h4 id="nslookup-OK-but-ping-fail"><a href="#nslookup-OK-but-ping-fail" class="headerlink" title="nslookup OK but ping fail"></a><a href="https://plantegg.github.io/2019/01/09/nslookup-OK-but-ping-fail/" target="_blank" rel="noopener">nslookup OK but ping fail</a></h4><h4 id="Docker中的DNS解析过程"><a href="#Docker中的DNS解析过程" class="headerlink" title="Docker中的DNS解析过程"></a><a href="https://plantegg.github.io/2019/01/12/Docker%E4%B8%AD%E7%9A%84DNS%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B/" target="_blank" rel="noopener">Docker中的DNS解析过程</a></h4><h4 id="windows7的wifi总是报DNS域名异常无法上网"><a href="#windows7的wifi总是报DNS域名异常无法上网" class="headerlink" title="windows7的wifi总是报DNS域名异常无法上网"></a><a href="https://plantegg.github.io/2019/01/10/windows7%E7%9A%84wifi%E6%80%BB%E6%98%AF%E6%8A%A5DNS%E5%9F%9F%E5%90%8D%E5%BC%82%E5%B8%B8%E6%97%A0%E6%B3%95%E4%B8%8A%E7%BD%91/" target="_blank" rel="noopener">windows7的wifi总是报DNS域名异常无法上网</a></h4><h2 id="LVS-负载均衡"><a href="#LVS-负载均衡" class="headerlink" title="LVS 负载均衡"></a>LVS 负载均衡</h2><h4 id="就是要你懂负载均衡–lvs和转发模式"><a href="#就是要你懂负载均衡–lvs和转发模式" class="headerlink" title="就是要你懂负载均衡–lvs和转发模式"></a><a href="https://plantegg.github.io/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/" target="_blank" rel="noopener">就是要你懂负载均衡–lvs和转发模式</a></h4><h4 id="就是要你懂负载均衡–负载均衡调度算法和为什么不均衡"><a href="#就是要你懂负载均衡–负载均衡调度算法和为什么不均衡" class="headerlink" title="就是要你懂负载均衡–负载均衡调度算法和为什么不均衡"></a><a href="https://plantegg.github.io/2019/07/19/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%9D%87%E8%A1%A1/" target="_blank" rel="noopener">就是要你懂负载均衡–负载均衡调度算法和为什么不均衡</a></h4><h2 id="网络工具"><a href="#网络工具" class="headerlink" title="网络工具"></a>网络工具</h2><h4 id="就是要你懂Unix-Socket-进行抓包解析"><a href="#就是要你懂Unix-Socket-进行抓包解析" class="headerlink" title="就是要你懂Unix Socket 进行抓包解析"></a><a href="https://plantegg.github.io/2019/04/04/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--Unix-Socket%E6%8A%93%E5%8C%85/" target="_blank" rel="noopener">就是要你懂Unix Socket 进行抓包解析</a></h4><h4 id="就是要你懂网络监控–ss用法大全"><a href="#就是要你懂网络监控–ss用法大全" class="headerlink" title="就是要你懂网络监控–ss用法大全"></a><a href="https://plantegg.github.io/2019/10/12/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C%E7%9B%91%E6%8E%A7--ss%E7%94%A8%E6%B3%95%E5%A4%A7%E5%85%A8/" target="_blank" rel="noopener">就是要你懂网络监控–ss用法大全</a></h4><h4 id="就是要你懂抓包–WireShark之命令行版tshark"><a href="#就是要你懂抓包–WireShark之命令行版tshark" class="headerlink" title="就是要你懂抓包–WireShark之命令行版tshark"></a><a href="https://plantegg.github.io/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--WireShark%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88tshark/" target="_blank" rel="noopener">就是要你懂抓包–WireShark之命令行版tshark</a></h4><h4 id="netstat-timer-keepalive-explain"><a href="#netstat-timer-keepalive-explain" class="headerlink" title="netstat timer keepalive explain"></a><a href="https://plantegg.github.io/2017/08/28/netstat%20--timer/" target="_blank" rel="noopener">netstat timer keepalive explain</a></h4><h4 id="Git-HTTP-Proxy-and-SSH-Proxy"><a href="#Git-HTTP-Proxy-and-SSH-Proxy" class="headerlink" title="Git HTTP Proxy and SSH Proxy"></a><a href="https://plantegg.github.io/2018/03/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82git%E4%BB%A3%E7%90%86--%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEgit%20Proxy/" target="_blank" rel="noopener">Git HTTP Proxy and SSH Proxy</a></h4>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/MarkDown-技巧/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/MarkDown-技巧/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/MarkDown-技巧/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/MarkDown-技巧/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <font face="courier">我是黑体字abc</font>

<font face="courier-new">我是微软雅黑</font>

<font face="微软雅黑">我是微软雅黑</font>

<font face="STCAIYUN">我是华文彩云</font>

<font color="blue" size="16" face="黑体">color=blue size=72 face=”黑体”</font><br><font color="red" size="16">color=red</font>


<font color="gray" size="24">color=gray</font>

<center>居中<font color="gray" size="24">color=gray</font></center>

<p>Markdown本身不支持背景色设置，需要采用内置html的方式实现：借助 table, tr, td 等表格标签的 bgcolor 属性来实现背景色的功能。举例如下：</p>
<table><tr><td bgcolor="orange">背景色是：orange</td></tr></table>


<p>代码块：用2个以上TAB键起始的段落，会被认为是代码块（效果如下）：</p>
<pre><code>/* We always get an MSS option.  The option bytes which will be seen in
 * normal data packets should timestamps be used, must be in the MSS
 * advertised.  But we subtract them from tp-&gt;mss_cache so that
 * calculations in tcp_sendmsg are simpler etc.  So account for this
 * fact here if necessary.  If we don&apos;t do this correctly, as a
 * receiver we won&apos;t recognize data packets as being full sized when we
 * should, and thus we won&apos;t abide by the delayed ACK rules correctly.
 * SACKs don&apos;t matter, we never delay an ACK when we have any of those
 * going out.  */
opts-&gt;mss = tcp_advertise_mss(sk);
remaining -= TCPOLEN_MSS_ALIGNED;

if (likely(sock_net(sk)-&gt;ipv4.sysctl_tcp_timestamps &amp;&amp; !*md5)) {
        opts-&gt;options |= OPTION_TS;
        opts-&gt;tsval = tcp_skb_timestamp(skb) + tp-&gt;tsoffset;
        opts-&gt;tsecr = tp-&gt;rx_opt.ts_recent;
        remaining -= TCPOLEN_TSTAMP_ALIGNED;
}
</code></pre><p>颜色名列表<br>颜色名    十六进制颜色值    颜色<br>AliceBlue    #F0F8FF    rgb(240, 248, 255)<br>AntiqueWhite    #FAEBD7    rgb(250, 235, 215)<br>Aqua    #00FFFF    rgb(0, 255, 255)<br>Aquamarine    #7FFFD4    rgb(127, 255, 212)<br>Azure    #F0FFFF    rgb(240, 255, 255)<br>Beige    #F5F5DC    rgb(245, 245, 220)<br>Bisque    #FFE4C4    rgb(255, 228, 196)<br>Black    #000000    rgb(0, 0, 0)<br>BlanchedAlmond    #FFEBCD    rgb(255, 235, 205)<br>Blue    #0000FF    rgb(0, 0, 255)<br>BlueViolet    #8A2BE2    rgb(138, 43, 226)<br>Brown    #A52A2A    rgb(165, 42, 42)<br>BurlyWood    #DEB887    rgb(222, 184, 135)<br>CadetBlue    #5F9EA0    rgb(95, 158, 160)<br>Chartreuse    #7FFF00    rgb(127, 255, 0)<br>Chocolate    #D2691E    rgb(210, 105, 30)<br>Coral    #FF7F50    rgb(255, 127, 80)<br>CornflowerBlue    #6495ED    rgb(100, 149, 237)<br>Cornsilk    #FFF8DC    rgb(255, 248, 220)<br>Crimson    #DC143C    rgb(220, 20, 60)<br>Cyan    #00FFFF    rgb(0, 255, 255)<br>DarkBlue    #00008B    rgb(0, 0, 139)<br>DarkCyan    #008B8B    rgb(0, 139, 139)<br>DarkGoldenRod    #B8860B    rgb(184, 134, 11)<br>DarkGray    #A9A9A9    rgb(169, 169, 169)<br>DarkGreen    #006400    rgb(0, 100, 0)<br>DarkKhaki    #BDB76B    rgb(189, 183, 107)<br>DarkMagenta    #8B008B    rgb(139, 0, 139)<br>DarkOliveGreen    #556B2F    rgb(85, 107, 47)<br>Darkorange    #FF8C00    rgb(255, 140, 0)<br>DarkOrchid    #9932CC    rgb(153, 50, 204)<br>DarkRed    #8B0000    rgb(139, 0, 0)<br>DarkSalmon    #E9967A    rgb(233, 150, 122)<br>DarkSeaGreen    #8FBC8F    rgb(143, 188, 143)<br>DarkSlateBlue    #483D8B    rgb(72, 61, 139)<br>DarkSlateGray    #2F4F4F    rgb(47, 79, 79)<br>DarkTurquoise    #00CED1    rgb(0, 206, 209)<br>DarkViolet    #9400D3    rgb(148, 0, 211)<br>DeepPink    #FF1493    rgb(255, 20, 147)<br>DeepSkyBlue    #00BFFF    rgb(0, 191, 255)<br>DimGray    #696969    rgb(105, 105, 105)<br>DodgerBlue    #1E90FF    rgb(30, 144, 255)<br>Feldspar    #D19275    rgb(209, 146, 117)<br>FireBrick    #B22222    rgb(178, 34, 34)<br>FloralWhite    #FFFAF0    rgb(255, 250, 240)<br>ForestGreen    #228B22    rgb(34, 139, 34)<br>Fuchsia    #FF00FF    rgb(255, 0, 255)<br>Gainsboro    #DCDCDC    rgb(220, 220, 220)<br>GhostWhite    #F8F8FF    rgb(248, 248, 255)<br>Gold    #FFD700    rgb(255, 215, 0)<br>GoldenRod    #DAA520    rgb(218, 165, 32)<br>Gray    #808080    rgb(128, 128, 128)<br>Green    #008000    rgb(0, 128, 0)<br>GreenYellow    #ADFF2F    rgb(173, 255, 47)<br>HoneyDew    #F0FFF0    rgb(240, 255, 240)<br>HotPink    #FF69B4    rgb(255, 105, 180)<br>IndianRed    #CD5C5C    rgb(205, 92, 92)<br>Indigo    #4B0082    rgb(75, 0, 130)<br>Ivory    #FFFFF0    rgb(255, 255, 240)<br>Khaki    #F0E68C    rgb(240, 230, 140)<br>Lavender    #E6E6FA    rgb(230, 230, 250)<br>LavenderBlush    #FFF0F5    rgb(255, 240, 245)<br>LawnGreen    #7CFC00    rgb(124, 252, 0)<br>LemonChiffon    #FFFACD    rgb(255, 250, 205)<br>LightBlue    #ADD8E6    rgb(173, 216, 230)<br>LightCoral    #F08080    rgb(240, 128, 128)<br>LightCyan    #E0FFFF    rgb(224, 255, 255)<br>LightGoldenRodYellow    #FAFAD2    rgb(250, 250, 210)<br>LightGrey    #D3D3D3    rgb(211, 211, 211)<br>LightGreen    #90EE90    rgb(144, 238, 144)<br>LightPink    #FFB6C1    rgb(255, 182, 193)<br>LightSalmon    #FFA07A    rgb(255, 160, 122)<br>LightSeaGreen    #20B2AA    rgb(32, 178, 170)<br>LightSkyBlue    #87CEFA    rgb(135, 206, 250)<br>LightSlateBlue    #8470FF    rgb(132, 112, 255)<br>LightSlateGray    #778899    rgb(119, 136, 153)<br>LightSteelBlue    #B0C4DE    rgb(176, 196, 222)<br>LightYellow    #FFFFE0    rgb(255, 255, 224)<br>Lime    #00FF00    rgb(0, 255, 0)<br>LimeGreen    #32CD32    rgb(50, 205, 50)<br>Linen    #FAF0E6    rgb(250, 240, 230)<br>Magenta    #FF00FF    rgb(255, 0, 255)<br>Maroon    #800000    rgb(128, 0, 0)<br>MediumAquaMarine    #66CDAA    rgb(102, 205, 170)<br>MediumBlue    #0000CD    rgb(0, 0, 205)<br>MediumOrchid    #BA55D3    rgb(186, 85, 211)<br>MediumPurple    #9370D8    rgb(147, 112, 216)<br>MediumSeaGreen    #3CB371    rgb(60, 179, 113)<br>MediumSlateBlue    #7B68EE    rgb(123, 104, 238)<br>MediumSpringGreen    #00FA9A    rgb(0, 250, 154)<br>MediumTurquoise    #48D1CC    rgb(72, 209, 204)<br>MediumVioletRed    #C71585    rgb(199, 21, 133)<br>MidnightBlue    #191970    rgb(25, 25, 112)<br>MintCream    #F5FFFA    rgb(245, 255, 250)<br>MistyRose    #FFE4E1    rgb(255, 228, 225)<br>Moccasin    #FFE4B5    rgb(255, 228, 181)<br>NavajoWhite    #FFDEAD    rgb(255, 222, 173)<br>Navy    #000080    rgb(0, 0, 128)<br>OldLace    #FDF5E6    rgb(253, 245, 230)<br>Olive    #808000    rgb(128, 128, 0)<br>OliveDrab    #6B8E23    rgb(107, 142, 35)<br>Orange    #FFA500    rgb(255, 165, 0)<br>OrangeRed    #FF4500    rgb(255, 69, 0)<br>Orchid    #DA70D6    rgb(218, 112, 214)<br>PaleGoldenRod    #EEE8AA    rgb(238, 232, 170)<br>PaleGreen    #98FB98    rgb(152, 251, 152)<br>PaleTurquoise    #AFEEEE    rgb(175, 238, 238)<br>PaleVioletRed    #D87093    rgb(216, 112, 147)<br>PapayaWhip    #FFEFD5    rgb(255, 239, 213)<br>PeachPuff    #FFDAB9    rgb(255, 218, 185)<br>Peru    #CD853F    rgb(205, 133, 63)<br>Pink    #FFC0CB    rgb(255, 192, 203)<br>Plum    #DDA0DD    rgb(221, 160, 221)<br>PowderBlue    #B0E0E6    rgb(176, 224, 230)<br>Purple    #800080    rgb(128, 0, 128)<br>Red    #FF0000    rgb(255, 0, 0)<br>RosyBrown    #BC8F8F    rgb(188, 143, 143)<br>RoyalBlue    #4169E1    rgb(65, 105, 225)<br>SaddleBrown    #8B4513    rgb(139, 69, 19)<br>Salmon    #FA8072    rgb(250, 128, 114)<br>SandyBrown    #F4A460    rgb(244, 164, 96)<br>SeaGreen    #2E8B57    rgb(46, 139, 87)<br>SeaShell    #FFF5EE    rgb(255, 245, 238)<br>Sienna    #A0522D    rgb(160, 82, 45)<br>Silver    #C0C0C0    rgb(192, 192, 192)<br>SkyBlue    #87CEEB    rgb(135, 206, 235)<br>SlateBlue    #6A5ACD    rgb(106, 90, 205)<br>SlateGray    #708090    rgb(112, 128, 144)<br>Snow    #FFFAFA    rgb(255, 250, 250)<br>SpringGreen    #00FF7F    rgb(0, 255, 127)<br>SteelBlue    #4682B4    rgb(70, 130, 180)<br>Tan    #D2B48C    rgb(210, 180, 140)<br>Teal    #008080    rgb(0, 128, 128)<br>Thistle    #D8BFD8    rgb(216, 191, 216)<br>Tomato    #FF6347    rgb(255, 99, 71)<br>Turquoise    #40E0D0    rgb(64, 224, 208)<br>Violet    #EE82EE    rgb(238, 130, 238)<br>VioletRed    #D02090    rgb(208, 32, 144)<br>Wheat    #F5DEB3    rgb(245, 222, 179)<br>White    #FFFFFF    rgb(255, 255, 255)<br>WhiteSmoke    #F5F5F5    rgb(245, 245, 245)<br>Yellow    #FFFF00    rgb(255, 255, 0)<br>YellowGreen    #9ACD32    rgb(154, 205, 50)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/wireshark-dup-ack-issue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/wireshark-dup-ack-issue/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/wireshark-dup-ack-issue/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/wireshark-dup-ack-issue/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>在wireshark中看到一个tcp会话中的两台机器突然一直互相发dup ack包，但是没有触发重传。每次重复ack都是间隔精确的20秒</p>
<h2 id="如下截图："><a href="#如下截图：" class="headerlink" title="如下截图："></a>如下截图：</h2><p><img src="http://i.imgur.com/bm3W68Q.png" alt></p>
<p>client都一直在回复收到2号包（ack=2）了，可是server跟傻了一样居然还发seq=1的包（按理，应该发比2大的包啊）</p>
<h2 id="系统配置："><a href="#系统配置：" class="headerlink" title="系统配置："></a>系统配置：</h2><pre><code>net.ipv4.tcp_keepalive_time = 20
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_intvl = 3
</code></pre><h2 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h2><p>抓包不全的话wireshark有缺陷，把keepalive包识别成了dup ack包，看内容这种dup ack和keepalive似乎是一样的，flags都是0x010。keep alive的定义的是后退一格(seq少1）。</p>
<p>2、4、6、8……号包，都有一个“tcp acked unseen segment”。这个一般表示它ack的这个包，没有被抓到。Wirshark如何作出此判断呢？前面一个包是seq=1, len=0，所以正常情况下是ack = seq + len = 1，然而Wireshark看到的确是ack = 2, 它只能判断有一个seq =1, len = 1的包没有抓到。<br>dup ack也是类似道理，这些包完全符合dup ack的定义，因为“ack = ” 某个数连续多次出现了。</p>
<p>这一切都是因为keep alive的特殊性导致的。打开66号包的tcp层（见后面的截图），可以看到它的 next sequence number = 12583，表示正常情况下server发出的下一个包应该是seq = 12583。可是在下一个包，也就是68号包中，却是seq = 12582。keep alive的定义的确是这样，即后退一格。<br>Wireshark只有在抓到数据包（66号包）和keep alive包的情况下才有可能正确识别，前面的抓包中恰好在keep alive之前丢失了数据包，所以Wireshark就蒙了。</p>
<h2 id="构造重现"><a href="#构造重现" class="headerlink" title="构造重现"></a>构造重现</h2><p>如果用“frame.number &gt;= 68” 过滤这个包，然后File–&gt;export specified packets保存成一个新文件，再打开那个新文件，就会发现Wireshark又蒙了。本来能够正常识别的keep alive包又被错看成dup ack了，所以一旦碰到这种情况不要慌要稳</p>
<p>下面是知识点啦</p>
<h2 id="Keepalive"><a href="#Keepalive" class="headerlink" title="Keepalive"></a>Keepalive</h2><p>TCP报文接收方必须回复的场景：</p>
<p>TCP携带字节数据<br>没有字节数据，携带SYN状态位<br>没有字节数据，携带FIN状态位</p>
<p>keepalive 提取历史发送的最后一个字节，充当心跳字节数据，依然使用该字节的最初序列号。也就是前面所说的seq回退了一个</p>
<p>对方收到后因为seq小于TCP滑动窗口的左侧，被判定为duplicated数据包，然后扔掉了，并回复一个duplicated ack</p>
<p>所以keepalive跟duplicated本质是一回事，就看wireshark能够正确识别了。</p>
<h2 id="Duplication-ack是指："><a href="#Duplication-ack是指：" class="headerlink" title="Duplication ack是指："></a>Duplication ack是指：</h2><p>server收到了3和8号包，但是没有收到中间的4/5/6/7，那么server就会ack 3，如果client还是继续发8/9号包，那么server会继续发dup ack 3#1 ; dup ack 3#2 来向客户端说明只收到了3号包，不要着急发后面的大包，把4/5/6/7给我发过来</p>
<h2 id="TCP-Window-Update"><a href="#TCP-Window-Update" class="headerlink" title="TCP Window Update"></a>TCP Window Update</h2><p><img src="https://cdn.nlark.com/yuque/0/2019/png/162611/1558941016099-bc4504f1-e9c7-4d84-85e1-a7f5c6554306.png" alt></p>
<p>如上图，当接收方的tcp Window Size不足一个MSS的时候，为了避免 Silly Window Syndrome，Client不再发小包，而是发送探测包（跟keepalive一样，发一个回退一格的包，触发server ack同时server ack的时候会带过来新的window size）探测包间隔时间是200/400/800/1600……ms这样</p>
<h2 id="正常的keep-alive-Case："><a href="#正常的keep-alive-Case：" class="headerlink" title="正常的keep-alive Case："></a>正常的keep-alive Case：</h2><p><img src="http://i.imgur.com/DsTWFZr.png" alt></p>
<p>keep-alive 通过发一个比实际seq小1的包，比如server都已经 ack 12583了，client故意发一个seq 12582来标识这是一个keep-Alive包</p>
<h2 id="Duplication-ack是指：-1"><a href="#Duplication-ack是指：-1" class="headerlink" title="Duplication ack是指："></a>Duplication ack是指：</h2><p>server收到了3和8号包，但是没有收到中间的4/5/6/7，那么server就会ack 3，如果client还是继续发8/9号包，那么server会继续发dup ack 3#1 ; dup ack 3#2 来向客户端说明只收到了3号包，不要着急发后面的大包，把4/5/6/7给我发过来</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/网络包的流转/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/网络包的流转/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/网络包的流转/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/网络包的流转/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="title-Linux-Network-Stack"><a href="#title-Linux-Network-Stack" class="headerlink" title="title: Linux Network Stack"></a>title: Linux Network Stack</h3><p>date: 2019-05-24 17:30:03<br>categories:</p>
<pre><code>- network
</code></pre><p>tags:</p>
<pre><code>- Linux
- network
- TCP
</code></pre><h1 id="Linux-Network-Stack"><a href="#Linux-Network-Stack" class="headerlink" title="Linux Network Stack"></a>Linux Network Stack</h1><h2 id="文章目标"><a href="#文章目标" class="headerlink" title="文章目标"></a>文章目标</h2><blockquote>
<p> 从一个网络包进到网卡后续如何流转，进而了解中间有哪些关键参数可以控制他们，有什么工具能帮忙可以看到各个环节的一些指征，怎么调整他们。</p>
</blockquote>
<h2 id="接收流程大纲"><a href="#接收流程大纲" class="headerlink" title="接收流程大纲"></a>接收流程大纲</h2><p>在开始收包之前，也就是OS启动的时候，Linux要做许多的准备工作：</p>
<ol>
<li>创建ksoftirqd线程，为它设置好它自己的线程函数，用来处理软中断</li>
<li>协议栈注册，linux要实现许多协议，比如arp，icmp，ip，udp，tcp，每一个协议都会将自己的处理函数注册一下，方便包来了迅速找到对应的处理函数</li>
<li>网卡驱动初始化，每个驱动都有一个初始化函数，内核会让驱动也初始化一下。在这个初始化过程中，把自己的DMA准备好，把NAPI的poll函数地址告诉内核</li>
<li>启动网卡，分配RX，TX队列，注册中断对应的处理函数</li>
</ol>
<p>以上是内核准备收包之前的重要工作，当上面都ready之后，就可以打开硬中断，等待数据包的到来了。</p>
<p>当数据到来了以后，第一个迎接它的是网卡：</p>
<ol>
<li>网卡将数据帧DMA到内存的RingBuffer中，然后向CPU发起中断通知</li>
<li>CPU响应中断请求，调用网卡启动时注册的中断处理函数</li>
<li>中断处理函数几乎没干啥，就发起了软中断请求</li>
<li>内核线程ksoftirqd线程发现有软中断请求到来，先关闭硬中断</li>
<li>ksoftirqd线程开始调用驱动的poll函数收包</li>
<li>poll函数将收到的包送到协议栈注册的ip_rcv函数中</li>
<li>ip_rcv函数再讲包送到udp_rcv函数中（对于tcp包就送到tcp_rcv）</li>
</ol>
<h2 id="详细接收流程"><a href="#详细接收流程" class="headerlink" title="详细接收流程"></a>详细接收流程</h2><ol>
<li>网络包进到网卡，网卡驱动校验MAC，看是否扔掉，取决是否是混杂 promiscuous mode</li>
<li>网卡在启动时会申请一个接收ring buffer，其条目都会指向一个skb的内存。</li>
<li>DMA完成数据报文从网卡硬件到内存到拷贝</li>
<li>网卡发送一个中断通知CPU。</li>
<li>CPU执行网卡驱动注册的中断处理函数，中断处理函数只做一些必要的工作，如读取硬件状态等，并把当前该网卡挂在NAPI的链表中;</li>
<li><strong>Driver “触发” soft IRQ(NET_RX_SOFTIRQ (其实就是设置对应软中断的标志位)</strong> </li>
<li>CPU中断处理函数返回后，会检查是否有软中断需要执行。因第6步设置了NET_RX_SOFTIRQ，则执行报文接收软中断。</li>
<li>在NET_RX_SOFTIRQ软中断中，执行NAPI操作，回调第5步挂载的驱动poll函数。</li>
<li>驱动会对interface进行poll操作，检查网卡是否有接收完毕的数据报文。</li>
<li>将网卡中已经接收完毕的数据报文取出，继续在软中断进行后续处理。注：驱动对interface执行poll操作时，会尝试循环检查网卡是否有接收完毕的报文，直到系统设置的<strong>net.core.netdev_budget</strong>上限(默认300)，或者已经就绪报文。</li>
<li><strong>net_rx_action</strong></li>
<li>内核分配 sk_buff 内存</li>
<li>内核填充 metadata: 协议等，移除 ethernet 包头信息</li>
<li><strong>将skb 传送给内核协议栈 netif_receive_skb</strong></li>
<li><code>__netif_receive_skb_core</code>：将数据送到抓包点（tap）或协议层(i.e. tcpdump)</li>
<li>进入到由 netdev_max_backlog 控制的qdisc</li>
<li>开始 <strong>ip_rcv</strong> 处理流程，主要处理ip协议包头相关信息</li>
<li><strong>调用内核 netfilter 框架(iptables PREROUTING)</strong></li>
<li>进入L4 protocol <strong>tcp_v4_rcv</strong></li>
<li>找到对应的socket</li>
<li>根据 tcp_rmem 进入接收缓冲队列</li>
<li>内核将数据送给接收的应用</li>
</ol>
<blockquote>
<p> 软中断：可以把软中断系统想象成一系列<strong>内核线程</strong>（每个 CPU 一个），这些线程执行针对不同 事件注册的处理函数（handler）。如果你用过 <code>top</code> 命令，可能会注意到 <code>ksoftirqd/0</code> 这个内核线程，其表示这个软中断线程跑在 CPU 0 上。</p>
<p> 硬中断发生在哪一个核上，它发出的软中断就由哪个核来处理。可以通过加大网卡队列数，这样硬中断工作、软中断工作都会有更多的核心参与进来。</p>
</blockquote>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/f2d5399f-4fba-4159-9ce4-aefa78132a43.png" alt="img"></p>
<h3 id="典型的接收堆栈"><a href="#典型的接收堆栈" class="headerlink" title="典型的接收堆栈"></a>典型的接收堆栈</h3><p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2019/png/33359/1557292725626-2e4b452b-8a9e-4d9f-91a6-64357fbd4e0e.png" alt="undefined"> </p>
<h3 id="从四层协议栈来看收包流程"><a href="#从四层协议栈来看收包流程" class="headerlink" title="从四层协议栈来看收包流程"></a>从四层协议栈来看收包流程</h3><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/ddd50d2c70215d477d72734b0834410a.png" alt="image.png"></p>
<h3 id="DMA驱动部分流程图"><a href="#DMA驱动部分流程图" class="headerlink" title="DMA驱动部分流程图"></a>DMA驱动部分流程图</h3><p><a href="https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/" target="_blank" rel="noopener">DMA是一个硬件逻辑</a>，数据传输到系统物理内存的过程中，全程不需要CPU的干预，除了占用总线之外(期间CPU不能使用总线)，没有任何额外开销。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/8edd2edf-5ae9-4f96-83fb-cef367697661.png" alt="img"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/308138af-b3a6-4404-93eb-82dce612ba5b.png" alt="img"></p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/ba2f1764fab3a7b3f485836e8e566ffb.png" alt="image.png"></p>
<ol>
<li>驱动在内存中分配一片缓冲区用来接收数据包，叫做sk_buffer;</li>
<li>将上述缓冲区的地址和大小（即接收描述符），加入到rx ring buffer。描述符中的缓冲区地址是DMA使用的物理地址;</li>
<li>驱动通知网卡有一个新的描述符;</li>
<li>网卡从rx ring buffer中取出描述符，从而获知缓冲区的地址和大小;</li>
<li>网卡收到新的数据包;</li>
<li>网卡将新数据包通过DMA直接写到sk_buffer中。</li>
</ol>
<h3 id="buffer和流控"><a href="#buffer和流控" class="headerlink" title="buffer和流控"></a>buffer和流控</h3><p>影响发送的速度的几个buffer和queue，接收基本一样</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/79e46270-de0d-48d5-99d8-90ced2964154.png" alt="img"></p>
<h3 id="网卡传递数据包到内核的流程图及参数"><a href="#网卡传递数据包到内核的流程图及参数" class="headerlink" title="网卡传递数据包到内核的流程图及参数"></a>网卡传递数据包到内核的流程图及参数</h3><p>软中断NET_TX_SOFTIRQ的处理函数为net_tx_action，NET_RX_SOFTIRQ的为net_rx_action</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/daf7318302c0e7f42fb506d6b47fdbd5.png" alt="image.png"></p>
<p>在网络子系统初始化中为NET_RX_SOFTIRQ注册了处理函数net_rx_action。所以<code>net_rx_action</code>函数就会被执行到了。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/68dc89e050901cd2478a0636a5f0dcbe.png" alt="image.png"></p>
<p>这里需要注意一个细节，<strong>硬中断中设置软中断标记，和ksoftirq的判断是否有软中断到达，都是基于smp_processor_id()的。这意味着只要硬中断在哪个CPU上被响应，那么软中断也是在这个CPU上处理的</strong>。所以说，如果你发现你的Linux软中断CPU消耗都集中在一个核上的话，做法是要把调整硬中断的CPU亲和性，来将硬中断打散到不同的CPU核上去。</p>
<p>软中断（也就是 Linux 里的 ksoftirqd 进程）里收到数据包以后，发现是 tcp 的包的话就会执行到 tcp_v4_rcv 函数。如果是 ESTABLISHED 状态下的数据包，则最终会把数据拆出来放到对应 socket 的接收队列中。然后调用 sk_data_ready 来唤醒用户进程。</p>
<p><img src="/Users/ren/Library/Application Support/typora-user-images/image-20210310144555255.png" alt></p>
<p><code>igb_fetch_rx_buffer</code>和<code>igb_is_non_eop</code>的作用就是把数据帧从RingBuffer上取下来。为什么需要两个函数呢？因为有可能帧要占多个RingBuffer，所以是在一个循环中获取的，直到帧尾部。获取下来的一个数据帧用一个sk_buff来表示。<strong>收取完数据以后，对其进行一些校验，然后开始设置sbk变量的timestamp, VLAN id, protocol等字段</strong>。接下来进入到napi_gro_receive中，里面还会调用关键的 netif_receive_skb， 在<code>netif_receive_skb</code>中，数据包将被送到协议栈中，上图中的tcp_v4_rcv就是其中之一（tcp协议）</p>
<h2 id="发送流程"><a href="#发送流程" class="headerlink" title="发送流程"></a>发送流程</h2><ol>
<li>应用调 sendmsg</li>
<li>TCP 分片 skb_buff</li>
<li>根据 tcp_wmem 缓存需要发送的包</li>
<li>构造TCP包头(src/dst port)</li>
<li>ipv4 调用 tcp_write_xmit 和 tcp_transmit_skb</li>
<li>ip_queue_xmit, 构建 ip 包头(获取目标ip和port)</li>
<li>进入 netfilter 流程 nf_hook()</li>
<li>路由流程 POST_ROUTING</li>
<li>ip_output 分片</li>
<li>进入L2 dev_queue_xmit</li>
<li>填入 txqueuelen 队列</li>
<li>进入发送 Ring Buffer tx</li>
<li>驱动触发软中断 soft IRQ (NET_TX_SOFTIRQ)</li>
</ol>
<h3 id="典型的发送堆栈"><a href="#典型的发送堆栈" class="headerlink" title="典型的发送堆栈"></a>典型的发送堆栈</h3><p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2019/png/33359/1557292508719-5b5a2507-a638-4035-a47a-a8599e69f879.png" alt="undefined"> </p>
<h3 id="从四层协议栈来看发包流程"><a href="#从四层协议栈来看发包流程" class="headerlink" title="从四层协议栈来看发包流程"></a>从四层协议栈来看发包流程</h3><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/0126bbb59ac317337ca963ef83817159.png" alt="image.png"></p>
<p><code>net.core.dev_weight</code> 用来调整 <code>__qdisc_run</code> 的循环处理权重，调大后也就是 <code>__netif_schedule</code> 更多的被调用执</p>
<p>用 <code>sudo ifconfig eth0 txqueuelen **</code> 来控制qdisc 发送队列长度</p>
<p>粗略汇总一下进出堆栈：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/9492686528d67d6f63bcf46fde1d3f58.png" alt="image.png"></p>
<p><a href="http://docshare02.docshare.tips/files/21804/218043783.pdf" target="_blank" rel="noopener">http://docshare02.docshare.tips/files/21804/218043783.pdf</a> 中也有描述：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/e26ce9ed-4075-4837-8064-ea4d4aef09b8.png" alt="img"></p>
<h2 id="内核相关参数"><a href="#内核相关参数" class="headerlink" title="内核相关参数"></a>内核相关参数</h2><h3 id="Ring-Buffer"><a href="#Ring-Buffer" class="headerlink" title="Ring Buffer"></a>Ring Buffer</h3><p>Ring Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。<br>可以使用ethtool -g eth0查看当前Ring Buffer的设置：</p>
<pre><code>$sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:        256
RX Mini:    0
RX Jumbo:    0
TX:        256
Current hardware settings:
RX:        256
RX Mini:    0
RX Jumbo:    0
TX:        256
</code></pre><p>上面的例子是一个小规格的ECS，接收队列、传输队列都为256。</p>
<pre><code>$sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:        4096
RX Mini:    0
RX Jumbo:    0
TX:        4096
Current hardware settings:
RX:        4096
RX Mini:    0
RX Jumbo:    0
TX:        512
</code></pre><p>这是一台物理机，接收队列为4096，传输队列为512。接收队列已经调到了最大，传输队列还可以调大。<strong>队列越大丢包的可能越小，但数据延迟会增加</strong></p>
<h4 id="调整-Ring-Buffer-队列数量"><a href="#调整-Ring-Buffer-队列数量" class="headerlink" title="调整 Ring Buffer 队列数量"></a>调整 Ring Buffer 队列数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ethtool -l eth0</span><br><span class="line">Channel parameters for eth0:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          1</span><br><span class="line">Combined:       8</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          1</span><br><span class="line">Combined:       8</span><br><span class="line"></span><br><span class="line">sudo ethtool -L eth0 combined 8</span><br><span class="line">sudo ethtool -L eth0 rx 8</span><br></pre></td></tr></table></figure>
<p>网卡多队列就是指的有多个RingBuffer，每个RingBufffer可以由一个core来处理</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/51f13ecb5002f628fbe1900ab8b820aa.png" alt="image.png"></p>
<h4 id="网卡各种统计数据查看"><a href="#网卡各种统计数据查看" class="headerlink" title="网卡各种统计数据查看"></a>网卡各种统计数据查看</h4><pre><code>ethtool -S eth0 | grep errors

ethtool -S eth0 | grep rx_ | grep errors //查看网卡是否丢包，一般是ring buffer太小

//监控
ethtool -S eth0 | grep -e &quot;err&quot; -e &quot;drop&quot; -e &quot;over&quot; -e &quot;miss&quot; -e &quot;timeout&quot; -e &quot;reset&quot; -e &quot;restar&quot; -e &quot;collis&quot; -e &quot;over&quot; | grep -v &quot;\: 0&quot;
</code></pre><h4 id="网卡进出队列大小调整"><a href="#网卡进出队列大小调整" class="headerlink" title="网卡进出队列大小调整"></a>网卡进出队列大小调整</h4><pre><code>//查看目前的进出队列大小
ethtool -g eth0
//修改进出队列
ethtool -G eth0 rx 8192 tx 8192
</code></pre><p>要注意如果设置的值超过了允许的最大值，用默认的最大值，一些ECS之类的虚拟机、容器就不允许修改这个值。</p>
<h3 id="txqueuelen"><a href="#txqueuelen" class="headerlink" title="txqueuelen"></a>txqueuelen</h3><p>ifconfig 看到的 txqueuelen 跟Ring Buffer是两个东西，IP协议下面就是 txqueuelen，txqueuelen下面才到Ring Buffer. </p>
<p>常用的tc qdisc、netfilter就是在txqueuelen这一环节。 qdisc 的队列长度是我们用 ifconfig 来看到的 txqueuelen</p>
<p>发送队列就是指的这个txqueuelen，和网卡关联着。 而每个Core接收队列由内核参数： net.core.netdev_max_backlog来设置<br>        //当前值通过ifconfig可以查看到，修改：<br>        ifconfig eth0 txqueuelen 2000<br>        //监控<br>        ip -s link</p>
<p>如果txqueuelen 太小导致数据包被丢弃的情况，这类问题可以通过下面这个命令来观察：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ip -s -s link ls dev eth0</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:16:3e:12:9b:c0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    RX: bytes  packets  errors  dropped overrun mcast</span><br><span class="line">    13189414480980 22529315912 0       0       0       0</span><br><span class="line">    RX errors: length   crc     frame   fifo    missed</span><br><span class="line">               0        0       0       0       0</span><br><span class="line">    TX: bytes  packets  errors  dropped carrier collsns</span><br><span class="line">    15487121408466 12925733540 0       0       0       0</span><br><span class="line">    TX errors: aborted  fifo   window heartbeat transns</span><br><span class="line">               0        0       0       0       2</span><br></pre></td></tr></table></figure>
<p>如果观察到 dropped 这一项不为 0，那就有可能是 txqueuelen 太小导致的。当遇到这种情况时，你就需要增大该值了，比如增加 eth0 这个网络接口的 txqueuelen：</p>
<blockquote>
<p> $ ifconfig eth0 txqueuelen 2000</p>
</blockquote>
<h3 id="Interrupt-Coalescence-IC-rx-usecs-tx-usecs-rx-frames-tx-frames-hardware-IRQ"><a href="#Interrupt-Coalescence-IC-rx-usecs-tx-usecs-rx-frames-tx-frames-hardware-IRQ" class="headerlink" title="Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)"></a>Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)</h3><p>可以通过降低终端的频率，也就是合并硬中断来提升处理网络包的能力，当然这是以增大网络包的延迟为代价。</p>
<pre><code>    //检查
    $ethtool -c eth0
    Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0

rx-usecs: 1
rx-frames: 0
rx-usecs-irq: 0
rx-frames-irq: 0

tx-usecs: 0
tx-frames: 0
tx-usecs-irq: 0
tx-frames-irq: 256

rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0

rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0
    //修改, 
    ethtool -C eth0 rx-usecs value tx-usecs value
    //监控
    cat /proc/interrupts
</code></pre><p>我们来说一下上述结果的大致含义</p>
<ul>
<li>Adaptive RX: 自适应中断合并，网卡驱动自己判断啥时候该合并啥时候不合并</li>
<li>rx-usecs：当过这么长时间过后，一个RX interrupt就会被产生</li>
<li>rx-frames：当累计接收到这么多个帧后，一个RX interrupt就会被产生</li>
</ul>
<h3 id="软中断合并-GRO"><a href="#软中断合并-GRO" class="headerlink" title="软中断合并 GRO"></a>软中断合并 GRO</h3><p>GRO和硬中断合并的思想很类似，不过阶段不同。硬中断合并是在中断发起之前，而GRO已经到了软中断上下文中了。</p>
<p>如果应用中是大文件的传输，大部分包都是一段数据，不用GRO的话，会每次都将一个小包传送到协议栈（IP接收函数、TCP接收）函数中进行处理。开启GRO的话，Linux就会智能进行包的合并，之后将一个大包传给协议处理函数。这样CPU的效率也是就提高了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ethtool -k eth0 | grep generic-receive-offload</span><br><span class="line">generic-receive-offload: on</span><br></pre></td></tr></table></figure>
<p>如果你的网卡驱动没有打开GRO的话，可以通过如下方式打开。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ethtool -K eth0 gro on</span><br></pre></td></tr></table></figure>
<p>这是收包，发包对应参数是GSO</p>
<h3 id="ifconfig-监控指标"><a href="#ifconfig-监控指标" class="headerlink" title="ifconfig 监控指标"></a>ifconfig 监控指标</h3><ul>
<li>RX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。或者Ring Buffer太小导致的（很少见），overruns数量持续增加，建议增大Ring Buffer ，使用ethtool ‐G 进行设置。</li>
<li>RX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。如下四种情况导致dropped：Softnet backlog full（pfmemalloc &amp;&amp; !skb_pfmemalloc_protocol(skb)–分配内存失败）；Bad / Unintended VLAN tags；Unknown / Unregistered protocols；IPv6 frames</li>
<li>RX errors：表示总的收包的错误数量，这包括 too-long-frames 错误，Ring Buffer 溢出错误，crc 校验错误，帧同步错误，fifo overruns 以及 missed pkg 等等。</li>
</ul>
<h4 id="overruns"><a href="#overruns" class="headerlink" title="overruns"></a>overruns</h4><p>当驱动处理速度跟不上网卡收包速度时，驱动来不及分配缓冲区，NIC接收到的数据包无法及时写到sk_buffer，就会产生堆积，当NIC内部缓冲区写满后，就会丢弃部分数据，引起丢包。这部分丢包为rx_fifo_errors，在 /proc/net/dev中体现为fifo字段增长，在ifconfig中体现为overruns指标增长。</p>
<h3 id="监控指标-proc-net-softnet-stat"><a href="#监控指标-proc-net-softnet-stat" class="headerlink" title="监控指标 /proc/net/softnet_stat"></a>监控指标 /proc/net/softnet_stat</h3><h4 id="net-core-netdev-budget"><a href="#net-core-netdev-budget" class="headerlink" title="net.core.netdev_budget"></a>net.core.netdev_budget</h4><p>一次软中断(ksoftirqd进程)能处理包的上限，有就多处理，处理到300个了一定要停下来让CPU能继续其它工作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.core.netdev_budget //默认300， The default value of the budget is 300. This will cause the SoftIRQ process to drain 300 messages from the NIC before getting off the CPU</span><br></pre></td></tr></table></figure>
<p>如果 /proc/net/softnet_stat <strong>第三列</strong>一直在增加的话需要，表示SoftIRQ 获取的CPU时间太短，来不及处理足够多的网络包，那么需要增大这个值，<strong>当这个值太大的话有可能导致包到了内核但是应用（userspace）抢不到时间片来读取这些packet。</strong></p>
<p>增大和查看 net.core.netdev_budget    </p>
<blockquote>
<p>sysctl -a | grep net.core.netdev_budget<br>sysctl -w net.core.netdev_budget=400 //临时性增大</p>
</blockquote>
<p>早期的时候网卡一般是10Mb的，现在基本都是10Gb的了，还是每一次软中断、上下文切换只处理一个包的话代价太大，需要改进性能。于是引入的NAPI，一次软中断会poll很多packet</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d0fb11d926f5f67357d98b69c23d86ae.png" alt="image.png"></p>
<p><a href="https://github.blog/2019-11-21-debugging-network-stalls-on-kubernetes/" target="_blank" rel="noopener">来源</a> This is much faster, but brings up another problem. What happens if we have so many packets to process that we spend all our time processing packets from the NIC, but we never have time to let the userspace processes actually drain those queues (read from TCP connections, etc.)? Eventually the queues would fill up, and we’d start dropping packets. To try and make this fair, the kernel limits the amount of packets processed in a given softirq context to a certain budget. Once this budget is exceeded, it wakes up a separate thread called <code>ksoftirqd</code> (you’ll see one of these in <code>ps</code> for each core) which processes these softirqs outside of the normal syscall/interrupt path. This thread is scheduled using the standard process scheduler, which already tries to be fair.</p>
<p>于是在Poll很多packet的时候有可能网卡队列一直都有包，那么导致这个Poll动作无法结束，造成应用一直在卡住状态，于是可以通过netdev_max_backlog来设置Poll多少Packet后停止Poll以响应用户请求。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/61fd62cdf0dc0270ce108a4d43a14c85.png" alt="image.png"></p>
<p>一旦出现slow syscall（如上图黄色部分慢）就会导致packet处理被延迟</p>
<h4 id="netdev-max-backlog"><a href="#netdev-max-backlog" class="headerlink" title="netdev_max_backlog"></a>netdev_max_backlog</h4><p>The netdev_max_backlog is a queue within the Linux kernel where traffic is stored after reception from the NIC, but before processing by the protocol stacks (IP, TCP, etc). There is one backlog queue per CPU core. </p>
<p>如果 /proc/net/softnet_stat 第二列一直在增加的话表示netdev backlog queue overflows. 需要增大 netdev_max_backlog</p>
<p>增大和查看 netdev_max_backlog：<br>        sysctl -a |grep netdev_max_backlog<br>        sysctl -w net.core.netdev_max_backlog=1024 //临时性增大</p>
<p>netdev_max_backlog(接收)和txqueuelen(发送)相对应 </p>
<h4 id="softnet-stat"><a href="#softnet-stat" class="headerlink" title="softnet_stat"></a>softnet_stat</h4><p>关于<code>/proc/net/softnet_stat</code> 的重要细节:</p>
<ol>
<li>每一行代表一个 <code>struct softnet_data</code> 变量。因为每个 CPU 只有一个该变量，所以每行 其实代表一个 CPU</li>
<li>每列用空格隔开，数值用 16 进制表示</li>
<li>第一列 <code>sd-&gt;processed</code>，是处理的网络帧的数量。如果你使用了 ethernet bonding， 那这个值会大于总的网络帧的数量，因为 ethernet bonding 驱动有时会触发网络数据被 重新处理（re-processed）</li>
<li>第二列，<code>sd-&gt;dropped</code>，是因为处理不过来而 drop 的网络帧数量。后面会展开这一话题</li>
<li>第三列，<code>sd-&gt;time_squeeze</code>，前面介绍过了，由于 budget 或 time limit 用完而退出 <code>net_rx_action</code> 循环的次数</li>
<li>接下来的 5 列全是 0</li>
<li>第九列，<code>sd-&gt;cpu_collision</code>，是为了发送包而获取锁的时候有冲突的次数</li>
<li>第十列，<code>sd-&gt;received_rps</code>，是这个 CPU 被其他 CPU 唤醒去收包的次数</li>
<li>最后一列，<code>flow_limit_count</code>，是达到 flow limit 的次数。flow limit 是 RPS 的特性， 后面会稍微介绍一下</li>
</ol>
<h3 id="TCP协议栈Buffer"><a href="#TCP协议栈Buffer" class="headerlink" title="TCP协议栈Buffer"></a>TCP协议栈Buffer</h3><pre><code>    sysctl -a | grep net.ipv4.tcp_rmem   // receive
    sysctl -a | grep net.ipv4.tcp_wmem   // send
    //监控
    cat /proc/net/sockstat

参考：[TCP性能优化大全](https://www.atatech.org/articles/140017)    
</code></pre><h4 id="接收Buffer"><a href="#接收Buffer" class="headerlink" title="接收Buffer"></a>接收Buffer</h4><pre><code>$netstat -sn | egrep &quot;prune|collap&quot;; sleep 30; netstat -sn | egrep &quot;prune|collap&quot;
17671 packets pruned from receive queue because of socket buffer overrun
18671 packets pruned from receive queue because of socket buffer overrun
</code></pre><p>如果 “pruning” 一直在增加很有可能是程序中调用了 setsockopt(SO_RCVBUF) 导致内核关闭了动态调整功能，或者压力大，缓存不够了。具体Case：<a href="https://blog.cloudflare.com/the-story-of-one-latency-spike/" target="_blank" rel="noopener">https://blog.cloudflare.com/the-story-of-one-latency-spike/</a></p>
<p>nstat也可以看到比较多的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$nstat -z |grep -i drop</span><br><span class="line">TcpExtLockDroppedIcmps          0                  0.0</span><br><span class="line">TcpExtListenDrops               0                  0.0</span><br><span class="line">TcpExtTCPBacklogDrop            0                  0.0</span><br><span class="line">TcpExtPFMemallocDrop            0                  0.0</span><br><span class="line">TcpExtTCPMinTTLDrop             0                  0.0</span><br><span class="line">TcpExtTCPDeferAcceptDrop        0                  0.0</span><br><span class="line">TcpExtTCPReqQFullDrop           0                  0.0</span><br><span class="line">TcpExtTCPOFODrop                0                  0.0</span><br><span class="line">TcpExtTCPZeroWindowDrop         0                  0.0</span><br><span class="line">TcpExtTCPRcvQDrop               0                  0.0</span><br></pre></td></tr></table></figure>
<h2 id="总体简略收包流程"><a href="#总体简略收包流程" class="headerlink" title="总体简略收包流程"></a>总体简略收包流程</h2><p><img src="http://img3.tbcdn.cn/L1/461/1/73d01c4c8164ae8642ff09d5d3fe0548d4162874" alt></p>
<p>带参数版收包流程：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/aaf4ff8bbcc26e9e5efe48c984abe508.png" alt="image.png"></p>
<h2 id="总体简略发送包流程"><a href="#总体简略发送包流程" class="headerlink" title="总体简略发送包流程"></a>总体简略发送包流程</h2><p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2019/png/33359/1557291324544-ca69d448-08e4-46c4-9c49-8cf516fc3eaa.png" alt></p>
<p>带参数版发包流程：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/955fc732d8620561a9ebce992b0129b1.png" alt="image.png"></p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="snat-dnat-宿主机port冲突，丢包"><a href="#snat-dnat-宿主机port冲突，丢包" class="headerlink" title="snat/dnat 宿主机port冲突，丢包"></a>snat/dnat 宿主机port冲突，丢包</h3><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/d42ccdb0b8f6270d8b559145c0b89c86.png" alt="image.png"></p>
<ol>
<li>snat 就是要把 192.168.1.10和192.168.1.11的两个连接替换成宿主机的ip:port</li>
<li>主要是在宿主机找可用port分别给这两个连接用</li>
<li>找可用port分两步<ul>
<li>找到可用port</li>
<li>将可用port写到数据库，后面做连接追踪用(conntrack)</li>
</ul>
</li>
<li>上述两步不是事务，可能两个连接同时找到一个相同的可用port，但是只有第一个能写入成功，第二个fail，fail后这个包被扔掉</li>
<li>1秒钟后被扔掉的包重传，后续正常</li>
</ol>
<p>症状：</p>
<ul>
<li>问题发生概率不高，跟压力没有关系，跟容器也没有关系，只要有snat/dnat和并发就会发生，只发生在创建连接的第一个syn包</li>
<li>可以通过conntrack工具来检查fail的数量</li>
<li>实际影响只是请求偶尔被拉长了1秒或者3秒</li>
<li>snat规则创建的时候增加参数：NF_NAT_RANGE_PROTO_RANDOM_FULLY 来将冲突降低几个数量级—-可以认为修复了这个问题</li>
</ul>
<pre><code>sudo conntrack -L -d ip-addr
</code></pre><p>来自：<a href="https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02" target="_blank" rel="noopener">https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02</a></p>
<h3 id="容器-bridge-通过udp访问宿主机服务失败"><a href="#容器-bridge-通过udp访问宿主机服务失败" class="headerlink" title="容器(bridge)通过udp访问宿主机服务失败"></a>容器(bridge)通过udp访问宿主机服务失败</h3><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/a067b484c593aa3a4b6a525d1f93506e.png" alt="image.png"></p>
<p>这个案例主要是讲述回包的逻辑，如果是tcp，那么用dest ip当自己的source ip，如果是UDP，无连接状态信息，那么会根据route来选择一块网卡(上面的IP) 来当source ip</p>
<p>来自：<a href="http://cizixs.com/2017/08/21/docker-udp-issue" target="_blank" rel="noopener">http://cizixs.com/2017/08/21/docker-udp-issue</a><br>     <a href="https://github.com/moby/moby/issues/15127" target="_blank" rel="noopener">https://github.com/moby/moby/issues/15127</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://blog.hyfather.com/blog/2013/03/04/ifconfig/" target="_blank" rel="noopener">The Missing Man Page for ifconfig–关于ifconfig的种种解释</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1400834?s=original-sharing" target="_blank" rel="noopener">Linux数据报文的来龙去脉</a></p>
<p><a href="https://www.atatech.org/articles/27189" target="_blank" rel="noopener">Linux TCP队列相关参数的总结–锋寒</a></p>
<p><a href="https://github.com/leandromoreira/linux-network-performance-parameters" target="_blank" rel="noopener">linux-network-performance-parameters</a></p>
<p><a href="https://www.cnblogs.com/fczjuever/archive/2013/04/17/3026694.html" target="_blank" rel="noopener">Linux之TCPIP内核参数优化</a></p>
<p><a href="https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf" target="_blank" rel="noopener">https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf</a></p>
<p><a href="https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/" target="_blank" rel="noopener">Linux 网络协议栈收消息过程-Ring Buffer</a> : 支持 RSS 的网卡内部会有多个 Ring Buffer，NIC 收到 Frame 的时候能通过 Hash Function 来决定 Frame 该放在哪个 Ring Buffer 上，触发的 IRQ 也可以通过操作系统或者手动配置 IRQ affinity 将 IRQ 分配到多个 CPU 上。这样 IRQ 能被不同的 CPU 处理，从而做到 Ring Buffer 上的数据也能被不同的 CPU 处理，从而提高数据的并行处理能力。</p>
<p><a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/" target="_blank" rel="noopener">Linux 网络栈监控和调优：发送数据</a></p>
<p><a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/" target="_blank" rel="noopener">Linux 网络栈监控和调优：接收数据（2016）</a></p>
<p>收到包后内核层面的处理：<a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247484058&amp;idx=1&amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从网卡注册软中断处理函数到收包逻辑</a></p>
<p>收到包后应用和协议层面的处理：图解 | 深入理解高性能网络开发路上的绊脚石 - 同步阻塞网络 IO<a href="https://mp.weixin.qq.com/s/cIcw0S-Q8pBl1-WYN0UwnA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cIcw0S-Q8pBl1-WYN0UwnA</a> 当软中断上收到数据包时会通过调用 sk_data_ready 函数指针（实际被设置成了 sock_def_readable()） 来唤醒在 sock 上等待的进程</p>
<p><a href="http://docshare02.docshare.tips/files/21804/218043783.pdf" target="_blank" rel="noopener">http://docshare02.docshare.tips/files/21804/218043783.pdf</a></p>
<p><a href="https://wiki.linuxfoundation.org/networking/kernel_flow" target="_blank" rel="noopener">https://wiki.linuxfoundation.org/networking/kernel_flow</a></p>
<p><a href="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Netfilter-packet-flow.svg/2000px-Netfilter-packet-flow.svg.png" target="_blank" rel="noopener">https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Netfilter-packet-flow.svg/2000px-Netfilter-packet-flow.svg.png</a></p>
<p><a href="https://wiki.nix-pro.com/view/Packet_journey_through_Linux_kernel" target="_blank" rel="noopener">https://wiki.nix-pro.com/view/Packet_journey_through_Linux_kernel</a></p>
<p><a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/" target="_blank" rel="noopener">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/方舟平台部署环境要求/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/方舟平台部署环境要求/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/方舟平台部署环境要求/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/方舟平台部署环境要求/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="方舟平台部署环境要求"><a href="#方舟平台部署环境要求" class="headerlink" title="方舟平台部署环境要求"></a>方舟平台部署环境要求</h1><pre><code>1.  操作系统CentOS7.2 64bit 选择标准安装（不能用最小化安装）
2.  配置好yum源能够安装基本的软件
3.  提供统一的NTP服务器
4.  物理机防火墙关闭
5.  物理机关闭selinux
6.  每台机器hostname都不一样( 执行 hostname 查看到的)
7.  网卡名要一样（可以都是eth0或者bond0这样的），不是随机数
8.  Ipv4_forward 处于开启状态
9.  提前配置好一个独立vlan（这个vlan里面除了网关其它ip都不能有其它机器在用），这个vlan要和物理机不是同一个vlan，要跟里外物理机都能连通。然后提供如下这三个信息：    
    ​    a) 这个vlan的范围，比如：10.16.9.0/24
    ​    b) 网关，比如10.16.9.254
    ​    c) vlan id，比如vlan169       
10.  物理服务器连接在交换机上的接口配置成trunk模式，允许物理机和docker的vlan通过【9和10 的配置不需要修改物理机的任何配置，在交换机上完成】  
11.  提供MySQL 5.6及以上版本，字符集为utf8（不能是latin，忽略表名大小写）    
12.  提供root帐号权限（部署完成后可修改）    
13.  能够提供外网访问跳板机，可以访问OSS并将的安装包复制到本地某台机器    
14.  物理机磁盘不小于2.4T， 数据盘做RAID10
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/soft lockup kernel bug/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/soft lockup kernel bug/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/soft lockup kernel bug/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/soft lockup kernel bug/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/459908b71f7a269b8443ed3f0df741ea.png" alt="image.png"></p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/1b0369b4cc1298f291086baef0e25476.png" alt="image.png"></p>
<p>tsar –load 结果，Load明显大于runq（procs_running， load大致应该是procs_running+procs_blocked+？）</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/1be8affd6ea21e623ec7d8507d6c65bd.png" alt="image.png"></p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/98e75570dba2abcfe2eb1028c8158e18.png" alt="image.png"></p>
<h1 id="soft-lockup-kernel-bug"><a href="#soft-lockup-kernel-bug" class="headerlink" title="soft lockup kernel bug"></a>soft lockup kernel bug</h1><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/5ca76f59f07b8007a17cc58d557f7bd5.png" alt="image.png"></p>
<p>内核软死锁（soft lockup）bug</p>
<p>如果是用户空间的进程或线程引起的问题backtrace是不会有内容的，如果内核线程那么在soft lockup消息中会显示出backtrace信息。</p>
<p>简单来说： 由于系统的某个驱动程序有问题导致watchdog无法收集每一个逻辑cpu运行时使用数据并抛出一个软死锁（soft lockup）错误</p>
<p><strong>Soft lockup名称解释：所谓soft lockup就是说，这个bug没有让系统彻底死机，但是若干个进程（或者kernel thread）被锁死在了某个状态（一般在内核区域），很多情况下这个是由于内核锁的使用的问题。</strong></p>
<p><a href="http://linuxperf.com/?p=83" target="_blank" rel="noopener">http://linuxperf.com/?p=83</a></p>
<p>所谓lockup，是指某段内核代码占着CPU不放。Lockup严重的情况下会导致整个系统失去响应。Lockup有几个特点：</p>
<ul>
<li>首先只有内核代码才能引起lockup，因为用户代码是可以被抢占的，不可能形成lockup；</li>
<li>其次内核代码必须处于禁止内核抢占的状态(preemption disabled)，因为Linux是可抢占式的内核，只在某些特定的代码区才禁止抢占，在这些代码区才有可能形成lockup。</li>
</ul>
<blockquote>
<p>Soft lockup是指CPU被内核代码占据，以至于无法执行其它进程。检测soft lockup的原理是给每个CPU分配一个定时执行的内核线程[watchdog/x]，如果该线程在设定的期限内没有得到执行的话就意味着发生了soft lockup。</p>
</blockquote>
<p>Linux kernel设计了一个检测lockup的机制，称为NMI Watchdog，是利用NMI中断<strong>(Non-maskable interrupt，不可屏蔽中断，简称NMI</strong>)实现的，用NMI是因为lockup有可能发生在中断被屏蔽的状态下，这时唯一能把CPU抢下来的方法就是通过NMI，因为NMI中断是不可屏蔽的。NMI Watchdog 中包含 soft lockup detector 和 hard lockup detector，2.6之后的内核的实现方法如下。</p>
<p>NMI Watchdog 的触发机制包括两部分：</p>
<p>一个高精度计时器(hrtimer)，对应的中断处理例程是kernel/watchdog.c: watchdog_timer_fn()，在该例程中：</p>
<ul>
<li>要递增计数器hrtimer_interrupts，这个计数器供hard lockup detector用于判断CPU是否响应中断；</li>
<li>还要唤醒[watchdog/x]内核线程，该线程的任务是更新一个时间戳；</li>
<li>soft lock detector检查时间戳，如果超过soft lockup threshold一直未更新，说明[watchdog/x]未得到运行机会，意味着CPU被霸占，也就是发生了soft lockup。</li>
</ul>
<p>基于PMU的NMI perf event，当PMU的计数器溢出时会触发NMI中断，对应的中断处理例程是 kernel/watchdog.c: watchdog_overflow_callback()，hard lockup detector就在其中，它会检查上述hrtimer的中断次数(hrtimer_interrupts)是否在保持递增，如果停滞则表明hrtimer中断未得到响应，也就是发生了hard lockup。</p>
<p>hrtimer的周期是：softlockup_thresh/5。<br>注：</p>
<p>在2.6内核中：</p>
<pre><code>softlockup_thresh的值等于内核参数kernel.watchdog_thresh，默认60秒；
</code></pre><p>而到3.10内核中：<br>    内核参数kernel.watchdog_thresh名称未变，但含义变成了hard lockup threshold，默认10秒；<br>    soft lockup threshold则等于（2*kernel.watchdog_thresh），即默认20秒。</p>
<p>在3.10内核里可以手工调整触发周期（hard lockup threshold）:</p>
<pre><code>$sudo sysctl -a | grep watchdog_thresh
#hard lockup threshold，默认10秒；soft lockup threshold则等于（2*kernel.watchdog_thresh），即默认20秒
kernel.watchdog_thresh = 10 

$sudo sysctl -a | grep softlockup
kernel.softlockup_all_cpu_backtrace = 0
kernel.softlockup_panic = 0

$sudo sysctl -a | grep watchdog
#如果为1，表示利用IO APIC的始终源；如果为2，表示利用LOCAL APIC的performance counter
kernel.nmi_watchdog = 1 
kernel.watchdog = 1
kernel.watchdog_thresh = 10
</code></pre><p><a href="https://access.redhat.com/solutions/2073603" target="_blank" rel="noopener">https://access.redhat.com/solutions/2073603</a><br><a href="https://bugzilla.redhat.com/show_bug.cgi?id=821594" target="_blank" rel="noopener">https://bugzilla.redhat.com/show_bug.cgi?id=821594</a></p>
<p>关于NMI解释：<a href="http://blog.csdn.net/freemancqcsdn/article/details/4499100" target="_blank" rel="noopener">http://blog.csdn.net/freemancqcsdn/article/details/4499100</a></p>
<h2 id="Linux-Load含义"><a href="#Linux-Load含义" class="headerlink" title="Linux Load含义"></a>Linux Load含义</h2><h3 id="uptime和top等命令都可以看到load-average指标，从左至右三个数字分别表示1分钟、5分钟、15分钟的load-average："><a href="#uptime和top等命令都可以看到load-average指标，从左至右三个数字分别表示1分钟、5分钟、15分钟的load-average：" class="headerlink" title="uptime和top等命令都可以看到load average指标，从左至右三个数字分别表示1分钟、5分钟、15分钟的load average："></a>uptime和top等命令都可以看到load average指标，从左至右三个数字分别表示1分钟、5分钟、15分钟的load average：</h3><pre><code>$ uptime
 10:16:25 up 3 days, 19:23,  2 users,  load average: 0.00, 0.01, 0.05
</code></pre><p>Load average的概念源自UNIX系统，虽然各家的公式不尽相同，但都是用于衡量正在使用CPU的进程数量和正在等待CPU的进程数量，一句话就是runnable processes的数量。所以load average可以作为CPU瓶颈的参考指标，如果大于CPU的数量，说明CPU可能不够用了。</p>
<p><strong>但是，Linux上不是这样的！</strong></p>
<p>Linux上的load average除了包括正在使用CPU的进程数量和正在等待CPU的进程数量之外，还包括uninterruptible sleep的进程数量。通常等待IO设备、等待网络的时候，进程会处于uninterruptible sleep状态。Linux设计者的逻辑是，uninterruptible sleep应该都是很短暂的，很快就会恢复运行，所以被等同于runnable。然而uninterruptible sleep即使再短暂也是sleep，何况现实世界中uninterruptible sleep未必很短暂，大量的、或长时间的uninterruptible sleep通常意味着IO设备遇到了瓶颈。众所周知，sleep状态的进程是不需要CPU的，即使所有的CPU都空闲，正在sleep的进程也是运行不了的，所以sleep进程的数量绝对不适合用作衡量CPU负载的指标，Linux把uninterruptible sleep进程算进load average的做法直接颠覆了load average的本来意义。所以在Linux系统上，load average这个指标基本失去了作用，因为你不知道它代表什么意思，当看到load average很高的时候，你不知道是runnable进程太多还是uninterruptible sleep进程太多，也就无法判断是CPU不够用还是IO设备有瓶颈。</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/7e52c04a38d5e952347623a7f3a467d1.png" alt="image.png"></p>
<p>CentOS7 源代码中对Load计算算法的说明：<br><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/94a879bed735e14409549dd6c2e85c90.png" alt="image.png"></p>
<p>参考资料：<a href="https://en.wikipedia.org/wiki/Load_(computing)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Load_(computing)</a></p>
<p>“Most UNIX systems count only processes in the running (on CPU) or runnable (waiting for CPU) states. However, Linux also includes processes in uninterruptible sleep states (usually waiting for disk activity), which can lead to markedly different results if many processes remain blocked in I/O due to a busy or stalled I/O system.“</p>
<h3 id="linux上进程有5种状态和ps工具标识进程的这5种状态码"><a href="#linux上进程有5种状态和ps工具标识进程的这5种状态码" class="headerlink" title="linux上进程有5种状态和ps工具标识进程的这5种状态码:"></a>linux上进程有5种状态和ps工具标识进程的这5种状态码:</h3><ul>
<li>D 不可中断 uninterruptible sleep (usually IO)(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生)</li>
<li>R 运行 runnable (on run queue) (正在运行或在运行队列中等待)</li>
<li>S 中断 sleeping (休眠中, 受阻, 在等待某个条件的形成或接受到信号)</li>
<li>T 停止 traced or stopped 停止(进程收到SIGSTOP, SIGTSTP, SIGTTIN, SIGTTOU信号后停止运行运行)</li>
<li>Z 僵死 a defunct (”zombie”) process 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放)</li>
</ul>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/6aa4c83d0b01bb503fc4a984a12ae7cb.png" alt="image.png"><br>from: <a href="https://idea.popcount.org/2012-12-11-linux-process-states/" target="_blank" rel="noopener">https://idea.popcount.org/2012-12-11-linux-process-states/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/README/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/README/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/README/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/README/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="case"><a href="#case" class="headerlink" title="case"></a>case</h1><p>case resource（img/blog)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/tcprt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/tcprt/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/tcprt/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/tcprt/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>from：<a href="http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt" target="_blank" rel="noopener">http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt</a></p>
<p>最新文档：<br><a href="http://gitlab.alibaba-inc.com/alicdn/netOpt-doc/tcprt/tcprt-wiki.md" target="_blank" rel="noopener">http://gitlab.alibaba-inc.com/alicdn/netOpt-doc/tcprt/tcprt-wiki.md</a> </p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>1 TCPRT介绍<br>1.1 TCPRT的实现原理<br>1.2 日志信息介绍<br>2 部署和使用<br>3 Q&amp;A</p>
<h3 id="TCPRT介绍"><a href="#TCPRT介绍" class="headerlink" title="TCPRT介绍"></a>TCPRT介绍</h3><p>TCPRT是一个面向web的从tcp层的角度监控和分析网络服务质量和网络覆盖的超轻量级的分析工具； TCPRT在单机进出流量超过10G，单机4w qps的情况下， cpu消耗不超过1%； 经过不断的演化，TCPRT现在具有两个功能： 1.网络服务质量的实时监控；包括 用户的请求文件的平均响应时间（rt），平均服务器响应时间（servertime），平均时延rtt，平均丢包率 等； 2.详细的网络服务质量数据采集：包括用户访问的每一条请求的响应时间（rt），服务器响应时间（servertime），时延rtt，丢包率等；</p>
<p>其中 rt的含义是指从 服务器接收到请求到服务器发送给用户的数据包全部被确认所花费的时间；</p>
<blockquote>
<p>servertime的含义是 从服务器接收到请求到服务器发送第一个数据包的时间；   （如果请求的是小文件，这个时间==应用服务器的rt）</p>
</blockquote>
<p><strong>目前增加了一版新架构实现的TCPRT，在功能上增加了域名粒度的实时访问数据。即可以查看一个周期内访问某个特定域名的rt,servertime rtt 等数据，并且方便更新。</strong></p>
<h3 id="TCPRT的实现原理"><a href="#TCPRT的实现原理" class="headerlink" title="TCPRT的实现原理"></a>TCPRT的实现原理</h3><p>tcprt采集的是主要有两方面的数据，一方面是用户下载文件实际所需要的时间，在现在的情况下，对于web用户来说，每个请求都是串行的每个请求下载完以后，才会有下一个请求，所以，我们只需要记录请求到达服务器的时间和发送的所有数据被确认的时间，就能知道这个请求用户实际上花了多长时间去下载， 后续如果pipeline 普及的话，很抱歉，基于网络四层的统计方法都要失效；另一方面是网络覆盖的数据，主要包括丢包个数、发送的有效数据、rtt（数据包一来一回的时间）；</p>
<p>tcprt利用的是内核的拥塞控制模块来采集数据的，tcp协议栈处理过程中，在很多特殊事件会调用拥塞控制模块，拥塞控制模块的处理函数又可以拿到这个连接的所有信息（sock），这里面包含了丢包、rtt等信息，因此 tcprt不需要做任何的分析工作，只需要采集数据即可；</p>
<p>同时，触发 拥塞控制模块的事件中，又包括了几个我们需要的时间点，包括请求到达， 第一个数据包发送，连接关闭；所以我们只需要做简单的if判断就能够获取服务质量相关的数据；</p>
<p><strong>新架构TCPRT 是在tcp 协议栈处理过程的几个地方添加了钩子，通过在钩子中做 rt 等数据的采集即可，所以只有特定的内核才能支持。实现域名粒度的数据采集是通过对TCP数据包进行分析，获取其域名并在内核使用hash 链表进行数据的统计。</strong></p>
<h3 id="日志信息介绍"><a href="#日志信息介绍" class="headerlink" title="日志信息介绍"></a>日志信息介绍</h3><p>tcprt会输出两种类型的日志</p>
<ul>
<li>实时的监控数据， tcprt会把实时的监控日志打印到 /sys/kernel/debug/rt-network-real* 中，日志格式如下所示：</li>
</ul>
<p>1379387910 all 80 332 5 55 80 2 27340 101712</p>
<p>时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数；</p>
<ul>
<li>详细的用户访问日志 tcprt把用户详细日志打印到 /sys/kernel/debug/rt-network-logs,访问的结果分为三类， 访问成功，访问失败、没有数据输出，分别对应的日志 第一位为 r w n 如果用户访问成功 ，打出的日志格式如下：</li>
</ul>
<p>r 1379063895 171.214.168.205:12486 80 24185 637 71 0 1 175 1024</p>
<p>访问成功 时间 ip：port 服务器访问端口 文件大小 rt rtt 重传包个数 连接的第n个请求 首字节 mss</p>
<p>w 1379063952 111.227.56.204:52025 80 174240 13134 88 11 28 6 0 2880</p>
<p>访问失败 时间 ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 初始窗口 丢包个数 连接的第n个请求 首字节时间 总发送的文件大小</p>
<p>n 1379063952 111.227.56.204:52025 80 1 2000</p>
<p>访问没有数据返回 时间 ip：port 服务器访问端口 连接的第n个请求 等待时间</p>
<h2 id="新架构TCPRT-的日志格式"><a href="#新架构TCPRT-的日志格式" class="headerlink" title="新架构TCPRT 的日志格式"></a>新架构TCPRT 的日志格式</h2><ol>
<li><p>实时的监控数据， tcprt会把实时的监控日志打印到 /sys/kernel/debug/tcp-watch-real<em> 中，下面的日志都是一个周期内（比如一分钟）的平均值<br>输出到/sys/kernel/debug/tcp-watch-real</em>文件中：<br>(1)基于端口的日志格式如下所示：<br>1379387910 all-new 80 332 5 55 80 2 27340 101712 0 0<br>时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数 https 建连时间 https 响应时间<br>(2)基于域名的日志格式如下所示：<br>1408327819 gdz03.md.alicdn.com 21 13 0 0 0 0 1 0 20182 32 1<br>时间 域名 RT 首字节时间 上传时间 https建连 https响应 丢包率 rtt 失败率 每个请求的平均文件大小 这段时间内请求个数 连接个数 平均建连时间 首播时间<br>输出到/sys/kernel/debug/tcp-watch-back<em>文件中：<br>(3) 回源到C段的 日志格式<br>1408329123 140.205.134 0 136 693 136<br>时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）<br>输出到/sys/kernel/debug/tcp-watch-front</em>文件中：<br>（4）从L2输出到L1的C段日志格式<br>1408329123 140.205.134 0 136 693 136<br>时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）</p>
</li>
<li><p>详细的用户访问日志 tcprt把用户详细日志打印到 /sys/kernel/debug/tcp-watch-log*,访问的结果分为五类， 访问成功、访问失败、没有数据输出、每连接数据统计、回源数据，分别对应的日志 第一位为 r_new w_new n_new c_new back如果用户访问成功 ，打出的日志格式如下：<br>(1)r_new 1408283588 img02.taobaocdn.com 115.238.23.16:34375 81 77480 145 1 0 8 57 0 112 1448<br>访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss<br>更新：<br>r_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 14 1 0 1 3 0 115 1460 1048854 14 3 0 1<br>访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss 连接建立到目前发送数据量 纯数据发送时间 首播时间 空等时间总和 tos<br>(2) w_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1740 13134 88 11 2 6 0 112 2880<br>访问失败 时间 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小<br>更新： w_new 1486624545 cdn.hc.org 115.238.23.194:22086 443 0 5 0 0 2 5 40 174 294 129 6 0 0 2021 1<br>访问失败 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小 连接建立以来确认的字节数 纯数据发送时间 首播时间 空等时间 写缓存排队字节量 tos<br>$1 w_new 访问失败<br>$2 1486624545 时间<br>$3 cdn.hc.org 域名<br>$4 115.238.23.194:22086 ip:port<br>$5 443 服务器访问端口<br>$6 0 被确认文件大小<br>$7 5 总下载时间<br>$8 0 rtt<br>$9 0 重传包个数<br>$10 2 连接的第n个请求<br>$11 5 首字节时间<br>$12 40 上传时间<br>$13 174 上传文件大小<br>$14 294 已发送未确认文件大小<br>$15 129 连接建立以来确认的字节数<br>$16 6 纯数据发送时间<br>$17 0 首播时间<br>$18 0 空等时间<br>$19 2021 写缓存排队字节量<br>$20 1 tos<br>(3)n_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1 4 200 2000<br>访问没有数据返回 时间 请求域名 ip:port 服务器访问端口 连接的第n个请求 上传时间 上传文件大小 等待时间<br>(4)c_new 1408327083 cdn.hc.org 115.238.23.16:40323 81 193 0 0 1 57 6 0 0 1448<br>连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss<br>更新：<br>c_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 0 1 1 116 0 0 0 1460 14 1<br>连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss 纯数据发送时间 tos<br>(5)back 1408327455 140.205.134.10:80 33484 290 0 5 1452<br>回源请求 时间 ip:port 请求端口 发送大小 重传个数 rtt mss</p>
</li>
<li>/proc/tcp_rt_file 中是基于域名的实时首字节展示<br>1408327819 total_statis 1061629323 81324204<br>统计开始时间 所有请求的统计 总首字节时间 总请求数<br>1408327819 cb.alimama.cn 2268 1463<br>统计开始时间 域名 总首字节时间 总请求数<br>部署和使用<br>安装： sudo yum install t-cdn-tcprt -b test</li>
</ol>
<p>开启： /etc/init.d/tcprt start</p>
<p>停止： /etc/init.d/tcprt stop</p>
<p>老版本使用方案：</p>
<p>启动后，请根据你所属的网络和应用配置如下参数：</p>
<p>1）机器是否在 fullnat后面： /sys/module/tcp_rt_base/parameters/nat 如果在fullnat 后面 建议设成1；</p>
<p>echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>如果不在fullnat 后面要设成0 否者的话取不到数据</p>
<p>echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>2）你需要监听的端口，tcprt现在支持 最多3个端口的监控， /sys/module/tcp_rt_statis/parameters/port_nummber 设置 需要监控的端口数 /sys/module/tcp_rt_statis/parameters/port0（1、2） 为需要监控的端口 比如，我需要监控 80 81 82 3个端口，那么可以这么设置</p>
<p>echo 80 &gt; /sys/module/tcp_rt_statis/parameters/port0</p>
<p>echo 81 &gt; /sys/module/tcp_rt_statis/parameters/port1</p>
<p>echo 82 &gt; /sys/module/tcp_rt_statis/parameters/port2</p>
<p>echo 3 &gt; /sys/module/tcp_rt_statis/parameters/port_nummber</p>
<p>3) 数据采集的间隔： /sys/module/tcp_rt_statis/parameters/check_interval 如果你希望 1分钟统计一次</p>
<p>echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</p>
<p>建议不要低于1分钟</p>
<p>以上配置都需要sudo 权限；</p>
<p>tcprt提供了两个配置范例，分别用于cdn和源站 /usr/local/tcprt/cdn.sh</p>
<p>echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</p>
<p>/usr/local/tcprt/source.sh</p>
<p>echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat</p>
<p>echo 300 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</p>
<p>/usr/local/tcprt/tcp_rt_a_log_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp; /usr/local/tcprt/tcp_rt_a_real_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp;</p>
<p>tcprt 还提供了两个日志采集的范例脚本 分别在： /usr/local/tcprt/tcp_rt_a_log_copy.sh /usr/local/tcprt/tcp_rt_a_real_copy.sh 分别是 详细日志和实时监控日志的采集脚本； 如果单机的qps数很高， 强烈建议不要用/usr/local/tcprt/tcp_rt_a_log_copy.sh 采集实时日志； 日志采集的数据会把数据存到 /home/admin/logs/tcprt/ 目录下 实时监控日志 会存到/home/admin/logs/tcp_rt/tcp_rt.statis文件里；</p>
<p>新版本使用方案：<br>你需要监听的端口，tcprt现在支持 最多6个端口的监控，假如需要监听80 443 81 三个端口可以执行<br>echo 80,443,81 &gt;/sys/module/tcp_watch/parameters/port_arr<br>日志的采集可以直接使用<br>cat /sys/kernel/debug/tcp-watch-log<em> &gt; logfile ,<br>cat /sys/kernel/debug/tcp-watch-real</em> &gt; realfile<br>tcp-watch-log<em> tcp-watch-real</em>中的数据只能读取一次，读取后不存在了。</p>
<h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><ol>
<li><p>setsockopt(c-&gt;fd, IPPROTO_TCP, TCP_DOMAIN, peer-&gt;host.data, peer-&gt;host.len); 失败</p>
<p>1）检查uname -r，是不是2.6.32-220.23.2.ali878.tcp1.34.el6.x86_64或2.6.32-220.23.2.ali878.tcp1.54.el6.x86_64，如果不是，则不是常规部署的cdn tcpkernel。</p>
<pre><code>tcprt需适配这两个特定内核版本。其余版本的支持情况，需要人工check。         
</code></pre><p>2）setDOMAIN需要tcprt hotfix版本的支持。检查t-cdn-tcprt版本，是不是最新的，否则更新。  </p>
<pre><code>http://rpm.corp.taobao.com/find.php?q=t-cdn-tcprt，找test/最先版的。部署和使用: http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt。    
</code></pre><p>3）检查配置端口。  </p>
</li>
<li><p>tcprt采集的rtt</p>
<p>tcprt采集以请求为单位，rtt是该次请求中，出现过的最小rtt值。 </p>
</li>
</ol>
<h2 id="TCP-RT-log"><a href="#TCP-RT-log" class="headerlink" title="TCP RT log"></a>TCP RT log</h2><pre><code>V5 R 1575428543 880110 172.18.34.6:48414 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1471 1 0 4 1296 0 37 0 1460
V5 R 1575428543 880977 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 98 621 1 0 0 451 0 0 0 1460
V5 R 1575428543 877650 172.18.34.6:48410 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 4180 1 0 3 4013 0 21 0 1460
V5 R 1575428543 881614 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 993 1 0 1 813 0 99 0 1460
V5 R 1575428543 881832 172.18.34.6:48410 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 843 1 0 4 664 0 39 0 1460
V5 R 1575428543 881582 172.18.34.6:48414 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1272 1 0 5 1097 0 21 0 1460
V5 R 1575428543 881576 172.18.34.6:48416 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 4434 1 0 4 4265 0 37 0 1460
V5 R 1575428543 882855 172.18.34.6:48414 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 3369 1 0 6 3175 0 39 0 1460
V5 R 1575428543 886012 172.18.34.6:48416 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 575 1 0 5 402 0 21 0 1460
V5 R 1575428543 879994 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 1506 6698 1 0 2 6523 0 919 0 1460
V5 R 1575428543 886731 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1140 1 0 3 955 0 19 0 1460
V5 R 1575428543 886436 172.18.34.6:48424 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 98 1573 0 0 0 1410 0 0 0 1460
V5 R 1575428543 887872 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 401 1 0 4 230 0 37 0 1460
V5 R 1575428543 886588 172.18.34.6:48416 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1918 1 0 6 1734 0 39 0 1460
V5 R 1575428543 888024 172.18.34.6:48424 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1007 0 0 1 816 0 99 0 1460
V5 R 1575428543 882609 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 1506 6388 1 0 2 6199 0 919 0 1460
V5 R 1575428543 889038 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 536 1 0 3 361 0 19 0 1460
V5 R 1575428543 889575 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 291 1 0 4 119 0 37 0 1460
V5 R 1575428543 889867 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 325 1 0 5 156 0 21 0 1460
V5 R 1575428543 888275 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 1953 1 0 5 1778 0 21 0 1460
V5 R 1575428543 889986 172.18.34.6:48428 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 98 311 0 0 0 145 0 0 0 1460
V5 R 1575428543 890193 172.18.34.6:48422 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 593 1 0 6 399 0 39 0 1460
V5 R 1575428543 890230 172.18.34.6:48418 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 11 2046 1 0 6 1855 0 39 0 1460
V5 R 1575428543 889032 172.18.34.6:48424 10.81.212.8:3306 0.0.0.0 172.18.34.8:3306:392396 1506 5406 0 0 2 5251 0 919 0 1460
</code></pre><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/c6389d75e8469fc75e5005d0311c9524.png" alt="image.png"></p>
<p>格式<a href="https://www.atatech.org/articles/68880" target="_blank" rel="noopener">说明</a></p>
<p>DebugFS，顾名思义，是一种用于内核调试的虚拟文件系统，内核开发者通过debugfs和用户空间交换数据。类似的虚拟文件系统还有procfs和sysfs等，这几种虚拟文件系统都并不实际存储在硬盘上，而是Linux内核运行起来后才建立起来。</p>
<p>通常情况下，最常用的内核调试手段是printk。但printk并不是所有情况都好用，比如打印的数据可能过多，我们真正关心的数据在大量的输出里不是那么一目了然；或者我们在调试时可能需要修改某些内核变量，这种情况下printk就无能为力，而如果为了修改某个值重新编译内核或者驱动又过于低效，此时就需要一个临时的文件系统可以把我们需要关心的数据映射到用户空间。在过去，procfs可以实现这个目的，到了2.6时代，新引入的sysfs也同样可以实现，但不论是procfs或是sysfs，用它们来实现某些debug的需求，似乎偏离了它们创建的本意。比如procfs，其目的是反映进程的状态信息；而sysfs主要用于Linux设备模型。不论是procfs或是sysfs的接口应该保持相对稳定，因为用户态程序很可能会依赖它们。当然，如果我们只是临时借用procfs或者sysfs来作debug之用，在代码发布之前将相关调试代码删除也无不可。但如果相关的调试借口要在相当长的一段时间内存在于内核之中，就不太适合放在procfs和sysfs里了。故此，debugfs应运而生。</p>
<p>[toc]<br>原版链接: <a href="http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt" target="_blank" rel="noopener">http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt</a>  </p>
<h1 id="TCPRT介绍-1"><a href="#TCPRT介绍-1" class="headerlink" title="TCPRT介绍"></a>TCPRT介绍</h1><p>TCPRT是一个面向web的从tcp层的角度监控和分析网络服务质量和网络覆盖的超轻量级的分析工具；<br>TCPRT在单机进出流量超过10G，单机4w qps的情况下， cpu消耗不超过1%；<br>经过不断的演化，TCPRT现在具有两个功能：<br>1.网络服务质量的实时监控；包括 用户的请求文件的平均响应时间（rt），平均服务器响应时间（servertime），平均时延rtt，平均丢包率 等；<br>2.详细的网络服务质量数据采集：包括用户访问的每一条请求的响应时间（rt），服务器响应时间（servertime），时延rtt，丢包率等；  </p>
<p>其中 rt的含义是指从 服务器接收到请求到服务器发送给用户的数据包全部被确认所花费的时间；<br>servertime的含义是 从服务器接收到请求到服务器发送第一个数据包的时间；   （如果请求的是小文件，这个时间==应用服务器的rt）    </p>
<p><strong>目前增加了一版新架构实现的TCPRT，在功能上增加了域名粒度的实时访问数据。即可以查看一个周期内访问某个特定域名的rt,servertime rtt 等数据，并且方便更新。</strong></p>
<h2 id="1-TCPRT的实现原理"><a href="#1-TCPRT的实现原理" class="headerlink" title="1. TCPRT的实现原理"></a>1. TCPRT的实现原理</h2><p>tcprt采集的是主要有两方面的数据  </p>
<p>一方面是用户下载文件实际所需要的时间，<br>在现在的情况下，对于web用户来说，每个请求都是串行的每个请求下载完以后，<br>才会有下一个请求，所以，我们只需要记录请求到达服务器的时间和发送的所有数据被确认的时间，<br>就能知道这个请求用户实际上花了多长时间去下载，<br>后续如果pipeline 普及的话，很抱歉，基于网络四层的统计方法都要失效；    </p>
<p>另一方面是网络覆盖的数据，主要包括丢包个数、发送的有效数据、rtt（数据包一来一回的时间）；<br>tcprt利用的是内核的拥塞控制模块来采集数据的，tcp协议栈处理过程中，<br>在很多特殊事件会调用拥塞控制模块，拥塞控制模块的处理函数又可以拿到这个连接的所有信息（sock），<br>这里面包含了丢包、rtt等信息，因此 tcprt不需要做任何的分析工作，只需要采集数据即可；<br>同时，触发 拥塞控制模块的事件中，又包括了几个我们需要的时间点，包括请求到达，<br>第一个数据包发送，连接关闭；所以我们只需要做简单的if判断就能够获取服务质量相关的数据；    </p>
<p><strong>新架构TCPRT 是在tcp 协议栈处理过程的几个地方添加了钩子，通过在钩子中做 rt 等数据的采集即可，<br>所以只有特定的内核才能支持。实现域名粒度的数据采集是通过对TCP数据包进行分析，获取其域名并在内核使用hash 链表进行数据的统计。</strong>  </p>
<h2 id="2-日志信息介绍"><a href="#2-日志信息介绍" class="headerlink" title="2. 日志信息介绍"></a>2. 日志信息介绍</h2><h3 id="2-1-原始版本"><a href="#2-1-原始版本" class="headerlink" title="2.1 原始版本"></a>2.1 原始版本</h3><p>tcprt会输出两种类型的日志  </p>
<ul>
<li><p>2.1.1 实时的监控数据<br>tcprt会把实时的监控日志打印到 /sys/kernel/debug/rt-network-real* 中，日志格式如下所示：  </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1379387910 all 80 332 5 55 80 2 27340 101712  </span><br><span class="line">时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数；</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>该日志，tsar还在用。</strong>    </p>
<p>~~2.详细的用户访问日志 tcprt把用户详细日志打印到 /sys/kernel/debug/rt-network-logs,    ~~  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">访问的结果分为三类， 访问成功，访问失败、没有数据输出，分别对应的日志 第一位为 r w n 如果用户访问成功 ，打出的日志格式如下：  </span><br><span class="line">r 1379063895 171.214.168.205:12486 80 24185 637 71 0 1 175 1024</span><br><span class="line">访问成功 时间 ip：port 服务器访问端口 文件大小 rt rtt 重传包个数 连接的第n个请求 首字节 mss</span><br><span class="line">w 1379063952 111.227.56.204:52025 80 174240 13134 88 11 28 6 0 2880</span><br><span class="line">访问失败 时间 ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 初始窗口 丢包个数 连接的第n个请求 首字节时间 总发送的文件大小</span><br><span class="line">n 1379063952 111.227.56.204:52025 80 1 2000</span><br><span class="line">访问没有数据返回 时间 ip：port 服务器访问端口 连接的第n个请求 等待时间</span><br></pre></td></tr></table></figure>
<h3 id="2-2-新架构TCPRT的日志格式"><a href="#2-2-新架构TCPRT的日志格式" class="headerlink" title="2.2 新架构TCPRT的日志格式"></a>2.2 新架构TCPRT的日志格式</h3><h4 id="2-2-1-实时的监控数据"><a href="#2-2-1-实时的监控数据" class="headerlink" title="2.2.1 实时的监控数据"></a>2.2.1 实时的监控数据</h4><p>tcprt会把实时的监控日志打印到 /sys/kernel/debug/tcp-watch-real* 中，下面的日志都是一个周期内（比如一分钟）的平均值  </p>
<h5 id="2-2-1-1-输出到-sys-kernel-debug-tcp-watch-real-文件中"><a href="#2-2-1-1-输出到-sys-kernel-debug-tcp-watch-real-文件中" class="headerlink" title="2.2.1.1 输出到/sys/kernel/debug/tcp-watch-real*文件中"></a>2.2.1.1 输出到/sys/kernel/debug/tcp-watch-real*文件中</h5><p><strong>这份文件，tengine读，采集给heka，到天眼业务状态的外网丢包率和内网丢包率页面。</strong>  </p>
<ul>
<li>2.2.1.1.1 基于端口的日志格式如下所示  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1379387910 all-new 80 332 5 55 80 2 27340 101712 0 0 23 120</span><br><span class="line">时间 所有域名 端口 rt servertime 丢包率 rtt 失败率 平均文件大小 这段时间内的访问次数 https 建连时间 https 响应时间 srtt(毫秒) rtt方差(毫秒)</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.1.1.2 基于域名的日志格式如下所示  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1408327819 gdz03.md.alicdn.com 21 13 0 0 0 0 1 0 20182 32 1 23 120</span><br><span class="line">时间 域名 RT 首字节时间 上传时间 https建连 https响应 丢包率 rtt 失败率 每个请求的平均文件大小 这段时间内请求个数 连接个数 平均建连时间 首播时间 srtt(毫秒) rtt方差(毫秒)</span><br></pre></td></tr></table></figure>
</code></pre><h5 id="2-2-1-2-输出到-sys-kernel-debug-tcp-watch-back-文件中"><a href="#2-2-1-2-输出到-sys-kernel-debug-tcp-watch-back-文件中" class="headerlink" title="2.2.1.2 输出到/sys/kernel/debug/tcp-watch-back*文件中"></a>2.2.1.2 输出到/sys/kernel/debug/tcp-watch-back*文件中</h5><ul>
<li>2.2.1.2.1 回源到C段的 日志格式  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1408329123 140.205.134 0 136 693 136 </span><br><span class="line">时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）</span><br></pre></td></tr></table></figure>
</code></pre><h5 id="2-2-1-3-输出到-sys-kernel-debug-tcp-watch-front-文件中"><a href="#2-2-1-3-输出到-sys-kernel-debug-tcp-watch-front-文件中" class="headerlink" title="2.2.1.3 输出到/sys/kernel/debug/tcp-watch-front*文件中"></a>2.2.1.3 输出到/sys/kernel/debug/tcp-watch-front*文件中</h5><ul>
<li>2.2.1.3.1 从L2输出到L1的C段日志格式  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1408329123 140.205.134 0 136 693 136</span><br><span class="line">时间 IP 前三段 总重传个数 总包个数 总rtt时间 总rtt个数（总连接个数）</span><br></pre></td></tr></table></figure>
</code></pre><h4 id="2-2-2-详细的用户访问日志"><a href="#2-2-2-详细的用户访问日志" class="headerlink" title="2.2.2 详细的用户访问日志"></a>2.2.2 详细的用户访问日志</h4><p>tcprt把用户详细日志打印到 /sys/kernel/debug/tcp-watch-log*,访问的结果分为五类，<br>访问成功、访问失败、没有数据输出、每连接数据统计、回源数据，<br>分别对应的日志 第一位为 r_new w_new n_new c_new back如果用户访问成功 ，打出的日志格式如下：  </p>
<ul>
<li>2.2.2.1 r_new 访问成功  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">r_new 1408283588 img02.taobaocdn.com 115.238.23.16:34375 81 77480 145 1 0 8 57 0 112 1448</span><br><span class="line">访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss</span><br><span class="line">更新：</span><br><span class="line">r_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 14 1 0 1 3 0 115 1460 1048854 14 3 0 1 </span><br><span class="line">访问成功 时间 请求域名 ip：port 服务器访问端口 下载文件大小 rt rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 mss 连接建立到目前发送数据量 纯数据发送时间 首播时间 空等时间总和 tos</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.2 w_new 访问失败   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">w_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1740 13134 88 11 2 6 0 112 2880</span><br><span class="line">访问失败 时间 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小</span><br><span class="line">更新： </span><br><span class="line">w_new 1486624545 cdn.hc.org 115.238.23.194:22086 443 0 5 0 0 2 5 40 174 294 129 6 0 0 2021 1 </span><br><span class="line">访问失败 请求域名ip:port 服务器访问端口 被确认的文件大小 总下载时间 rtt 重传包个数 连接的第n个请求 首字节时间 上传时间 上传文件大小 已发送未确认文件大小 连接建立以来确认的字节数 纯数据发送时间 首播时间 空等时间 写缓存排队字节量 tos </span><br><span class="line">$1 w_new 访问失败 </span><br><span class="line">$2 1486624545 时间 </span><br><span class="line">$3 cdn.hc.org 域名 </span><br><span class="line">$4 115.238.23.194:22086 ip:port </span><br><span class="line">$5 443 服务器访问端口 </span><br><span class="line">$6 0 被确认文件大小 </span><br><span class="line">$7 5 总下载时间 </span><br><span class="line">$8 0 rtt </span><br><span class="line">$9 0 重传包个数 </span><br><span class="line">$10 2 连接的第n个请求 </span><br><span class="line">$11 5 首字节时间 </span><br><span class="line">$12 40 上传时间 </span><br><span class="line">$13 174 上传文件大小 </span><br><span class="line">$14 294 已发送未确认文件大小 </span><br><span class="line">$15 129 连接建立以来确认的字节数 </span><br><span class="line">$16 6 纯数据发送时间 </span><br><span class="line">$17 0 首播时间 </span><br><span class="line">$18 0 空等时间 </span><br><span class="line">$19 2021 写缓存排队字节量 </span><br><span class="line">$20 1 tos</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.3 n_new 访问没有数据返回  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_new 1379063952 img02.taobaocdn.com 111.227.56.204:52025 80 1 4 200 2000</span><br><span class="line">访问没有数据返回 时间 请求域名 ip:port 服务器访问端口 连接的第n个请求 上传时间 上传文件大小 等待时间</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.4 c_new 建连数据  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c_new 1408327083 cdn.hc.org 115.238.23.16:40323 81 193 0 0 1 57 6 0 0 1448</span><br><span class="line">连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss</span><br><span class="line">更新：</span><br><span class="line">c_new 1484190246 192.168.122.118 192.168.122.77:56966 80 1048854 0 1 1 116 0 0 0 1460 14 1 </span><br><span class="line">连接数据 时间 访问域名 ip:port 服务器访问端口 下载文件大小 重传个数 rtt 请求个数 上传数据大小 建连时间 https建连时间 https响应时间 mss 纯数据发送时间 tos</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>2.2.2.5 back 回源请求   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">back 1408327455 140.205.134.10:80 33484 290 0 5 1452</span><br><span class="line">回源请求 时间 ip:port 请求端口 发送大小 重传个数 rtt mss</span><br></pre></td></tr></table></figure>
</code></pre><h4 id="2-2-3-proc-tcp-rt-file中是基于域名的实时首字节展示"><a href="#2-2-3-proc-tcp-rt-file中是基于域名的实时首字节展示" class="headerlink" title="2.2.3 /proc/tcp_rt_file中是基于域名的实时首字节展示"></a>2.2.3 /proc/tcp_rt_file中是基于域名的实时首字节展示</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1408327819 total_statis 1061629323 81324204</span><br><span class="line">统计开始时间 所有请求的统计 总首字节时间 总请求数 </span><br><span class="line">1408327819 cb.alimama.cn 2268 1463</span><br><span class="line">统计开始时间 域名 总首字节时间 总请求数</span><br></pre></td></tr></table></figure>
<h3 id="2-3-支持主动查询的日志格式"><a href="#2-3-支持主动查询的日志格式" class="headerlink" title="2.3 支持主动查询的日志格式"></a>2.3 支持主动查询的日志格式</h3><h4 id="2-3-1-支持方式"><a href="#2-3-1-支持方式" class="headerlink" title="2.3.1 支持方式"></a>2.3.1 支持方式</h4><p>提供getsockopt接口，参数level＝SOL_TCP, optionname＝TCP_OPT_TCPRT,数据定义如下：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#ifndef SOL_TCP</span><br><span class="line">#define SOL_TCP 6</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#define TCP_OPT_TCPRT 21</span><br><span class="line"></span><br><span class="line">struct tcprt_info &#123;</span><br><span class="line">    __u32	tcpi_srtt;                    /* 当前协议栈计算出的平滑rtt */</span><br><span class="line">	__u32	tcpi_send_bytes;              /* 发送数据大小(byte), 区间值，每调用一次，都表示和上次调用间隔这段区间的发送数据大小（不包含重传数据）*/</span><br><span class="line">	__u32	tcpi_retrans_segs;            /* 重传包数，区间值，每调用一次，都表示和上次调用间隔这段区间的重传包数 */</span><br><span class="line">	__u32	tcpi_snd_mss;                 /* 发送端的tcp mss */</span><br><span class="line">	__u32	tcpi_wmem_queued;             /* 发送队列总大小(byte)，包含发送队列中skb负荷大小，以及sk_buff、sk_shared_info结构体、协议头的额外开销 */</span><br><span class="line">	__u32	tcpi_rcv_nxt;                 /* 本端希望接收的下一个字节序 */</span><br><span class="line">	__u32	tcpi_selective_rcvseq0;       /* tp-&gt;selective_acks[0].end_seq */</span><br><span class="line">	__u32	tcpi_min_cwnd;                /* 调用区间内的最小拥塞窗口 */</span><br><span class="line">	__u32	tcpi_min_wnd;                 /* 调用区间内的最小对端接收窗口	*/</span><br><span class="line">	__u32	tcpi_max_inflight;            /* 调用区间内的最大发送但未确认包数 */</span><br><span class="line">	</span><br><span class="line">	__u8	tcpi_max_backoff;             /* 调用区间内发生的 RTO 最大退避次数 */</span><br><span class="line">	__u8	tcpi_backoff;                 /* 最近一次发生的 RTO 退避次数*/</span><br><span class="line">	__u8	tcpi_state;                   /* 当前tcp 栈的状态机 */</span><br><span class="line">	__u8	tcpi_ca_state;                /* 当前tcp 栈的拥塞状态机 */</span><br><span class="line">	</span><br><span class="line">	__u8	tcpi_probes;                  /* 当前正在发生零窗口探测的次数 */</span><br><span class="line">	__u8	tcpi_retransmits;             /* 当前重传定时器正在触发的次数 */</span><br><span class="line">	__u8	tcpi_options;                 /* tcp 会话协商的tcp 选项 */</span><br><span class="line">	__u8	tcpi_snd_wscale : 4, tcpi_rcv_wscale : 4;  /* 发送端，接收端通告的 ws 大小 */</span><br><span class="line"></span><br><span class="line">    __u16	tcpi_pad;</span><br><span class="line">	__u16	tcpi_rinit_maxmss;            /* tcp 会话协商的对端初始最大 mss */</span><br><span class="line">	__u32	tcpi_rinit_maxrwnd;           /* tcp 会话协商的对端初始最大接收窗口*/</span><br><span class="line"></span><br><span class="line">	__u32	tcpi_cwnd;                   /* 当前协议栈的拥塞窗口 */</span><br><span class="line">	__u32	tcpi_snd_wnd;                /* 当前协议栈的对端接收窗口 */</span><br><span class="line">	__u32	tcpi_inflight;               /* 当前发送未确认包的数目 */</span><br><span class="line">	__u32	tcpi_srttvar;                /* 最近采样的 rtt 平均偏差， 用来衡量 rtt 的抖动*/</span><br><span class="line">	__u32	tcpi_max_srttvar;            /* 采样周期内的最大 mdev */</span><br><span class="line">	__u32	tcpi_minrtt;                 /* 全局最小采样rtt */</span><br><span class="line"></span><br><span class="line">	__u32	tcpi_minsrtt;                /* 调用区间内的最小平滑rtt */</span><br><span class="line">	__u32	tcpi_min_rto;                /* 调用区间内的最小 rto */</span><br><span class="line">	__u32	tcpi_max_rto;                /* 调用区间内的最大 rto */</span><br><span class="line"></span><br><span class="line">	__u32	tcpi_max_wnd;	             /* 调用区间内的最大期望接收窗口	*/</span><br><span class="line">	__u32	tcpi_min_rewnd;              /* 调用区间内的最小剩余对端可用接收窗口 */</span><br><span class="line">	__u32	tcpi_max_rewnd;              /* 调用区间内的最大剩余对端可用接收窗口 */</span><br><span class="line">	__u32	tcpi_min_recwnd;             /* 调用区间内的最小剩余发送窗口 */</span><br><span class="line">	__u32	tcpi_max_recwnd;             /* 调用区间内的最大剩余发送窗口 */</span><br><span class="line">	__u32	tcpi_min_sendq_bytes;        /* 调用区间内发送队列排队的数据包大小(bytes) */</span><br><span class="line">	</span><br><span class="line">	__u32	tcpi_sample_intv;	         /* 采样间隔 */</span><br><span class="line">	__u32	tcpi_delivery_rate;          /* 调用区间内的发送速率 */</span><br><span class="line">	__u32	tcpi_ack_bytes;              /* 调用区间内对端累积确认的字节数 */</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h4 id="2-3-2-使用示例"><a href="#2-3-2-使用示例" class="headerlink" title="2.3.2 使用示例"></a>2.3.2 使用示例</h4><p>每次调用返回前次调用到现在的统计数据，使用示例：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">int fd;</span><br><span class="line">struct tcprt_info info;</span><br><span class="line">unsigned optlen = sizeof(info);</span><br><span class="line">int ret;</span><br><span class="line">fd = socket(PF_INET, SOCK_STREAM, 0);</span><br><span class="line">ret = getsockopt(fd, SOL_TCP, TCP_OPT_TCPRT, &amp;info, &amp;optlen);</span><br><span class="line">if(ret &lt; 0)&#123;</span><br><span class="line">    fprintf(stderr, &quot;getsockopt return %d\n&quot;, ret);</span><br><span class="line">    return ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1. 每调用一次getsockopt，该sk相关的tcprt测量值，都会重新赋值一次 ！这个接口是用来获取区间值的！  </span><br><span class="line">2. 线上端口配置，默认配置 1935 1936 1937 80端口，目前只允许出现在live_port_arr配置项，否则会影响采集数据的其它逻辑 ！</span><br><span class="line">3. live_port_arr 最大配置端口数 10个，配置文件/etc/tcp_watch.conf;如果需要配置增加端口，可联系网络团队支持</span><br></pre></td></tr></table></figure>
<p>在tengine中，可以使用上述方式来获取网络监控数据   </p>
<h2 id="3-部署和使用"><a href="#3-部署和使用" class="headerlink" title="3. 部署和使用"></a>3. 部署和使用</h2><p>安装： sudo yum install t-cdn-tcprt -b test<br>开启： /etc/init.d/tcprt start<br>停止： /etc/init.d/tcprt stop  </p>
<h3 id="3-1-老版本使用方案"><a href="#3-1-老版本使用方案" class="headerlink" title="3.1 老版本使用方案"></a>3.1 老版本使用方案</h3><p>启动后，请根据你所属的网络和应用配置如下参数：</p>
<ul>
<li><p>3.1.1 机器是否在 fullnat后面：<br>/sys/module/tcp_rt_base/parameters/nat 如果在fullnat 后面 建议设成1；<br>echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat<br>如果不在fullnat 后面要设成0 否者的话取不到数据<br>echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat  </p>
</li>
<li><p>3.1.2 你需要监听的端口，tcprt现在支持 最多3个端口的监控， </p>
</li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/sys/module/tcp_rt_statis/parameters/port_nummber 设置 需要监控的端口数 </span><br><span class="line">/sys/module/tcp_rt_statis/parameters/port0（1、2） 为需要监控的端口</span><br></pre></td></tr></table></figure>
</code></pre><p>比如，我需要监控 80 81 82 3个端口，那么可以这么设置  </p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo 80 &gt; /sys/module/tcp_rt_statis/parameters/port0</span><br><span class="line">echo 81 &gt; /sys/module/tcp_rt_statis/parameters/port1</span><br><span class="line">echo 82 &gt; /sys/module/tcp_rt_statis/parameters/port2</span><br><span class="line">echo 3 &gt; /sys/module/tcp_rt_statis/parameters/port_nummber</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>3.1.3 数据采集的间隔：   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/sys/module/tcp_rt_statis/parameters/check_interval 如果你希望 1分钟统计一次</span><br><span class="line">echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</span><br></pre></td></tr></table></figure>


建议不要低于1分钟    
</code></pre><p><strong>以上配置都需要sudo 权限；</strong>  </p>
<ul>
<li>3.1.4 tcprt提供了两个配置范例<br>分别用于cdn和源站   </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/tcprt/cdn.sh</span><br><span class="line">echo 0 &gt; /sys/module/tcp_rt_base/parameters/nat</span><br><span class="line">echo 60 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/tcprt/source.sh</span><br><span class="line">echo 1 &gt; /sys/module/tcp_rt_base/parameters/nat</span><br><span class="line">echo 300 &gt; /sys/module/tcp_rt_statis/parameters/check_interval</span><br><span class="line">/usr/local/tcprt/tcp_rt_a_log_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp;   </span><br><span class="line">/usr/local/tcprt/tcp_rt_a_real_copy.sh &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
</code></pre><p>tcprt 还提供了两个日志采集的范例脚本 分别在：<br>/usr/local/tcprt/tcp_rt_a_log_copy.sh /usr/local/tcprt/tcp_rt_a_real_copy.sh<br>分别是 详细日志和实时监控日志的采集脚本；<br>如果单机的qps数很高， 强烈建议不要用/usr/local/tcprt/tcp_rt_a_log_copy.sh 采集实时日志；<br>日志采集的数据会把数据存到 /home/admin/logs/tcprt/ 目录下 实时监控日志 会存到/home/admin/logs/tcp_rt/tcp_rt.statis文件里；</p>
<h3 id="3-2-新版本使用方案"><a href="#3-2-新版本使用方案" class="headerlink" title="3.2 新版本使用方案"></a>3.2 新版本使用方案</h3><p>你需要监听的端口，tcprt现在支持 最多6个端口的监控，假如需要监听80 443 81 三个端口可以执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 80,443,81 &gt;/sys/module/tcp_watch/parameters/port_arr</span><br></pre></td></tr></table></figure>
<p>日志的采集可以直接使用  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/kernel/debug/tcp-watch-log* &gt; logfile , </span><br><span class="line">cat /sys/kernel/debug/tcp-watch-real* &gt; realfile</span><br></pre></td></tr></table></figure>
<p>tcp-watch-log* tcp-watch-real*中的数据只能读取一次，读取后不存在了。  </p>
<h1 id="4-Q-amp-A"><a href="#4-Q-amp-A" class="headerlink" title="4. Q&amp;A"></a>4. Q&amp;A</h1><ul>
<li>4.1 setsockopt(c-&gt;fd, IPPROTO_TCP, TCP_DOMAIN, peer-&gt;host.data, peer-&gt;host.len); 失败  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1）检查uname -r，是不是2.6.32-220.23.2.ali878.tcp1.34.el6.x86_64或2.6.32-220.23.2.ali878.tcp1.54.el6.x86_64，如果不是，则不是常规部署的cdn tcpkernel。</span><br><span class="line">      tcprt需适配这两个特定内核版本。其余版本的支持情况，需要人工check。         </span><br><span class="line">2）setDOMAIN需要tcprt hotfix版本的支持。检查t-cdn-tcprt版本，是不是最新的，否则更新。  </span><br><span class="line">      http://rpm.corp.taobao.com/find.php?q=t-cdn-tcprt，找test/最先版的。  </span><br><span class="line">      部署和使用: http://baike.corp.taobao.com/index.php/CS_RD/CDN/AppAccelerate/tcprt。    </span><br><span class="line">3）检查配置端口。</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>4.2 tcprt采集的rtt  </li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcprt采集以请求为单位，rtt是该次请求中，出现过的最小rtt值。</span><br></pre></td></tr></table></figure>
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/答疑问题汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/答疑问题汇总/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/答疑问题汇总/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/答疑问题汇总/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="答疑问题汇总"><a href="#答疑问题汇总" class="headerlink" title="答疑问题汇总"></a>答疑问题汇总</h1><hr>
<h2 id="天津一网通-vxlan-网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包"><a href="#天津一网通-vxlan-网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包" class="headerlink" title="天津一网通 vxlan 网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包"></a>天津一网通 vxlan 网络始终不通，宿主机能抓到发出去的包，但是抓不到回复包。对端容器所在的宿主机抓不到进来的包</h2><blockquote>
<p>一定是网络上把这个包扔掉了</p>
</blockquote>
<h4 id="证明问题"><a href="#证明问题" class="headerlink" title="证明问题"></a>证明问题</h4><ul>
<li>先选择两台宿主机，停掉上面的 ovs 容器(腾出4789端口)</li>
<li>一台宿主机上执行： nc -l -u 4789 //在4789端口上启动udp服务</li>
<li>另外一台主机上执行： nc -u 第一台宿主机的IP 4789 //从第二台宿主机连第一台的4789端口</li>
<li>从两边都发送一些内容看看，看是否能到达对方</li>
</ul>
<p><strong>如果通过nc发送的东西也无法到达对方（跟方舟没有关系了）那么就是链路上的问题</strong></p>
<hr>
<h2 id="天津一网通-vxlan-网络能通，但是pca容器初始化的时候失败"><a href="#天津一网通-vxlan-网络能通，但是pca容器初始化的时候失败" class="headerlink" title="天津一网通 vxlan 网络能通，但是pca容器初始化的时候失败"></a>天津一网通 vxlan 网络能通，但是pca容器初始化的时候失败</h2><p>通过报错信息发现pca容器访问数据库SocketTimeout，同时看到异常信息都是Timeout大于15分钟以上了。</p>
<h4 id="需找问题"><a href="#需找问题" class="headerlink" title="需找问题"></a>需找问题</h4><ul>
<li>先在 pca容器和数据库容器互相 ping 证明网络没有问题，能够互通</li>
<li>在 pca 容器中通过mysql 命令行连上 mysql，并创建table，insert一些记录，结果也没有问题</li>
<li>抓包发现pca容器访问数据库的时候在重传包（以往经验）</li>
</ul>
<p><img src="http://img4.tbcdn.cn/L1/461/1/1d010b9937198aee9e798bb02913603874f19ddc" alt="screenshot"></p>
<h4 id="细化证明问题"><a href="#细化证明问题" class="headerlink" title="细化证明问题"></a>细化证明问题</h4><ul>
<li>ping -s -M 尝试发送1460大小的包</li>
<li>检查宿主机、容器MTU设置</li>
</ul>
<p><strong>确认问题在宿主机网卡MTU设置为1350</strong>，从而导致容器发出的包被宿主机网卡丢掉</p>
<h2 id="居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping-这个域名能通，但是nslookup解析不了这个域名"><a href="#居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping-这个域名能通，但是nslookup解析不了这个域名" class="headerlink" title="居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping 这个域名能通，但是nslookup解析不了这个域名"></a>居然之家通过vpn部署好中间件后，修改笔记本的dns设置后通过浏览器来访问中间件的console，但是报找不到server。同时在cmd中ping 这个域名能通，但是nslookup解析不了这个域名</h2><p>ping 这个域名能通，但是nslookup不行，基本可以确认网络没有大问题，之所以ping可以nslookup不行，是因为他们底层取dns server的逻辑不一样。</p>
<p>先检查dns设置：</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d4634f74c0b0b38f784a1657864d5089.png" alt="image.png"><br>如上图，配置的填写</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d4a9cddf56d23059f98850c7c0bcf067.png" alt="image.png"></p>
<p>多出来一个127.0.0.1肯定有问题，明明配置的时候只填了114.114.114.114. nslookup、浏览器默认把域名解析丢给了127.0.0.1，但是 ping丢给了114.114.114.114，所以看到如上描述的结果。</p>
<p>经过思考发现应该是本机同时运行了easyconnect（vpn软件），127.0.0.1 是他强行塞进来的。马上停掉easyconnect再ipconfig /all 验证一下这个时候的dns server，果然127.0.0.1不见了。</p>
<h2 id="中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns-server-ip-但是nslookup-解析不了域名，显示request-time-out"><a href="#中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns-server-ip-但是nslookup-解析不了域名，显示request-time-out" class="headerlink" title="中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns server ip, 但是nslookup 解析不了域名，显示request time out"></a>中航信windows下通过方舟dns域名解析不了方舟域名，但是宿主机上可以。windows机器能ping通dns server ip, 但是nslookup 解析不了域名，显示request time out</h2><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/548975c04a8587e0fb33b5722b1a38f2.png" alt="image.png"></p>
<p>能ping通说明网络能通，但是dns域名要能解析依赖于：</p>
<ul>
<li>网络能通</li>
<li>dns server上有dns服务（53udp端口）</li>
<li>中间的防火墙对这个udp53端口放行了</li>
</ul>
<p>这里的问题非常明显是中间的防火墙没放行 udp 53端口</p>
<h2 id="方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping-域名不通，但是nslookup能通"><a href="#方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping-域名不通，但是nslookup能通" class="headerlink" title="方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping 域名不通，但是nslookup能通"></a>方舟环境在ECS底座上DNS会继承rotate模式，导致域名解析不正常，ping 域名不通，但是nslookup能通</h2><p><a href="https://www.atatech.org/articles/93688" target="_blank" rel="noopener">nslookup 域名结果正确，但是 ping 域名失败</a></p>
<h2 id="民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包"><a href="#民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包" class="headerlink" title="民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包"></a>民生银行POC环境物理机搬迁到新机房后网络不通，通过在物理机上抓包，抓不到任何容器的包</h2><p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/896f8f14d3be725515f192ed64542cb0.png" alt="image.png"></p>
<p><strong>如图所示容器中发了 arp包（IP 10.100.2.2 寻找10.100.2.1 的mac地址），这个包从bond0 网卡发出去了，也是带的正确的 vlanid 1011，但是交换机没有回复，那么就一定是交换机上vlan配置不对，需要找分配这个vlan的网工来检查交换机的配置</strong></p>
<font color="red" size="4"><strong>能抓到进出的容器包–外部环境正确，方舟底座的问题</strong></font>

<font color="red" size="4"><strong>不能抓到出去的容器包–方舟底座的问题</strong></font>

<font color="red" size="4"><strong>能抓到出去的容器包，抓不到回来的包–外部环境的问题</strong></font>

<p>所以这里是方舟底座的问题。检查ovs、vlan插件一切都正常，见鬼了</p>
<p>检查宿主机网卡状态，发现没插网线，<strong>如果容器所用的宿主机网卡没有插网线，那么ovs不会转发任何包到宿主机网卡</strong>。</p>
<h2 id="一台应用服务器无法访问部分drds-server"><a href="#一台应用服务器无法访问部分drds-server" class="headerlink" title="一台应用服务器无法访问部分drds-server"></a><a href="https://aone.alibaba-inc.com/task/9753887" target="_blank" rel="noopener">一台应用服务器无法访问部分drds-server</a></h2><p>应用机器： 10.100.10.201 这台机器抛502异常比较多，进一步诊断发现 ping youku.tddl.tbsite.net 的时候解析到 10.100.53.15/16就不通</p>
<p>直接ping 10.100.53.15/16 也不通，经过诊断发现是交换机上记录了两个 10.100.10.201的mac地址导致网络不通。</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/9deff3045e3213df81c3ad785cfddefa.gif" alt="youku-mac-ip.gif"></p>
<p><strong>上图是不通的IP，下图是正常IP</strong></p>
<p>经过调查发现是土豆业务也用了10.100.10.201这个IP导致交换机的ARP mac table冲突，土豆删除这个IP后故障就恢复了。</p>
<h3 id="当时交换机上发现的两条记录："><a href="#当时交换机上发现的两条记录：" class="headerlink" title="当时交换机上发现的两条记录："></a>当时交换机上发现的两条记录：</h3><pre><code>00:18:51:38:b1:cd 10.100.10.201 
8c:dc:d4:b3:af:14 10.100.10.201
</code></pre><h2 id="针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask"><a href="#针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask" class="headerlink" title="针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask"></a>针对中移在线这个客户默认修改了umask导致黑屏脚本权限不够，部署中途不断卡壳，直接在黑屏脚本中修复了admin这个用户的umask</h2><ol>
<li>客户环境的 umask 是 0027 会导致所有copy文件的权限都不对了</li>
<li>因为admin没权限执行 /bin/jq 导致daemon.json是空的</li>
<li>/etc/docker/daemon.json 文件是空的，docker启动报错</li>
</ol>
<h2 id="修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复"><a href="#修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复" class="headerlink" title="修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复"></a>修复centos下udp和批量处理脚本因为环境变量的确实不能执行modprobe和ping等等命令的问题，同时将alios的这块修复逻辑放到了方舟安装脚本中，init的时候会先把这个问题修复</h2><p><a href="https://www.atatech.org/articles/105673" target="_blank" rel="noopener">Linux环境变量问题汇总</a></p>
<h2 id="Centos系统重启后-etc-resolv-conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新-etc-resolv-conf"><a href="#Centos系统重启后-etc-resolv-conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新-etc-resolv-conf" class="headerlink" title="Centos系统重启后 /etc/resolv.conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新 /etc/resolv.conf"></a>Centos系统重启后 /etc/resolv.conf总是被还原，开始以为是系统Bug，研究后发现是可以配置的，dhcp默认会每次重启后拉取DNS自动更新 /etc/resolv.conf</h2><h2 id="MonkeyKing-burn-cpu-mkt-burncpu-sh-脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。"><a href="#MonkeyKing-burn-cpu-mkt-burncpu-sh-脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。" class="headerlink" title="MonkeyKing burn cpu:  mkt-burncpu.sh 脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。"></a>MonkeyKing burn cpu:  mkt-burncpu.sh 脚本在方舟服务器上运行一段时间后，进程不见了，MK团队认为是方舟杀掉了他们。</h2><p>好奇心看代码、<strong>看openssl测试输出日志</strong>：内部调用 openssl speed 测试cpu的速度，这个测试一轮跑完了opessl就结束了，本身不是死循环一直跑, 不是方舟杀掉的</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/7860a67e52c0de0febd7ec944a4b1517.png" alt="image.png"></p>
<h2 id="重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常"><a href="#重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常" class="headerlink" title="重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常"></a>重庆长安汽车部署过程中愚公不能正常启动，怀疑是依赖的zk问题，zk网络访问正常</h2><p>尝试telnet zk发现不通，客户现场安装了kerberos导致telnet测试有问题（telnet被kerberos替换过）,换一个其他环境的telnet就可以了（md5sum、telnet –help）</p>
<h2 id="开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器"><a href="#开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器" class="headerlink" title="开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器"></a><a href="https://aone.alibaba-inc.com/issue/10403085" target="_blank" rel="noopener">开发反应两个容器之间的网络不稳定，偶尔报连不上某些容器</a></h2><p>主要是出现在tlog-console访问hbase容器的时候报连接异常</p>
<ol>
<li>在 task_1114_g0_tlog-console_tlog_1（10.16.11.131） 的56789 端口上启动了一个简单的http服务，然后从 task_1114_g0_tlog-hbase_tlog（10.16.11.108） 每秒钟去访问一下10.16.11.131:56789 , 如果丢包率很高的时候服务 10.16.11.131:56789 也很慢或者访问不到就是网络问题，否则就有可能是hbase服务不响应导致的丢包、网络不通（仅仅是影响hbase服务） </li>
<li>反过来在hbase上同样启动http服务，tlog-console不停地去get</li>
<li>整个过程我的http服务响应非常迅速稳定，从没出现过异常</li>
<li>在重现问题侯，贺飞发现 是tlog线程数目先增多，retran才逐渐增高的， retran升高，并没有影响在那台机器上ping 或者telnet hbase的服务</li>
<li>最终确认跟容器、网络无关，是应用本身的问题，交由产品开发继续解决</li>
</ol>
<h4 id="最终开发确认网络没有问题后一门心思闷头自查得出结论："><a href="#最终开发确认网络没有问题后一门心思闷头自查得出结论：" class="headerlink" title="最终开发确认网络没有问题后一门心思闷头自查得出结论："></a>最终开发确认网络没有问题后一门心思闷头自查得出结论：</h4><p>信息更新：</p>
<p>问题：<br>tlog-console进程线程数多，卡在连接hbase上的问题</p>
<p>直接原因：</p>
<ol>
<li>tlog-console有巡检程序，每m分钟会检查运行超过n秒的线程，并且中断这个线程； 这个操作直接导致hbase客户端在等待hbaseserver返回数据的时候被中断，这种中断会经常发生，累积久了，就会打爆tlog-console服务的线程数目，这时候，tlogconsole机器的retran就会变多，连接hbaseserver就会出问题， 具体的机理不明</li>
</ol>
<p>解决问题的有效操作：</p>
<ol>
<li>停止对tlog-console的巡检程序后，问题没有发生过</li>
</ol>
<p>其他潜在问题，这些问题是检查问题的时候，发现的其他潜在问题，已经反馈给tlog团队：<br>a. Htable实例不是线程安全,有逻辑多线程使用相同的htable实例<br>b. 程序中有new HTable 不close的路径</p>
<h2 id="中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares-128-相当于4Core-导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题"><a href="#中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares-128-相当于4Core-导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题" class="headerlink" title="中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares=128(相当于4Core), 导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题"></a>中国石化私有云DRDS扩容总是报资源不足，主要是因为有些drds-server容器指定了–cpu-shares=128(相当于4Core), 导致物理机CPU不够。现场将所有容器的–cpu-shares改成2后修复这个问题，但是最终需要产品方不要使用这个参数，不但会导致CPU不足，还会导致可能部分容器竞争不到CPU，进而出现很难排查的诡异问题</h2><h2 id="mq-diamond的异常日志总是打爆磁盘。mq-diamond-容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond-频繁输出日志，两天就用掉了1T磁盘"><a href="#mq-diamond的异常日志总是打爆磁盘。mq-diamond-容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond-频繁输出日志，两天就用掉了1T磁盘" class="headerlink" title="mq-diamond的异常日志总是打爆磁盘。mq-diamond 容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond 频繁输出日志，两天就用掉了1T磁盘."></a>mq-diamond的异常日志总是打爆磁盘。mq-diamond 容器一天输出500G日志的问题，本质是调用的依赖不可用了，导致mq-diamond 频繁输出日志，两天就用掉了1T磁盘.</h2><p>这里有两个问题需要处理：</p>
<ol>
<li>mq-diamond 依赖的服务可用； </li>
<li>mq-diamond 自身保护，不要被自己的日志把磁盘撑爆了  </li>
</ol>
<p>对于问题二修改log4j来保护；对于问题1查看异常内容，mq-diamond尝试连接server：ip1,ip2,ip3 正常这里应该是一个ip而不是三个ip放一起。判断是mq-diamond从mq-cai获取diamond iplist有问题，这个iplist应该放在三行，但是实际被放到了1行，用逗号隔开</p>
<p>手工修改这个文件，放到三行，问题没完，还是异常，我自己崩溃没管。最后听ma-diamond的开发讲他们取iplist的url比较特殊，是自己定义的，所以我修改的地方不起作用。<strong>反思，为什么异常的时候不去看看Nginx的access日志？</strong></p>
<h2 id="内核migration进程bug导致宿主机Load非常高，同时CPU-idle也很高（两者矛盾）"><a href="#内核migration进程bug导致宿主机Load非常高，同时CPU-idle也很高（两者矛盾）" class="headerlink" title="内核migration进程bug导致宿主机Load非常高，同时CPU idle也很高（两者矛盾）"></a>内核migration进程bug导致宿主机Load非常高，同时CPU idle也很高（两者矛盾）</h2><p><a href="https://aone.alibaba-inc.com/issue/12510664" target="_blank" rel="noopener">内核migration进程bug导致对应的CPU核卡死</a>（图一），这个核上的所有进程得不到执行（Load高，CPU没有任何消耗， 图二），直到内核进程 watchdog 发现这个问题并恢复它。</p>
<p>出现这个bug后的症状，通过top命令看到CPU没有任何消耗但是Load偏高，如果应用进程恰好被调度到这个出问题的CPU核上，那么这个进程会卡住（大概20秒）没有任何响应，比如 ping 进程（图三图四），watchdog恢复这个问题后，多个网络包在同一时间全部通。其实所影响的不仅仅是网络卡顿，中间件容器里面的服务如果调度到这个CPU核上同样得不到执行，从外面就是感觉容器不响应了</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/f4843725cf82e257fa14fd3742c2f9ce.png" alt="image.png"></p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/11d6db76c6de822385c0f63d2bf6eb03.png" alt="image.png"></p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/ac9e2eb1b01976cefa1b74dcddd23885.png" alt="image.png"></p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/371870b7db916e3edf515beec3a80bda.png" alt="image.png"></p>
<p>拿如上证据求助内核开发</p>
<p>关键信息在这里：<br>代码第297行<br>2017-09-15T06:52:37.820783+00:00 ascliveedas4.sgdc kernel: [598346.499872] WARNING: at net/sched/sch_generic.c:297 dev_watchdog+0x270/0x280()<br>2017-09-15T06:52:37.820784+00:00 ascliveedas4.sgdc kernel: [598346.499873] NETDEV WATCHDOG: ens2f0 (ixgbe): transmit queue 28 timed out</p>
<p>kernel version: kernel-3.10.0-327.22.2.el7.src.rpm</p>
<pre><code>265 static void dev_watchdog(unsigned long arg)
266 {
267 struct net_device *dev = (struct net_device *)arg;
268
269 netif_tx_lock(dev);
270 if (!qdisc_tx_is_noop(dev)) {
271 if (netif_device_present(dev) &amp;&amp;
272 netif_running(dev) &amp;&amp;
273 netif_carrier_ok(dev)) {
274 int some_queue_timedout = 0;
275 unsigned int i;
276 unsigned long trans_start;
277
278 for (i = 0; i &lt; dev-&gt;num_tx_queues; i++) {
279 struct netdev_queue *txq;
280
281 txq = netdev_get_tx_queue(dev, i);
282 /*
283  * old device drivers set dev-&gt;trans_start
284  */
285 trans_start = txq-&gt;trans_start ? : dev-&gt;trans_start;
286 if (netif_xmit_stopped(txq) &amp;&amp;
287 time_after(jiffies, (trans_start +
288  dev-&gt;watchdog_timeo))) {
289 some_queue_timedout = 1;
290 txq-&gt;trans_timeout++;
291 break;
292 }
293 }
294
295 if (some_queue_timedout) {
296 WARN_ONCE(1, KERN_INFO &quot;NETDEV WATCHDOG: %s (%s): transmit queue %u timed out\n&quot;,
297dev-&gt;name, netdev_drivername(dev), i);
298 dev-&gt;netdev_ops-&gt;ndo_tx_timeout(dev);
299 }
300 if (!mod_timer(&amp;dev-&gt;watchdog_timer,
301round_jiffies(jiffies +
302  dev-&gt;watchdog_timeo)))
303 dev_hold(dev);
304 }





$ cat  kernel_log.0915
2017-09-15T02:19:55.975310+00:00 ascliveedas4.sgdc kernel: [582026.288227] openvswitch: netlink: Key type 62 is out of range max 22
2017-09-15T03:49:41.312168+00:00 ascliveedas4.sgdc kernel: [587409.546584] md: md0: data-check interrupted.
2017-09-15T06:52:37.820782+00:00 ascliveedas4.sgdc kernel: [598346.499865] ------------[ cut here ]------------
2017-09-15T06:52:37.820783+00:00 ascliveedas4.sgdc kernel: [598346.499872] WARNING: at net/sched/sch_generic.c:297 dev_watchdog+0x270/0x280()
2017-09-15T06:52:37.820784+00:00 ascliveedas4.sgdc kernel: [598346.499873] NETDEV WATCHDOG: ens2f0 (ixgbe): transmit queue 28 timed out
2017-09-15T06:52:37.820784+00:00 ascliveedas4.sgdc kernel: [598346.499916] Modules linked in: 8021q garp mrp xt_nat veth xt_addrtype ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 iptable_filter xt_conntrack nf_nat nf_conntrack bridge stp llc tcp_diag udp_diag inet_diag binfmt_misc overlay() vfat fat intel_powerclamp coretemp intel_rapl kvm_intel kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd raid10 ipmi_devintf iTCO_wdt iTCO_vendor_support sb_edac lpc_ich hpwdt edac_core hpilo i2c_i801 ipmi_si sg mfd_core pcspkr ioatdma ipmi_msghandler acpi_power_meter shpchp wmi pcc_cpufreq openvswitch libcrc32c nfsd auth_rpcgss nfs_acl lockd grace sunrpc ip_tables ext4 mbcache jbd2 sd_mod crc_t10dif crct10dif_generic mgag200 syscopyarea sysfillrect sysimgblt drm_kms_helper ixgbe crct10dif_pclmul ahci ttm crct10dif_common igb crc32c_intel mdio libahci ptp drm pps_core i2c_algo_bit libata i2c_core dca dm_mirror dm_region_hash dm_log dm_mod
2017-09-15T06:52:37.820786+00:00 ascliveedas4.sgdc kernel: [598346.499928] CPU: 10 PID: 123 Comm: migration/10 Tainted: G L ------------ T 3.10.0-327.22.2.el7.x86_64#1
2017-09-15T06:52:37.820787+00:00 ascliveedas4.sgdc kernel: [598346.499929] Hardware name: HP ProLiant DL160 Gen9/ProLiant DL160 Gen9, BIOS U20 12/27/2015
2017-09-15T06:52:37.820788+00:00 ascliveedas4.sgdc kernel: [598346.499935]  ffff88207fc43d88 000000001cdfb0f1 ffff88207fc43d40 ffffffff816360fc
2017-09-15T06:52:37.820789+00:00 ascliveedas4.sgdc kernel: [598346.499939]  ffff88207fc43d78 ffffffff8107b200 000000000000001c ffff881024660000
2017-09-15T06:52:37.820790+00:00 ascliveedas4.sgdc kernel: [598346.499942]  ffff881024654f40 0000000000000040 000000000000000a ffff88207fc43de0
2017-09-15T06:52:37.820791+00:00 ascliveedas4.sgdc kernel: [598346.499943] Call Trace:
2017-09-15T06:52:37.820792+00:00 ascliveedas4.sgdc kernel: [598346.499952]  &lt;IRQ&gt;  [&lt;ffffffff816360fc&gt;] dump_stack+0x19/0x1b
2017-09-15T06:52:37.820794+00:00 ascliveedas4.sgdc kernel: [598346.499956]  [&lt;ffffffff8107b200&gt;] warn_slowpath_common+0x70/0xb0
2017-09-15T06:52:37.820795+00:00 ascliveedas4.sgdc kernel: [598346.499959]  [&lt;ffffffff8107b29c&gt;] warn_slowpath_fmt+0x5c/0x80
2017-09-15T06:52:37.820795+00:00 ascliveedas4.sgdc kernel: [598346.499964]  [&lt;ffffffff8154d4f0&gt;] dev_watchdog+0x270/0x280
2017-09-15T06:52:37.820796+00:00 ascliveedas4.sgdc kernel: [598346.499966]  [&lt;ffffffff8154d280&gt;] ? dev_graft_qdisc+0x80/0x80
2017-09-15T06:52:37.820797+00:00 ascliveedas4.sgdc kernel: [598346.499972]  [&lt;ffffffff8108b0a6&gt;] call_timer_fn+0x36/0x110
2017-09-15T06:52:37.820798+00:00 ascliveedas4.sgdc kernel: [598346.499974]  [&lt;ffffffff8154d280&gt;] ? dev_graft_qdisc+0x80/0x80
2017-09-15T06:52:37.820799+00:00 ascliveedas4.sgdc kernel: [598346.499977]  [&lt;ffffffff8108dd97&gt;] run_timer_softirq+0x237/0x340
2017-09-15T06:52:37.820800+00:00 ascliveedas4.sgdc kernel: [598346.499980]  [&lt;ffffffff81084b0f&gt;] __do_softirq+0xef/0x280
2017-09-15T06:52:37.820801+00:00 ascliveedas4.sgdc kernel: [598346.499985]  [&lt;ffffffff81103360&gt;] ? cpu_stop_should_run+0x50/0x50
2017-09-15T06:52:37.820801+00:00 ascliveedas4.sgdc kernel: [598346.499988]  [&lt;ffffffff8164819c&gt;] call_softirq+0x1c/0x30
2017-09-15T06:52:37.820802+00:00 ascliveedas4.sgdc kernel: [598346.499994]  [&lt;ffffffff81016fc5&gt;] do_softirq+0x65/0xa0
2017-09-15T06:52:37.820803+00:00 ascliveedas4.sgdc kernel: [598346.499996]  [&lt;ffffffff81084ea5&gt;] irq_exit+0x115/0x120
2017-09-15T06:52:37.820804+00:00 ascliveedas4.sgdc kernel: [598346.499999]  [&lt;ffffffff81648e15&gt;] smp_apic_timer_interrupt+0x45/0x60
2017-09-15T06:52:37.820805+00:00 ascliveedas4.sgdc kernel: [598346.500003]  [&lt;ffffffff816474dd&gt;] apic_timer_interrupt+0x6d/0x80
2017-09-15T06:52:37.820813+00:00 ascliveedas4.sgdc kernel: [598346.500007]  &lt;EOI&gt;  [&lt;ffffffff811033df&gt;] ? multi_cpu_stop+0x7f/0xf0
2017-09-15T06:52:37.820815+00:00 ascliveedas4.sgdc kernel: [598346.500010]  [&lt;ffffffff81103666&gt;] cpu_stopper_thread+0x96/0x170
</code></pre><h2 id="鸡汤"><a href="#鸡汤" class="headerlink" title="鸡汤"></a>鸡汤</h2><p><a href="https://www.zhihu.com/question/39430220/answer/81648584" target="_blank" rel="noopener">Do More， Do Better， Do exercise</a>（<strong>口号和实践</strong>）</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/dd8b77138555d5a23563f5691a60e2dd.png" alt="image.png"></p>
<h2 id="最后讲两个超级大Case：卡行天下、汇通天下"><a href="#最后讲两个超级大Case：卡行天下、汇通天下" class="headerlink" title="最后讲两个超级大Case：卡行天下、汇通天下"></a>最后讲两个超级大Case：卡行天下、<a href="https://aone.alibaba-inc.com/task/10409778" target="_blank" rel="noopener">汇通天下</a></h2>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/btrace/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/btrace/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:09:51+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/btrace/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/btrace/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="btrace-startup"><a href="#btrace-startup" class="headerlink" title="btrace startup"></a>btrace startup</h1><p>直接下载安装 <a href="https://github.com/jbachorik/btrace/releases/tag/v1.3.4" target="_blank" rel="noopener">github release</a> 即可。或者从oss下载：</p>
<pre><code>osscmd get oss://fzpackages/ark/btrace-bin-1.3.4.zip ./btrace-bin-1.3.4.zip
</code></pre><p>解压后即可直接运行(8817是目标进程pid)： </p>
<pre><code>./bin/btrace 8817 ./BTraceNew.java
</code></pre><p>BTraceNew.java 代码如下：<br>    import com.sun.btrace.annotations.<em>;<br>    import static com.sun.btrace.BTraceUtils.</em>;<br>    import com.sun.btrace.BTraceUtils;<br>    import com.sun.btrace.BTraceUtils.Strings;</p>
<pre><code>import java.net.*;
import java.lang.reflect.Field;

@BTrace
public class BTraceNew {

@OnMethod(clazz=&quot;java.lang.StackTraceElement&quot;, method=&quot;&lt;init&gt;&quot;)
public static void onCreateInit(@Self Object me) {
      println(&quot;\n==== init ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
            }

@OnMethod(clazz = &quot;java.lang.Throwable&quot;, method=&quot;&lt;init&gt;&quot;)
@OnMethod(clazz=&quot;java.lang.StackTraceElement&quot;, method=&quot;/.*/&quot;)
public static void onCreateNew(@Self Object me) {
      println(&quot;\n==== call new ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
            }

@OnMethod(clazz=&quot;+java.net.Socket&quot;, method=&quot;close&quot;)
public static void onSocketClose(@Self Object me) {
      println(&quot;\n==== java.net.Socket#close ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
            }

}
</code></pre><p>​    </p>
<h2 id="btrace代码自动生成工具"><a href="#btrace代码自动生成工具" class="headerlink" title="btrace代码自动生成工具"></a>btrace代码自动生成工具</h2><p><a href="https://btrace.org/btrace/" target="_blank" rel="noopener">https://btrace.org/btrace/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/11/test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/11/test/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-11T10:07:31+08:00">
                2021-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/11/test/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/05/11/test/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="博客维护中"><a href="#博客维护中" class="headerlink" title="博客维护中"></a>博客维护中</h1><p>weibo: plantegg</p>
<p><img src="/Users/ren/src/blog/951413iMgBlog/750px-cascade_lake_naming_scheme.svg.png" alt="cascade lake naming scheme.svg"><br><img src="750px-cascade_lake_naming_scheme.svg.png" alt="cascade lake naming scheme_source.svg"><br><img src="/951413iMgBlog/750px-cascade_lake_naming_scheme.svg.png" alt="cascade lake naming scheme_951413.svg"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/04/06/为什么这么多CLOSE_WAIT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/04/06/为什么这么多CLOSE_WAIT/" itemprop="url">为什么这么多CLOSE_WAIT</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-04-06T10:30:03+08:00">
                2021-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/04/06/为什么这么多CLOSE_WAIT/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/04/06/为什么这么多CLOSE_WAIT/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么这么多CLOSE-WAIT"><a href="#为什么这么多CLOSE-WAIT" class="headerlink" title="为什么这么多CLOSE_WAIT"></a>为什么这么多CLOSE_WAIT</h1><p>业务同学发现业务端口上的TCP连接处于CLOSE_WAIT状态的数量有积压，多的时候能堆积到几万个，有时候应用无法响应了</p>
<blockquote>
<p>怎么样才能获取举三反一的秘籍， 普通人为什么要案例来深化对理论知识的理解。</p>
</blockquote>
<h2 id="检查机器状态"><a href="#检查机器状态" class="headerlink" title="检查机器状态"></a>检查机器状态</h2><p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/418b94ee-18ee-4976-857b-69f3016af2b0.png" alt="img"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/160490c8-56e9-46f2-9c48-713944b94a5c.png" alt="img"></p>
<p>从上述两个图中可以看到磁盘 sdb压力非常道，util经常会到 100%，这个时候对应地从top中也可以看到cpu wait%很高（这个ECS cpu本来竞争很激烈），st%一直非常高，所以整体留给应用的CPU不多，碰上磁盘缓慢的话，如果业务写日志是同步刷盘那么就会导致程序卡顿严重。</p>
<p>实际看到FGC的时间也是正常状态下的10倍了。</p>
<p>再看看实际上应用写磁盘比较猛，平均20-30M，高的时候能到200M每秒。如果输出的时候磁盘卡住了那么就整个卡死了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#dstat</span><br><span class="line">----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--</span><br><span class="line">usr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw</span><br><span class="line">  4   1  89   5   0   0|1549M 8533M|   0     0 | 521k  830k|6065k 7134</span><br><span class="line">  3   1  95   0   0   0|3044k   19M|1765k   85k|   0    84k| 329k 7770</span><br><span class="line">  5   1  93   0   0   0|3380k   18M|4050k  142k|   0     0 | 300k 8008</span><br><span class="line">  7   1  91   1   0   1|2788k  227M|5094k  141k|   0    28k| 316k 8644</span><br><span class="line">  4   1  93   2   0   0|2788k   55M|2897k   63k|   0    68k| 274k 6453</span><br><span class="line">  6   1  91   1   0   0|4464k   24M|3683k   98k|   0    28k| 299k 7379</span><br><span class="line">  7   1  91   1   0   0|  10M   34M|3655k  130k|   0   208k| 375k 8417</span><br><span class="line">  3   1  87   8   0   0|6940k   33M|1335k   91k|   0   112k| 334k 7369</span><br><span class="line">  3   1  88   7   0   0|4932k   16M|1918k   61k|   0    44k| 268k 6542</span><br><span class="line">  7   1  86   6   0   0|5508k   20M|5377k  111k|   0     0 | 334k 7998</span><br><span class="line">  7   2  88   3   0   0|5628k  115M|4713k  104k|   0     0 | 280k 7392</span><br><span class="line">  4   1  95   0   0   0|   0   732k|2940k   85k|   0    76k| 189k 7682</span><br><span class="line">  3   1  96   0   0   0|   0   800k|1809k   68k|   0    16k| 181k 9640</span><br><span class="line">  7   2  76  14   0   1|6300k   38M|3834k  132k|   0     0 | 333k 7502</span><br><span class="line">  7   2  90   1   0   0|3896k   19M|3786k   93k|   0     0 | 357k 7578</span><br><span class="line">  4   1  94   0   0   0|5732k   29M|2906k  806k|   0     0 | 338k 8966</span><br><span class="line">  4   1  94   1   0   0|6044k   17M|2202k   95k|   0     0 | 327k 7573</span><br><span class="line">  4   1  95   1   0   0|3524k   17M|2277k   88k|   0     0 | 299k 6462</span><br><span class="line">  4   1  96   0   0   0| 456k   14M|2770k   91k|  60k    0 | 252k 6644</span><br><span class="line">  6   2  92   0   0   0|   0    12M|4251k  847k|   0     0 | 264k   10k</span><br><span class="line">  3   1  92   4   0   0| 788k  204M|1555k   43k|   0     0 | 249k 6215</span><br><span class="line">  6   1  86   6   0   0|7180k   20M|2073k   92k|   0     0 | 303k 7028</span><br><span class="line"> 11   4  84   1   0   0|6116k   29M|3079k   99k|  28k    0 | 263k 6605</span><br></pre></td></tr></table></figure>
<p>磁盘util 100%和CLOSE_WAIT强相关，也和理论比较符合，CLOSE_WAIT就是应用没调socket.close</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/3b7dedca-1c79-4317-8042-bb9ba8c957b9.png" alt="img"></p>
<p>大概的原因推断是：</p>
<p>1）新版本代码需要消耗更多的CPU，代码增加了新的逻辑 </p>
<p>2） 机器本身资源(CPU /IO）很紧张 这两个条件下导致应用响应缓慢。 目前看到的稳定重现条件就是重启一个dncs节点，重启dncs会触发dncs之间重新同步数据，以及重新推送很多数据到客户端的新连接上，这两件事情都会让应用CPU占用飙升响应缓慢，响应慢了之后会导致更多的心跳失效进一步加剧数据同步，然后就雪崩恶化了。最后表现就是看到系统卡死了，也就是tcp buffer中的数据也不读走、连接也不close，连接大量堆积在close_wait状态</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/227c69f1-0467-425c-a19d-26c03d50c36c.png" alt="img"></p>
<h2 id="先看TCP连接状态图"><a href="#先看TCP连接状态图" class="headerlink" title="先看TCP连接状态图"></a>先看TCP连接状态图</h2><p>这是网络、书本上凡是描述TCP状态一定会出现的状态图，理论上看这个图能解决任何TCP状态问题。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b3d075782450b0c8d2615c5d2b75d923.png" alt="image.png"></p>
<p>反复看这个图的右下部分的CLOSE_WAIT ，从这个图里可以得到如下结论：</p>
<p><strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong></p>
<p>基本上这一结论要能帮助解决所有CLOSE_WAIT相关的问题，如果不能说明对这个知识点理解的不够。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>机器超卖严重、IO卡顿，导致应用线程卡顿</p>
<h2 id="server端大量close-wait案例"><a href="#server端大量close-wait案例" class="headerlink" title="server端大量close_wait案例"></a>server端大量close_wait案例</h2><p>用实际案例来检查自己对CLOSE_WAIT 理论（<strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong>）的掌握 – 能不能用这个结论来解决实际问题。同时也可以看看自己从知识到问题的推理能力（跟前面的知识效率呼应一下）。</p>
<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><blockquote>
<p>服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
</blockquote>
<p>根据这个描述先不要往下看，自己推理分析下可能的原因。</p>
<p>我的推理如下：</p>
<p>从这里看起来，client跟server成功建立了somaxconn个连接（somaxconn小于backlog，所以accept queue只有这么大），但是应用没有accept这个连接，导致这些连接一直在accept queue中。但是这些连接的状态已经是ESTABLISHED了，也就是client可以发送数据了，数据发送到server后OS ack了，并放在os的tcp buffer中，应用一直没有accept也就没法读取数据。client于是发送fin（可能是超时、也可能是简单发送数据任务完成了得结束连接），这时Server上这个连接变成了CLOSE_WAIT .</p>
<p>也就是从开始到结束这些连接都在accept queue中，没有被应用accept，很快他们又因为client 发送 fin 包变成了CLOSE_WAIT ，所以始终看到的是服务端出现大量CLOSE_WAIT 并且个数正好等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）。</p>
<p>如下图所示，在连接进入accept queue后状态就是ESTABLISED了，也就是可以正常收发数据和fin了。client是感知不到server是否accept()了，只是发了数据后server的os代为保存在OS的TCP buffer中，因为应用没来取自然在CLOSE_WAIT 后应用也没有close()，所以一直维持CLOSE_WAIT 。</p>
<p>得检查server 应用为什么没有accept。</p>
<p><img src="http://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/2703fc07dfc4dd5b6e1bb4c2ce620e59.png" alt="image.png"></p>
<p>如上是老司机的思路靠经验缺省了一些理论推理，缺省还是对理论理解不够， 这个分析抓住了 大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）但是没有抓住 CLOSE_WAIT 背后的核心原因</p>
<h3 id="更简单的推理"><a href="#更简单的推理" class="headerlink" title="更简单的推理"></a>更简单的推理</h3><p>如果没有任何实战经验，只看上面的状态图的学霸应该是这样推理的：</p>
<p>看到server上有大量的CLOSE_WAIT说明client主动断开了连接，server的OS收到client 发的fin，并回复了ack，这个过程不需要应用感知，进而连接从ESTABLISHED进入CLOSE_WAIT，此时在等待server上的应用调用close连关闭连接（处理完所有收发数据后才会调close()） —- 结论：server上的应用一直卡着没有调close().</p>
<h2 id="CLOSE-WAIT-状态拆解"><a href="#CLOSE-WAIT-状态拆解" class="headerlink" title="CLOSE_WAIT 状态拆解"></a>CLOSE_WAIT 状态拆解</h2><p>通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：</p>
<ul>
<li><strong>程序问题</strong>：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。</li>
<li>响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。</li>
<li>BACKLOG 太大：此处的 backlog 不是 syn backlog，而是 accept 的 backlog，如果 backlog 太大的话，设想突然遭遇大访问量的话，即便响应速度不慢，也可能出现来不及消费的情况，导致多余的请求还在<a href="http://jaseywang.me/2014/07/20/tcp-queue-的一些问题/" target="_blank" rel="noopener">队列</a>里就被对方关闭了。</li>
</ul>
<p>如果你通过「netstat -ant」或者「ss -ant」命令发现了很多 CLOSE_WAIT 连接，请注意结果中的「Recv-Q」和「Local Address」字段，通常「Recv-Q」会不为空，它表示应用还没来得及接收数据，而「Local Address」表示哪个地址和端口有问题，我们可以通过「lsof -i:<port>」来确认端口对应运行的是什么程序以及它的进程号是多少。</port></p>
<p>如果是我们自己写的一些程序，比如用 HttpClient 自定义的蜘蛛，那么八九不离十是程序问题，如果是一些使用广泛的程序，比如 Tomcat 之类的，那么更可能是响应速度太慢或者 timeout 设置太小或者 BACKLOG 设置过大导致的故障。</p>
<p>看完这段 CLOSE_WAIT 更具体深入点的分析后再来分析上面的案例看看，能否推导得到正确的结论。</p>
<h2 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h2><h3 id="连接都没有被accept-client端就能发送数据了？"><a href="#连接都没有被accept-client端就能发送数据了？" class="headerlink" title="连接都没有被accept(), client端就能发送数据了？"></a>连接都没有被accept(), client端就能发送数据了？</h3><p>答：是的。只要这个连接在OS看来是ESTABLISHED的了就可以，因为握手、接收数据都是由内核完成的，内核收到数据后会先将数据放在内核的tcp buffer中，然后os回复ack。另外三次握手之后client端是没法知道server端是否accept()了。</p>
<h3 id="CLOSE-WAIT与accept-queue有关系吗？"><a href="#CLOSE-WAIT与accept-queue有关系吗？" class="headerlink" title="CLOSE_WAIT与accept queue有关系吗？"></a>CLOSE_WAIT与accept queue有关系吗？</h3><p>答：没有关系。只是本案例中因为open files不够了，影响了应用accept(), 导致accept queue满了，同时因为即使应用不accept（三次握手后，server端是否accept client端无法感知），client也能发送数据和发 fin断连接，这些响应都是os来负责，跟上层应用没关系，连接从握手到ESTABLISHED再到CLOSE_WAIT都不需要fd，也不需要应用参与。CLOSE_WAIT只跟应用不调 close() 有关系。 </p>
<h3 id="CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？"><a href="#CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？" class="headerlink" title="CLOSE_WAIT与accept queue为什么刚好一致并且联动了？"></a>CLOSE_WAIT与accept queue为什么刚好一致并且联动了？</h3><p>答：这里他们的数量刚好一致是因为所有新建连接都没有accept，堵在queue中。同时client发现问题后把所有连接都fin了，也就是所有queue中的连接从来没有被accept过，但是他们都是ESTABLISHED，过一阵子之后client端发了fin所以所有accept queue中的连接又变成了 CLOSE_WAIT, 所以二者刚好一致并且联动了</p>
<h3 id="openfiles和accept-的关系是？"><a href="#openfiles和accept-的关系是？" class="headerlink" title="openfiles和accept()的关系是？"></a>openfiles和accept()的关系是？</h3><p>答：accept()的时候才会创建文件句柄，消耗openfiles</p>
<h3 id="一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"><a href="#一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？" class="headerlink" title="一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"></a>一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？</h3><p>答：是</p>
<h3 id="如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？"><a href="#如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？" class="headerlink" title="如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？"></a>如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？</h3><p>答：对</p>
<h3 id="如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？"><a href="#如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？" class="headerlink" title="如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？"></a>如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？</h3><p>答： 能，这个时候在client和server两边看到的连接状态都是 ESTABLISHED，只是Server上的全连接队列占用加1。连接握手并切换到ESTABLISHED状态都是由OS来负责的，应用不参与，ESTABLISHED后应用才能accept，进而收发数据。也就是能放入到全连接队列里面的连接肯定都是 ESTABLISHED 状态的了</p>
<h3 id="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"><a href="#接着上面的问题，如果新连接继续连接进而全连接队列满了呢？" class="headerlink" title="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"></a>接着上面的问题，如果新连接继续连接进而全连接队列满了呢？</h3><p>答：那就连不上了，server端的OS因为全连接队列满了直接扔掉第一个syn握手包，这个时候连接在client端是SYN_SENT，Server端没有这个连接，这是因为syn到server端就直接被OS drop 了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//如下图，本机测试，只有一个client端发起的syn_send, 3306的server端没有任何连接</span><br><span class="line">$netstat -antp  |grep -i 127.0.0.1:3306</span><br><span class="line">tcp     0   1 127.0.0.1:61106      127.0.0.1:3306    SYN_SENT    21352/telnet</span><br></pre></td></tr></table></figure>
<p>能进入到accept queue中的连接都是 ESTABLISHED，不管用户态有没有accept，用户态accept后队列大小减1</p>
<h3 id="如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？"><a href="#如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？" class="headerlink" title="如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？"></a>如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？</h3><p>答： 如果此时收到对方的RESET了，那么OS会释放这个连接。但是内核认为所有 listen 到的连接, 必须要 accept 走, 因为用户有权利知道有过这么一个连接存在过。所以OS不会到全连接队列拿掉这个连接，全连接队列数量也不会减1，直到应用accept这个连接，然后read/write才发现这个连接断开了，报communication failure异常</p>
<h3 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h3><p>三次握手成功就变成 ESTABLISHED 了，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>这是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -at |grep &quot;:12346 &quot;</span><br><span class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</span><br><span class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</span><br><span class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</span><br><span class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </span><br><span class="line">LISTEN      73     1024            *:terabase                 *:*</span><br></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -at |grep &quot;:12346 &quot;  </span><br><span class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</span><br><span class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</span><br><span class="line"># ss -lt</span><br><span class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </span><br><span class="line">LISTEN      1025   1024             *:terabase              *:*</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/07/一次海光物理机资源竞争压测的记录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/07/一次海光物理机资源竞争压测的记录/" itemprop="url">一次海光物理机资源竞争压测的记录</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-07T17:30:03+08:00">
                2021-03-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/03/07/一次海光物理机资源竞争压测的记录/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/03/07/一次海光物理机资源竞争压测的记录/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一次海光物理机资源竞争压测的记录"><a href="#一次海光物理机资源竞争压测的记录" class="headerlink" title="一次海光物理机资源竞争压测的记录"></a>一次海光物理机资源竞争压测的记录</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>问题描述如下</p>
<blockquote>
<p>sysbench 压200个服务节点(每个4c16G，总共800core), 发现qps不能线性增加（200节点比100节点好1.2倍而已)。</p>
<p>如果压单个服务节点节点QPS 2.4万，CPU跑到390%（每个服务节点独占4个核），如果压200个服务节点（分布在16台64核的海光物理机上）平均每个服务节点节点QPS才1.2万。但是每个服务节点的CPU也跑到了390%左右。 现在的疑问就是为什么CPU跑上去了QPS打了个5折。</p>
<p>机器集群为16*64core 为1024core，也就是每个服务节点独占4core还有冗余</p>
</blockquote>
<p>因为服务节点还需要通过LVS调用后端的多个MySQL集群，所以需要排除LVS、网络等链路瓶颈，然后找到根因是什么。</p>
<h3 id="海光物理机CPU相关信息"><a href="#海光物理机CPU相关信息" class="headerlink" title="海光物理机CPU相关信息"></a>海光物理机CPU相关信息</h3><p>总共有16台如下的海光服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                64</span><br><span class="line">On-line CPU(s) list:   0-63</span><br><span class="line">Thread(s) per core:    2      //每个物理core有两个超线程</span><br><span class="line">Core(s) per socket:    16     //每路16个物理core</span><br><span class="line">Socket(s):             2      //2路</span><br><span class="line">NUMA node(s):          4</span><br><span class="line">Vendor ID:             HygonGenuine</span><br><span class="line">CPU family:            24</span><br><span class="line">Model:                 1</span><br><span class="line">Model name:            Hygon C86 5280 16-core Processor</span><br><span class="line">Stepping:              1</span><br><span class="line">CPU MHz:               2455.552</span><br><span class="line">CPU max MHz:           2500.0000</span><br><span class="line">CPU min MHz:           1600.0000</span><br><span class="line">BogoMIPS:              4999.26</span><br><span class="line">Virtualization:        AMD-V</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             64K</span><br><span class="line">L2 cache:              512K</span><br><span class="line">L3 cache:              8192K</span><br><span class="line">NUMA node0 CPU(s):     0-7,32-39</span><br><span class="line">NUMA node1 CPU(s):     8-15,40-47</span><br><span class="line">NUMA node2 CPU(s):     16-23,48-55</span><br><span class="line">NUMA node3 CPU(s):     24-31,56-63</span><br><span class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd sev ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 MySQLeed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca</span><br><span class="line"></span><br><span class="line">[root@d42a01106.cloud.a02.am78 /root]</span><br><span class="line">#numactl -H</span><br><span class="line">available: 4 nodes (0-3)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 32 33 34 35 36 37 38 39</span><br><span class="line">node 0 size: 128854 MB</span><br><span class="line">node 0 free: 89350 MB</span><br><span class="line">node 1 cpus: 8 9 10 11 12 13 14 15 40 41 42 43 44 45 46 47</span><br><span class="line">node 1 size: 129019 MB</span><br><span class="line">node 1 free: 89326 MB</span><br><span class="line">node 2 cpus: 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55</span><br><span class="line">node 2 size: 128965 MB</span><br><span class="line">node 2 free: 86542 MB</span><br><span class="line">node 3 cpus: 24 25 26 27 28 29 30 31 56 57 58 59 60 61 62 63</span><br><span class="line">node 3 size: 129020 MB</span><br><span class="line">node 3 free: 98227 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1   2   3</span><br><span class="line">  0:  10  16  28  22</span><br><span class="line">  1:  16  10  22  28</span><br><span class="line">  2:  28  22  10  16</span><br><span class="line">  3:  22  28  16  10</span><br></pre></td></tr></table></figure>
<p>这CPU据说是胶水核，也就是把两个CPU拼一块，所以跨CPU之间延迟还是很高的。</p>
<h4 id="64-个-core-的分配策略"><a href="#64-个-core-的分配策略" class="headerlink" title="64 个 core 的分配策略"></a>64 个 core 的分配策略</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">physical         core      processor</span><br><span class="line">0                0~15         0~15</span><br><span class="line">1                0~15         16~31</span><br><span class="line">0                0~15         32~47</span><br><span class="line">1                0~15         48~63</span><br></pre></td></tr></table></figure>
<p>如果physical id和core id都一样的话，说明这两个core实际是一个物理core，其中一个是HT</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/191276e2a1a1731969da748f1690bc9b.png" alt="image.png"></p>
<h3 id="Intel-CPU"><a href="#Intel-CPU" class="headerlink" title="Intel CPU"></a>Intel CPU</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                104</span><br><span class="line">On-line CPU(s) list:   0-103</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    26</span><br><span class="line">座：                 2</span><br><span class="line">NUMA 节点：         1</span><br><span class="line">厂商 ID：           GenuineIntel</span><br><span class="line">CPU 系列：          6</span><br><span class="line">型号：              85</span><br><span class="line">型号名称：        Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</span><br><span class="line">步进：              7</span><br><span class="line">CPU MHz：             1200.000</span><br><span class="line">CPU max MHz:           2501.0000</span><br><span class="line">CPU min MHz:           1200.0000</span><br><span class="line">BogoMIPS：            5000.00</span><br><span class="line">虚拟化：           VT-x</span><br><span class="line">L1d 缓存：          32K</span><br><span class="line">L1i 缓存：          32K</span><br><span class="line">L2 缓存：           1024K</span><br><span class="line">L3 缓存：           36608K</span><br><span class="line">NUMA 节点0 CPU：    0-103</span><br><span class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni spec_ctrl intel_stibp flush_l1d arch_capabilities</span><br><span class="line"></span><br><span class="line"># numactl -H</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103</span><br><span class="line">node 0 size: 785826 MB</span><br><span class="line">node 0 free: 108373 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0</span><br><span class="line">  0:  10</span><br></pre></td></tr></table></figure>
<h3 id="鲲鹏920"><a href="#鲲鹏920" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                96</span><br><span class="line">On-line CPU(s) list:   0-95</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    48</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          1</span><br><span class="line">Model:                 0</span><br><span class="line">CPU max MHz:           2600.0000</span><br><span class="line">CPU min MHz:           200.0000</span><br><span class="line">BogoMIPS:              200.00</span><br><span class="line">L1d cache:             64K</span><br><span class="line">L1i cache:             64K</span><br><span class="line">L2 cache:              512K</span><br><span class="line">L3 cache:              49152K</span><br><span class="line">NUMA node0 CPU(s):     0-95</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br><span class="line"></span><br><span class="line">[root@j63c03439.sqa.eu95 /root]</span><br><span class="line">#numactl -H</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</span><br><span class="line">node 0 size: 773421 MB</span><br><span class="line">node 0 free: 756084 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0</span><br><span class="line">  0:  10</span><br><span class="line">  </span><br><span class="line">#dmidecode -t processor | grep Version</span><br><span class="line">	Version: Kunpeng 920-4826</span><br><span class="line">	Version: Kunpeng 920-4826</span><br></pre></td></tr></table></figure>
<h3 id="飞腾"><a href="#飞腾" class="headerlink" title="飞腾"></a>飞腾</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                128</span><br><span class="line">On-line CPU(s) list:   0-127</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    64</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          16</span><br><span class="line">Model:                 3</span><br><span class="line">BogoMIPS:              100.00</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              2048K</span><br><span class="line">L3 cache:              65536K</span><br><span class="line">NUMA node0 CPU(s):     0-7</span><br><span class="line">NUMA node1 CPU(s):     8-15</span><br><span class="line">NUMA node2 CPU(s):     16-23</span><br><span class="line">NUMA node3 CPU(s):     24-31</span><br><span class="line">NUMA node4 CPU(s):     32-39</span><br><span class="line">NUMA node5 CPU(s):     40-47</span><br><span class="line">NUMA node6 CPU(s):     48-55</span><br><span class="line">NUMA node7 CPU(s):     56-63</span><br><span class="line">NUMA node8 CPU(s):     64-71</span><br><span class="line">NUMA node9 CPU(s):     72-79</span><br><span class="line">NUMA node10 CPU(s):    80-87</span><br><span class="line">NUMA node11 CPU(s):    88-95</span><br><span class="line">NUMA node12 CPU(s):    96-103</span><br><span class="line">NUMA node13 CPU(s):    104-111</span><br><span class="line">NUMA node14 CPU(s):    112-119</span><br><span class="line">NUMA node15 CPU(s):    120-127</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</span><br></pre></td></tr></table></figure>
<h3 id="cpu-node-memory"><a href="#cpu-node-memory" class="headerlink" title="cpu node memory"></a>cpu node memory</h3><p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/64118c14-da63-4775-abd3-a4f86ec84cf5.png" alt="img"></p>
<h2 id="验证是否是上下游的瓶颈"><a href="#验证是否是上下游的瓶颈" class="headerlink" title="验证是否是上下游的瓶颈"></a>验证是否是上下游的瓶颈</h2><p>需要先分析问题是否在LVS调用后端的多个MySQL集群上。</p>
<p>先写一个简单的测试程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#cat Test.java</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line">/*</span><br><span class="line"> * 目录：/home/admin/jdbc</span><br><span class="line"> *</span><br><span class="line"> * 编译：</span><br><span class="line"> *  javac -cp /home/admin/lib/*:. Test.java</span><br><span class="line"> *</span><br><span class="line"> *  运行：</span><br><span class="line"> *   java -cp /home/admin/MySQL-server/lib/*:. Test &quot;jdbc:mysql://172.16.160.1:4261/qc_pay_0xwd_0002&quot; &quot;myhhzi0d&quot; &quot;jOXaC1Lbif-k&quot; &quot;select count(*) from pay_order where user_id=1169257092557639682 and order_no=&apos;201909292111250000102&apos;&quot; &quot;100&quot;</span><br><span class="line"> *   */</span><br><span class="line">public class Test &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String args[]) throws NumberFormatException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);</span><br><span class="line">        String url = args[0];</span><br><span class="line">        String user = args[1];</span><br><span class="line">        String pass = args[2];</span><br><span class="line">        String sql = args[3];</span><br><span class="line">        String interval = args[4];</span><br><span class="line">        try &#123;</span><br><span class="line">            Connection conn = DriverManager.getConnection(url, user, pass);</span><br><span class="line">            while (true) &#123;</span><br><span class="line">                long start = System.currentTimeMillis();</span><br><span class="line">                for(int i=0; i&lt;1000; ++i)&#123;</span><br><span class="line">                    Statement stmt = conn.createStatement();</span><br><span class="line">                    ResultSet rs = stmt.executeQuery(sql);</span><br><span class="line">                    while (rs.next()) &#123;</span><br><span class="line">                    &#125;</span><br><span class="line">                    rs.close();</span><br><span class="line">                    stmt.close();</span><br><span class="line">                    Thread.sleep(Long.valueOf(interval));</span><br><span class="line">                &#125;</span><br><span class="line">                long end = System.currentTimeMillis();</span><br><span class="line">                System.out.println(&quot;rt : &quot; + (end - start));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后通过传入不同的jdbc参数跑2组测试:</p>
<ol>
<li>走服务节点执行指定id的点查； </li>
<li>直接从服务节点节点连MySQL指定id点查  </li>
</ol>
<p>上述2组测试同时跑在三组场景下：</p>
<ul>
<li>A) 服务节点和MySQL都没有压力； </li>
<li>B) 跑1、2测试的服务节点没有压力，但是sysbench 在压别的服务节点，这样后端的MySQL是有sysbench压侧压力，LVS也有流量压力的； </li>
<li>C) sysbench压所有服务节点, 包含运行 1、2测试程序节点） </li>
</ul>
<p>这样2组测试3个场景组合可以得到6组响应时间的测试数据</p>
<p>从最终得到6组数据来看可以排除链路以及MySQL的问题，瓶颈似乎还是在服务节点上</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/595bc15fdd72860f2d1875c86384a14b.png" alt="image.png"></p>
<p>单独压一个服务节点节点并在上面跑测试，服务节点 CPU被压到 390%(每个服务节点 节点固定绑到4核), 这个时候整个宿主机压力不大，但是这四个核比较紧张了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#cat rt.log  | awk &apos;&#123; print $3 &#125;&apos;  | awk &apos;&#123;if(min==&quot;&quot;)&#123;min=max=$1&#125;; if($1&gt;max) &#123;max=$1&#125;; if($1&lt;min) &#123;min=$1&#125;; total+=$1; count+=1&#125; END &#123;print &quot;avg &quot; total/count,&quot; | max &quot;max,&quot; | min &quot; min, &quot;| count &quot;, count&#125;&apos; ; cat MySQL.log  | awk &apos;&#123; print $3 &#125;&apos;  | awk &apos;&#123;if(min==&quot;&quot;)&#123;min=max=$1&#125;; if($1&gt;max) &#123;max=$1&#125;; if($1&lt;min) &#123;min=$1&#125;; total+=$1; count+=1&#125; END &#123;print &quot;avg &quot; total/count,&quot; | max &quot;max,&quot; | min &quot; min, &quot;| count &quot;, count &#125;&apos;;</span><br><span class="line">avg 2589.13  | max 3385  | min 2502 | count  69</span><br><span class="line">avg 1271.07  | max 1405  | min 1254 | count  141</span><br><span class="line"></span><br><span class="line">[root@d42a01107.cloud.a02.am78 /root]</span><br><span class="line">#taskset -pc 48759</span><br><span class="line">pid 48759&apos;s current affinity list: 19,52-54</span><br></pre></td></tr></table></figure>
<p>通过这6组测试数据可以看到，只有在整个系统都有压力（服务节点所在物理机、LVS、MySQL）的时候rt飙升最明显（C组数据），如果只是LVS、MySQL有压力，服务节点没有压力的时候可以看到数据还是很好的（B组数据）</p>
<h2 id="分析宿主机资源竞争"><a href="#分析宿主机资源竞争" class="headerlink" title="分析宿主机资源竞争"></a>分析宿主机资源竞争</h2><h3 id="perf分析"><a href="#perf分析" class="headerlink" title="perf分析"></a>perf分析</h3><p>只压单个服务节点</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/7aea9045e50794fadc0439ee806f2496.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/86c3f14887345a1d5f08cae985816564.png" alt="image.png"></p>
<p><strong>从以上截图，可以看到关键的 insn per cycle 能到0.51和0.66（这个数值越大性能越好）</strong></p>
<p>如果同时压物理机上的所有服务节点</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/02f47474c612c2bf6e612efea3ab5de3.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/c3d90e077d00a7a3db54770d9eea2dbb.png" alt="image.png"></p>
<p><strong>从以上截图，可以看到关键的 insn per cycle 能降到了0.27和0.31（这个数值越大性能越好），基本相当于单压的5折</strong></p>
<p>通过 perf list 找出所有Hardware event，然后对他们进行perf:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo perf stat -e branch-instructions,branch-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-prefetches,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads -a -- `pidof java`</span><br></pre></td></tr></table></figure>
<h3 id="尝试不同的绑核后的一些数据"><a href="#尝试不同的绑核后的一些数据" class="headerlink" title="尝试不同的绑核后的一些数据"></a>尝试不同的绑核后的一些数据</h3><p>通过以上perf数据以及numa结构，尝试将不同服务进程绑定到指定的4个核上</p>
<p>试了以下三种绑核的办法：</p>
<p>1）docker swarm随机绑（<strong>以上测试都是用的这种默认方案</strong>）；</p>
<p>2）一个服务节点绑连续4个core,这4个core都在同一个node； </p>
<p>3）一个服务节点绑4个core，这个4个core都在在同一个node，同时尽量HT在一起，也就是0，1，32，33 ； 2，3，34，35 这种绑法 </p>
<p><strong>结果是这三种绑法没看到什么明显的性能差异</strong>. </p>
<p>如果是绑法2，压单个服务节点 QPS能到2.3万；绑法1和3，压单个服务节点性能差别不明显，都是2万左右。 </p>
<p>不过将进程绑在一个node内理论上性能是有提升的。</p>
<h2 id="尝试将Java进程开启HugePage"><a href="#尝试将Java进程开启HugePage" class="headerlink" title="尝试将Java进程开启HugePage"></a>尝试将Java进程开启HugePage</h2><p>从perf数据来看压满后tlab miss比较高，得想办法降低这个值</p>
<h3 id="修改JVM启动参数"><a href="#修改JVM启动参数" class="headerlink" title="修改JVM启动参数"></a>修改JVM启动参数</h3><p>JVM启动参数增加如下三个(-XX:LargePageSizeInBytes=2m, 这个一定要，有些资料没提这个，在我的JDK8.0环境必须要)：</p>
<blockquote>
<p>-XX:+UseLargePages -XX:LargePageSizeInBytes=2m -XX:+UseHugeTLBFS</p>
</blockquote>
<h3 id="修改机器系统配置"><a href="#修改机器系统配置" class="headerlink" title="修改机器系统配置"></a>修改机器系统配置</h3><p>设置HugePage的大小</p>
<blockquote>
<p>cat /proc/sys/vm/nr_hugepages</p>
</blockquote>
<p>nr_hugepages设置多大参考如下计算方法：</p>
<blockquote>
<p>If you are using the option <code>-XX:+UseSHM</code> or <code>-XX:+UseHugeTLBFS</code>, then specify the number of large pages. In the following example, 3 GB of a 4 GB system are reserved for large pages (assuming a large page size of 2048kB, then 3 GB = 3 <em> 1024 MB = 3072 MB = 3072 </em> 1024 kB = 3145728 kB and 3145728 kB / 2048 kB = 1536):</p>
<p>echo 1536 &gt; /proc/sys/vm/nr_hugepages </p>
</blockquote>
<p>透明大页是没有办法减少系统tlab，tlab是对应于进程的，系统分给进程的透明大页还是由物理上的4K page组成。</p>
<p>Java进程用上HugePages后iTLB-load-misses从80%下降到了14%左右, dTLB也从30%下降到了20%，但是ipc变化不明显，QPS有不到10%的增加(不能确定是不是抖动所致)</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/f6882f4c671b4c4b46feb01aa5e272fd.png" alt="image.png"></p>
<p>在公有云ecs虚拟机上测试对性能没啥帮助，实际看到用掉的HuagPage不多，如果/proc/sys/vm/nr_hugepages 设置比较大的话JVM会因为内存不足起不来，两者内存似乎是互斥的</p>
<h2 id="用sysbench验证一下海光服务器的多core能力"><a href="#用sysbench验证一下海光服务器的多core能力" class="headerlink" title="用sysbench验证一下海光服务器的多core能力"></a>用sysbench验证一下海光服务器的多core能力</h2><p>用sysbench 测试Hygon C86 5280 16-core Processor，分别1、8、16、24、32、40、48、56、64 个thread，32个thread前都是完美的线性增加，32core之后基本不增长了，这个应该能说明这个服务器就是32core的能力</p>
<blockquote>
<p>sysbench –threads=1 –cpu-max-prime=50000 cpu run</p>
</blockquote>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/f86cd786f3a8297078579b547f78ec81.png" alt="image.png"></p>
<p>对比下intel的 Xeon 104core，也是物理52core，但是性能呈现完美线性</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/8086f47b955d8d951e4dd4c7fe5e135e.png" alt="image.png"></p>
<h3 id="openssl场景多核能力验证"><a href="#openssl场景多核能力验证" class="headerlink" title="openssl场景多核能力验证"></a>openssl场景多核能力验证</h3><p>据说海光的一个core只有一个fpu，所以超线程算除法完全没用，那么我们再来跑一组openssl 加密</p>
<blockquote>
<p>openssl speed aes-256-ige -multi N</p>
</blockquote>
<p>intel 52 VS 26，可以看到52个线程的性能大概是26个的1.8倍</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/4534d8e5901cc812aa54e610d1386445.png" alt="image.png"></p>
<p>intel 104 VS 52 线程，性能还能提升1.4倍</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/6583f52e03b9753969e52d6a8b211725.png" alt="image.png"></p>
<p>海光32 VS 16, 性能能提升大概1.8倍，跟intel一致</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/41e7f230ec27653c1b5ae5287971cd3a.png" alt="image.png"></p>
<p>海光64 VS 32, 性能能提升大概1.2倍</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/2ad45a252392a06fa64d7475848e5601.png" alt="image.png"></p>
<p>总结下就是，在物理core数以内的线程数intel和海光性能基本增加一致；但如果超过物理core数开始使用HT后海光明显相比Intel差了很多。</p>
<p>intel超线程在openssl场景下性能能提升40%，海光就只能提升20%了。</p>
<h3 id="对比一下鲲鹏920-ARM架构的芯片"><a href="#对比一下鲲鹏920-ARM架构的芯片" class="headerlink" title="对比一下鲲鹏920 ARM架构的芯片"></a>对比一下鲲鹏920 ARM架构的芯片</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#numactl -H</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</span><br><span class="line">node 0 size: 773421 MB</span><br><span class="line">node 0 free: 756092 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0</span><br><span class="line">  0:  10</span><br></pre></td></tr></table></figure>
<p>96核一起跑openssl基本就是1核的96倍，完美线性，这是因为鲲鹏就没有超线程，都是物理核。如果并发增加到192个，性能和96个基本一样的。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/be30ab94eddc37f1d90c53204a0ed215.png" alt="image.png"></p>
<h3 id="用Sysbench直接压MySQL-oltp-read-only的场景"><a href="#用Sysbench直接压MySQL-oltp-read-only的场景" class="headerlink" title="用Sysbench直接压MySQL oltp_read_only的场景"></a>用Sysbench直接压MySQL oltp_read_only的场景</h3><p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/89c7a0228c45f79b688710206ba9d414.png" alt="image.png"></p>
<p>从1到10个thread的时候完美呈现线性，到20个thread就只比10个thread增加50%了，30thread比20增加40%，过了32个thread后增加10个core性能加不到10%了。</p>
<p>在32thread前，随着并发的增加 IPC也有所减少，这也是导致thread翻倍性能不能翻倍的一个主要原因。</p>
<p>基本也和openssl 场景一致，海光的HT基本可以忽略，做的不是很好。超过32个thread后（物理core数）性能增加及其微弱</p>
<h2 id="Intel-海光-鲲鹏-CPU性能比较"><a href="#Intel-海光-鲲鹏-CPU性能比较" class="headerlink" title="Intel 海光 鲲鹏 CPU性能比较"></a>Intel 海光 鲲鹏 CPU性能比较</h2><p>测试脚本</p>
<blockquote>
<p>openssl speed aes-256-ige -multi 1</p>
</blockquote>
<p>单核能力</p>
<table>
<thead>
<tr>
<th>Intel (52物理core)</th>
<th>aes-256 ige      89602.86k    97498.37k    98271.49k    98399.91k    89101.65k</th>
</tr>
</thead>
<tbody>
<tr>
<td>海光（32物理core）</td>
<td>aes-256 ige      76919.66k    77935.81k    79201.88k    79529.30k    79555.24k</td>
</tr>
<tr>
<td>鲲鹏920（96物理core)</td>
<td>aes-256 ige     133174.89k   140578.99k   142156.46k   142663.34k   143196.16k</td>
</tr>
</tbody>
</table>
<p>测试32个线程并行</p>
<table>
<thead>
<tr>
<th>Intel (52物理core)</th>
<th>aes-256 ige    2642742.25k  2690638.98k  2703860.74k  2734114.82k  2680422.40</th>
</tr>
</thead>
<tbody>
<tr>
<td>海光（32物理core）</td>
<td>aes-256 ige    2464568.75k  2499381.80k  2528665.34k  2544845.14k  2550723.93k</td>
</tr>
<tr>
<td>鲲鹏920（96物理core)</td>
<td>aes-256 ige    4261589.92k  4501245.55k  4552731.56k  4570456.75k  4584330.58k</td>
</tr>
</tbody>
</table>
<p>将所有核跑满包括HT</p>
<table>
<thead>
<tr>
<th>Intel (52物理core)</th>
<th>aes-256 ige    4869950.82k  5179884.71k  5135412.14k  5211367.08k  5247858.60k</th>
</tr>
</thead>
<tbody>
<tr>
<td>海光（32物理core）</td>
<td>aes-256 ige    2730195.74k  2836759.53k  2865252.35k  2857900.71k  2884302.17k</td>
</tr>
<tr>
<td>鲲鹏920（96物理core)</td>
<td>aes-256 ige   12788358.79k 13502288.53k 13657385.98k 13710908.76k 13751432.53k</td>
</tr>
</tbody>
</table>
<h2 id="比较-bash-c-‘echo-“7-999999”-bc-gt-dev-null’-计算能力"><a href="#比较-bash-c-‘echo-“7-999999”-bc-gt-dev-null’-计算能力" class="headerlink" title="比较 bash -c ‘echo “7^999999” | bc &gt; /dev/null’ 计算能力"></a>比较 bash -c ‘echo “7^999999” | bc &gt; /dev/null’ 计算能力</h2><p>当然也可以通过计算pi值来测试</p>
<blockquote>
<p>bash -c ‘ echo “scale=5000; 4*a(1)” | bc -l -q &gt;/dev/null ‘</p>
</blockquote>
<p>多核一起跑的话可以这样:</p>
<blockquote>
<p>for i in {0..95}; do time echo “scale=5000; 4*a(1)” | bc -l -q &gt;/dev/null &amp; done</p>
<p>perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads – </p>
</blockquote>
<h3 id="intel"><a href="#intel" class="headerlink" title="intel"></a>intel</h3><p>耗时18.60秒，ipc 2.19</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># sudo perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores -- bash -c &apos;echo &quot;7^999999&quot; | bc &gt; /dev/null&apos;</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;bash -c echo &quot;7^999999&quot; | bc &gt; /dev/null&apos;:</span><br><span class="line"></span><br><span class="line">    25,130,886,211      branch-instructions                                           (10.72%)</span><br><span class="line">     1,200,086,175      branch-misses             #    4.78% of all branches          (14.29%)</span><br><span class="line">       460,824,074      bus-cycles                                                    (14.29%)</span><br><span class="line">         1,983,459      cache-misses              #   46.066 % of all cache refs      (14.30%)</span><br><span class="line">         4,305,730      cache-references                                              (14.30%)</span><br><span class="line">    58,626,314,801      cpu-cycles                                                    (17.87%)</span><br><span class="line">   128,284,870,917      instructions              #    2.19  insn per cycle           (21.45%)</span><br><span class="line">    46,040,656,499      ref-cycles                                                    (25.02%)</span><br><span class="line">        22,821,794      L1-dcache-load-misses     #    0.10% of all L1-dcache hits    (25.02%)</span><br><span class="line">    23,041,732,649      L1-dcache-loads                                               (25.01%)</span><br><span class="line">     5,386,243,625      L1-dcache-stores                                              (25.00%)</span><br><span class="line">        12,443,154      L1-icache-load-misses                                         (25.00%)</span><br><span class="line">           178,790      LLC-load-misses           #   30.52% of all LL-cache hits     (14.28%)</span><br><span class="line">           585,724      LLC-loads                                                     (14.28%)</span><br><span class="line">           469,381      LLC-store-misses                                              (7.14%)</span><br><span class="line">           664,865      LLC-stores                                                    (7.14%)</span><br><span class="line">     1,201,547,113      branch-load-misses                                            (10.71%)</span><br><span class="line">    25,139,625,428      branch-loads                                                  (14.28%)</span><br><span class="line">            63,334      dTLB-load-misses          #    0.00% of all dTLB cache hits   (14.28%)</span><br><span class="line">    23,023,969,089      dTLB-loads                                                    (14.28%)</span><br><span class="line">            17,355      dTLB-store-misses                                             (14.28%)</span><br><span class="line">     5,378,496,562      dTLB-stores                                                   (14.28%)</span><br><span class="line">           341,119      iTLB-load-misses          #  119.92% of all iTLB cache hits   (14.28%)</span><br><span class="line">           284,445      iTLB-loads                                                    (14.28%)</span><br><span class="line">           151,608      node-load-misses                                              (14.28%)</span><br><span class="line">            37,553      node-loads                                                    (14.29%)</span><br><span class="line">           434,537      node-store-misses                                             (7.14%)</span><br><span class="line">            65,709      node-stores                                                   (7.14%)</span><br><span class="line"></span><br><span class="line">      18.603323495 seconds time elapsed</span><br><span class="line"></span><br><span class="line">      18.525904000 seconds user</span><br><span class="line">       0.015197000 seconds sys</span><br></pre></td></tr></table></figure>
<h3 id="鲲鹏920-1"><a href="#鲲鹏920-1" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><p>耗时24.6秒, IPC 1.84</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads -- bash -c &apos;echo &quot;7^999999&quot; | bc &gt; /dev/null&apos;</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;bash -c echo &quot;7^999999&quot; | bc &gt; /dev/null&apos;:</span><br><span class="line"></span><br><span class="line">     1,467,769,425      branch-misses                                                 (59.94%)</span><br><span class="line">    63,866,536,853      bus-cycles                                                    (59.94%)</span><br><span class="line">         6,571,273      cache-misses              #    0.021 % of all cache refs      (59.94%)</span><br><span class="line">    30,768,754,927      cache-references                                              (59.96%)</span><br><span class="line">    63,865,354,560      cpu-cycles                                                    (64.97%)</span><br><span class="line">   117,790,453,518      instructions              #    1.84  insns per cycle</span><br><span class="line">                                                  #    0.07  stalled cycles per insn  (64.98%)</span><br><span class="line">       833,090,930      stalled-cycles-backend    #    1.30% backend  cycles idle     (65.00%)</span><br><span class="line">     7,918,227,782      stalled-cycles-frontend   #   12.40% frontend cycles idle     (65.01%)</span><br><span class="line">         6,962,902      L1-dcache-load-misses     #    0.02% of all L1-dcache hits    (65.03%)</span><br><span class="line">    30,804,266,645      L1-dcache-loads                                               (65.05%)</span><br><span class="line">         6,960,157      L1-dcache-store-misses                                        (65.06%)</span><br><span class="line">    30,807,954,068      L1-dcache-stores                                              (65.06%)</span><br><span class="line">         1,012,171      L1-icache-load-misses                                         (65.06%)</span><br><span class="line">    45,256,066,296      L1-icache-loads                                               (65.04%)</span><br><span class="line">     1,470,467,198      branch-load-misses                                            (65.03%)</span><br><span class="line">    27,108,794,972      branch-loads                                                  (65.01%)</span><br><span class="line">           475,707      dTLB-load-misses          #    0.00% of all dTLB cache hits   (65.00%)</span><br><span class="line">    35,159,826,836      dTLB-loads                                                    (59.97%)</span><br><span class="line">               912      iTLB-load-misses          #    0.00% of all iTLB cache hits   (59.96%)</span><br><span class="line">    45,325,885,822      iTLB-loads                                                    (59.94%)</span><br><span class="line"></span><br><span class="line">      24.604603640 seconds time elapsed</span><br></pre></td></tr></table></figure>
<h3 id="海光"><a href="#海光" class="headerlink" title="海光"></a>海光</h3><p>耗时 26.73秒, IPC 0.92</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">sudo perf stat -e branch-instructions,branch-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-prefetches,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads -a -- bash -c &apos;echo &quot;7^999999&quot; | bc &gt; /dev/null&apos;</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;system wide&apos;:</span><br><span class="line"></span><br><span class="line">    57,795,675,025      branch-instructions                                           (27.78%)</span><br><span class="line">     2,459,509,459      branch-misses             #    4.26% of all branches          (27.78%)</span><br><span class="line">    12,171,133,272      cache-references                                              (27.79%)</span><br><span class="line">   317,353,262,523      cpu-cycles                                                    (27.79%)</span><br><span class="line">   293,162,940,548      instructions              #    0.92  insn per cycle</span><br><span class="line">                                                  #    0.19  stalled cycles per insn  (27.79%)</span><br><span class="line">    55,152,807,029      stalled-cycles-backend    #   17.38% backend cycles idle      (27.79%)</span><br><span class="line">    44,410,732,991      stalled-cycles-frontend   #   13.99% frontend cycles idle     (27.79%)</span><br><span class="line">     4,065,273,083      L1-dcache-load-misses     #    3.58% of all L1-dcache hits    (27.79%)</span><br><span class="line">   113,699,208,151      L1-dcache-loads                                               (27.79%)</span><br><span class="line">     1,351,513,191      L1-dcache-prefetches                                          (27.79%)</span><br><span class="line">     2,091,035,340      L1-icache-load-misses     #    4.43% of all L1-icache hits    (27.79%)</span><br><span class="line">    47,240,289,316      L1-icache-loads                                               (27.79%)</span><br><span class="line">     2,459,838,728      branch-load-misses                                            (27.79%)</span><br><span class="line">    57,855,156,991      branch-loads                                                  (27.78%)</span><br><span class="line">        69,731,473      dTLB-load-misses          #   20.40% of all dTLB cache hits   (27.78%)</span><br><span class="line">       341,773,319      dTLB-loads                                                    (27.78%)</span><br><span class="line">        26,351,132      iTLB-load-misses          #   15.91% of all iTLB cache hits   (27.78%)</span><br><span class="line">       165,656,863      iTLB-loads                                                    (27.78%)</span><br><span class="line"></span><br><span class="line">      26.729972414 seconds time elapsed</span><br></pre></td></tr></table></figure>
<h3 id="飞腾-1"><a href="#飞腾-1" class="headerlink" title="飞腾"></a>飞腾</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">time perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses -a -- bash -c &apos;echo &quot;7^999999&quot; | bc &gt; /dev/null&apos;</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;system wide&apos;:</span><br><span class="line"></span><br><span class="line">        2552812813      branch-misses                                                 (38.08%)</span><br><span class="line">      602038279874      bus-cycles                                                    (37.54%)</span><br><span class="line">        1742826523      cache-misses              #    2.017 % of all cache refs      (37.54%)</span><br><span class="line">       86400294181      cache-references                                              (37.55%)</span><br><span class="line">      612467194375      cpu-cycles                                                    (43.79%)</span><br><span class="line">      263691445872      instructions              #    0.43  insns per cycle          (43.79%)</span><br><span class="line">        1706247569      L1-dcache-load-misses     #    2.00% of all L1-dcache hits    (43.78%)</span><br><span class="line">       85122454139      L1-dcache-loads                                               (43.77%)</span><br><span class="line">        1711243358      L1-dcache-store-misses                                        (39.38%)</span><br><span class="line">       86288158984      L1-dcache-stores                                              (37.52%)</span><br><span class="line">        2006641212      L1-icache-load-misses                                         (37.51%)</span><br><span class="line">      146380907111      L1-icache-loads                                               (37.51%)</span><br><span class="line">        2560208048      branch-load-misses                                            (37.52%)</span><br><span class="line">       63127187342      branch-loads                                                  (41.38%)</span><br><span class="line">         768494735      dTLB-load-misses                                              (43.77%)</span><br><span class="line">         124424415      iTLB-load-misses                                              (43.77%)</span><br><span class="line"></span><br><span class="line">      39.654819568 seconds time elapsed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real	0m39.763s</span><br><span class="line">user	0m39.635s</span><br><span class="line">sys	0m0.127s</span><br></pre></td></tr></table></figure>
<h2 id="再来比较下-perf-数据"><a href="#再来比较下-perf-数据" class="headerlink" title="再来比较下 perf 数据"></a>再来比较下 perf 数据</h2><h3 id="Intel"><a href="#Intel" class="headerlink" title="Intel"></a>Intel</h3><p>intel的cpu随着线程的增加，ipc稳定减少，但不是线性的</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/dcb68dff74ace2cf6f9c30378acdb377.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d0151c855011b24590efd672398bd9eb.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/175a1df9274a830d4a7157dfda96c180.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/e63a992fcd1df547568eb93f515a5c99.png" alt="image.png"></p>
<h3 id="海光-1"><a href="#海光-1" class="headerlink" title="海光"></a>海光</h3><p>如下数据可以看到在用满32个物理core之前，ipc保持稳定，超过32core后随着兵法增加ipc相应减少，性能再也上不去了。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/ded1ee0ed8d5d2fa3822e6fdfa4335f1.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/0f2410165932835a36d8c0611877ae77.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/67df9ff04209a00bd864ba21b7593477.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/1bc01f6e880c7e49672170f940ff40a0.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/307d30c2b3507d5561d774f96b13e67a.png" alt="image.png"></p>
<h3 id="鲲鹏920-2"><a href="#鲲鹏920-2" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><p>可以看到<strong>鲲鹏920多核跑openssl是没有什么争抢的，所以还能保证完全线性</strong></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/39720b5eb41937b462e1772854e2d832.png" alt="image.png"></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/a98a482a10f09bccd4a6ac49fd2850b9.png" alt="image.png"></p>
<p>intel的流水线适合跑高带宽应用，不适合跑密集计算应用，也就是intel的pipeline数量少，但是内存读写上面优化好，乱序优化好。跑纯计算，不是intel的强项。</p>
<p>数据库场景下鲲鹏920大概相当于X86的70%的能力</p>
<p>prime计算已搬走的fpu，不走cpu</p>
<h2 id="飞腾ARM芯片性能数据"><a href="#飞腾ARM芯片性能数据" class="headerlink" title="飞腾ARM芯片性能数据"></a>飞腾ARM芯片性能数据</h2><p><strong>飞腾处理器</strong>，又称<strong>银河飞腾处理器</strong>，是由<a href="https://zh.wikipedia.org/wiki/中國人民解放軍國防科學技術大學" target="_blank" rel="noopener">中国人民解放军国防科学技术大学</a>研制的一系列嵌入式<a href="https://zh.wikipedia.org/wiki/数字信号处理器" target="_blank" rel="noopener">数字信号处理器</a>（DSP）和<a href="https://zh.wikipedia.org/wiki/中央处理器" target="_blank" rel="noopener">中央处理器</a>（CPU）芯片。<a href="https://zh.wikipedia.org/wiki/飞腾处理器#cite_note-cw-1" target="_blank" rel="noopener">[1]</a>这个处理器系列的研发，是由国防科技大的<a href="https://zh.wikipedia.org/w/index.php?title=邢座程&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">邢座程</a>教授<a href="https://zh.wikipedia.org/wiki/飞腾处理器#cite_note-2" target="_blank" rel="noopener">[2]</a>带领的团队负责研发。<a href="https://zh.wikipedia.org/wiki/飞腾处理器#cite_note-Xing_671-3" target="_blank" rel="noopener">[3]</a>其<a href="https://zh.wikipedia.org/w/index.php?title=商業化&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">商业化</a><a href="https://zh.wikipedia.org/wiki/推廣" target="_blank" rel="noopener">推广</a>则是由<a href="https://zh.wikipedia.org/wiki/中国电子信息产业集团有限公司" target="_blank" rel="noopener">中国电子信息产业集团有限公司</a>旗下的天津飞腾信息技术有限公司负责</p>
<h3 id="测试芯片详细信息"><a href="#测试芯片详细信息" class="headerlink" title="测试芯片详细信息"></a><a href="https://pdf.dfcfw.com/pdf/H3_AP202010201422468889_1.pdf?1603181661000.pdf" target="_blank" rel="noopener">测试芯片详细信息</a></h3><p>2020 年 7 月 23 日，飞腾发布新一代高可扩展多路服务器芯片腾云 S2500，采用 16nm 工艺， 主频 2.0~2.2Ghz，拥有 64 个 FTC663 内核，片内集成 64MB 三级 Cache，支持 8 个 DDR4-3200 存 储通道，功耗 150W。 </p>
<p>基于 ARM 架构，兼具高可拓展性和低功耗，扩展支持 2 路到 8 路直连。与主流架构 X86 相比， ARM 架构具备低功耗、低发热和低成本的优势，ARM 单核的面积仅为 X86 核的 1/7，同样芯片尺 寸下可以继承更多核心数，可以通过增加核心数提高性能，在性能快速提升下，也能保持较低的 功耗，符合云计算场景下并行计算上高并发和高效率的要求，也能有效控制服务器的能耗和成本 支出。腾云 S2500 增加了 4 个直连接口，总带宽 800Gbps，支持 2 路、4 路和 8 路直连，具备高可 拓展性，可以形成 128 核到 512 核的计算机系统，带动算力提升。</p>
<p>飞腾(FT2500), ARMv8架构，主频2.1G，服务器两路，每路64个物理core，没有超线程，总共16个numa，每个numa 8个core</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">#dmidecode -t processor</span><br><span class="line"># dmidecode 3.0</span><br><span class="line">Getting SMBIOS data from sysfs.</span><br><span class="line">SMBIOS 3.2.0 present.</span><br><span class="line"># SMBIOS implementations newer than version 3.0 are not</span><br><span class="line"># fully supported by this version of dmidecode.</span><br><span class="line"></span><br><span class="line">Handle 0x0004, DMI type 4, 48 bytes</span><br><span class="line">Processor Information</span><br><span class="line">	Socket Designation: BGA3576</span><br><span class="line">	Type: Central Processor</span><br><span class="line">	Family: &lt;OUT OF SPEC&gt;</span><br><span class="line">	Manufacturer: PHYTIUM</span><br><span class="line">	ID: 00 00 00 00 70 1F 66 22</span><br><span class="line">	Version: FT2500</span><br><span class="line">	Voltage: 0.8 V</span><br><span class="line">	External Clock: 50 MHz</span><br><span class="line">	Max Speed: 2100 MHz</span><br><span class="line">	Current Speed: 2100 MHz</span><br><span class="line">	Status: Populated, Enabled</span><br><span class="line">	Upgrade: Other</span><br><span class="line">	L1 Cache Handle: 0x0005</span><br><span class="line">	L2 Cache Handle: 0x0007</span><br><span class="line">	L3 Cache Handle: 0x0008</span><br><span class="line">	Serial Number: 1234567</span><br><span class="line">	Asset Tag: No Asset Tag</span><br><span class="line">	Part Number: NULL</span><br><span class="line">	Core Count: 64</span><br><span class="line">	Core Enabled: 64</span><br><span class="line">	Thread Count: 64</span><br><span class="line">	Characteristics:</span><br><span class="line">		64-bit capable</span><br><span class="line">		Multi-Core</span><br><span class="line">		Hardware Thread</span><br><span class="line">		Execute Protection</span><br><span class="line">		Enhanced Virtualization</span><br><span class="line">		Power/Performance Control</span><br><span class="line">			</span><br><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                128</span><br><span class="line">On-line CPU(s) list:   0-127</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    64</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          16</span><br><span class="line">Model:                 3</span><br><span class="line">BogoMIPS:              100.00</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              2048K</span><br><span class="line">L3 cache:              65536K</span><br><span class="line">NUMA node0 CPU(s):     0-7</span><br><span class="line">NUMA node1 CPU(s):     8-15</span><br><span class="line">NUMA node2 CPU(s):     16-23</span><br><span class="line">NUMA node3 CPU(s):     24-31</span><br><span class="line">NUMA node4 CPU(s):     32-39</span><br><span class="line">NUMA node5 CPU(s):     40-47</span><br><span class="line">NUMA node6 CPU(s):     48-55</span><br><span class="line">NUMA node7 CPU(s):     56-63</span><br><span class="line">NUMA node8 CPU(s):     64-71</span><br><span class="line">NUMA node9 CPU(s):     72-79</span><br><span class="line">NUMA node10 CPU(s):    80-87</span><br><span class="line">NUMA node11 CPU(s):    88-95</span><br><span class="line">NUMA node12 CPU(s):    96-103</span><br><span class="line">NUMA node13 CPU(s):    104-111</span><br><span class="line">NUMA node14 CPU(s):    112-119</span><br><span class="line">NUMA node15 CPU(s):    120-127</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</span><br><span class="line"></span><br><span class="line">node distances:</span><br><span class="line">node   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15</span><br><span class="line">  0:  10  20  40  30  20  30  50  40  100  100  100  100  100  100  100  100</span><br><span class="line">  1:  20  10  30  40  50  20  40  50  100  100  100  100  100  100  100  100</span><br><span class="line">  2:  40  30  10  20  40  50  20  30  100  100  100  100  100  100  100  100</span><br><span class="line">  3:  30  40  20  10  30  20  40  50  100  100  100  100  100  100  100  100</span><br><span class="line">  4:  20  50  40  30  10  50  30  20  100  100  100  100  100  100  100  100</span><br><span class="line">  5:  30  20  50  20  50  10  50  40  100  100  100  100  100  100  100  100</span><br><span class="line">  6:  50  40  20  40  30  50  10  30  100  100  100  100  100  100  100  100</span><br><span class="line">  7:  40  50  30  50  20  40  30  10  100  100  100  100  100  100  100  100</span><br><span class="line">  8:  100  100  100  100  100  100  100  100  10  20  40  30  20  30  50  40</span><br><span class="line">  9:  100  100  100  100  100  100  100  100  20  10  30  40  50  20  40  50</span><br><span class="line"> 10:  100  100  100  100  100  100  100  100  40  30  10  20  40  50  20  30</span><br><span class="line"> 11:  100  100  100  100  100  100  100  100  30  40  20  10  30  20  40  50</span><br><span class="line"> 12:  100  100  100  100  100  100  100  100  20  50  40  30  10  50  30  20</span><br><span class="line"> 13:  100  100  100  100  100  100  100  100  30  20  50  20  50  10  50  40</span><br><span class="line"> 14:  100  100  100  100  100  100  100  100  50  40  20  40  30  50  10  30</span><br><span class="line"> 15:  100  100  100  100  100  100  100  100  40  50  30  50  20  40  30  10</span><br></pre></td></tr></table></figure>
<p><img src="/Users/ren/src/blog/951413iMgBlog/image-20210422121346490.png" alt="image-20210422121346490"></p>
<h3 id="numa太多，每个numa下core比较少"><a href="#numa太多，每个numa下core比较少" class="headerlink" title="numa太多，每个numa下core比较少"></a>numa太多，每个numa下core比较少</h3><p>导致跨numa高概率发生，如下是在正常部署下的测试perf 数据，可以看到IPC极低，才0.08，同样的场景在其他家芯片都能打到0.6</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/16b271c8-5132-4273-a26a-4b35e8f92882.png" alt="img"></p>
<p>执行绑核，将一个进程限制在2个numa内，因为进程需要16core，理论上用8core的进程性能会更好</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/4d4fdebb-6146-407e-881d-19170fbfd82b.png" alt="img"></p>
<p>可以看到IPC从0.08提升到了0.22，实际能到0.27，对应的业务测试QPS也是原来的4倍。</p>
<h3 id="飞腾2500-和-鲲鹏9200-参数对比"><a href="#飞腾2500-和-鲲鹏9200-参数对比" class="headerlink" title="飞腾2500 和 鲲鹏9200 参数对比"></a>飞腾2500 和 鲲鹏9200 参数对比</h3><p><img src="/Users/ren/src/blog/951413iMgBlog/image-20210422095217195.png" alt="image-20210422095217195"></p>
<h2 id="intel-x86-cpu-bound和memory-bond数据"><a href="#intel-x86-cpu-bound和memory-bond数据" class="headerlink" title="intel x86 cpu bound和memory bond数据"></a>intel x86 cpu bound和memory bond数据</h2><p>测试代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;emmintrin.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;signal.h&gt;</span><br><span class="line"></span><br><span class="line">char a = 1;</span><br><span class="line"></span><br><span class="line">void memory_bound() &#123;</span><br><span class="line">        register unsigned i=0;</span><br><span class="line">        register char b;</span><br><span class="line"></span><br><span class="line">        for (i=0;i&lt;(1u&lt;&lt;24);i++) &#123;</span><br><span class="line">                // evict cacheline containing a</span><br><span class="line">                 _mm_clflush(&amp;a);</span><br><span class="line">                 b = a;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">void cpu_bound() &#123;</span><br><span class="line">        register unsigned i=0;</span><br><span class="line">        for (i=0;i&lt;(1u&lt;&lt;31);i++) &#123;</span><br><span class="line">                __asm__ (&quot;nop\nnop\nnop&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">int main() &#123;</span><br><span class="line">        int i=0;</span><br><span class="line">	      for(i=0;i&lt;10; ++i)&#123;</span><br><span class="line">	             //cpu_bound();</span><br><span class="line">        	     memory_bound();</span><br><span class="line">	      &#125;</span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如下测试perf数据可以看到IPC的明显差异</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># sudo perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores -a ./memory_bound</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;system wide&apos;:</span><br><span class="line"></span><br><span class="line">    36,162,872,212      branch-instructions                                           (14.21%)</span><br><span class="line">       586,644,153      branch-misses             #    1.62% of all branches          (12.95%)</span><br><span class="line">     4,632,787,085      bus-cycles                                                    (14.40%)</span><br><span class="line">       476,189,785      cache-misses              #   17.714 % of all cache refs      (14.38%)</span><br><span class="line">     2,688,284,129      cache-references                                              (14.35%)</span><br><span class="line">   258,946,713,506      cpu-cycles                                                    (17.93%)</span><br><span class="line">   181,069,328,200      instructions              #    0.70  insn per cycle           (21.51%)</span><br><span class="line">   456,889,428,341      ref-cycles                                                    (22.31%)</span><br><span class="line">     3,928,434,098      L1-dcache-load-misses     #    7.46% of all L1-dcache hits    (14.21%)</span><br><span class="line">    52,656,559,902      L1-dcache-loads                                               (14.31%)</span><br><span class="line">    26,711,751,387      L1-dcache-stores                                              (14.30%)</span><br><span class="line">     2,618,739,340      L1-icache-load-misses                                         (18.05%)</span><br><span class="line">       154,326,888      LLC-load-misses           #    8.60% of all LL-cache hits     (19.84%)</span><br><span class="line">     1,795,112,198      LLC-loads                                                     (9.81%)</span><br><span class="line">        66,802,375      LLC-store-misses                                              (10.19%)</span><br><span class="line">       206,810,811      LLC-stores                                                    (11.16%)</span><br><span class="line">       586,120,789      branch-load-misses                                            (14.28%)</span><br><span class="line">    36,121,237,395      branch-loads                                                  (14.29%)</span><br><span class="line">       114,927,298      dTLB-load-misses          #    0.22% of all dTLB cache hits   (14.29%)</span><br><span class="line">    52,902,163,128      dTLB-loads                                                    (14.29%)</span><br><span class="line">         7,010,297      dTLB-store-misses                                             (14.29%)</span><br><span class="line">    26,587,353,417      dTLB-stores                                                   (18.00%)</span><br><span class="line">       106,209,281      iTLB-load-misses          #  174.17% of all iTLB cache hits   (19.33%)</span><br><span class="line">        60,978,626      iTLB-loads                                                    (21.53%)</span><br><span class="line">       117,197,042      node-load-misses                                              (19.71%)</span><br><span class="line">        35,764,508      node-loads                                                    (11.65%)</span><br><span class="line">        57,655,994      node-store-misses                                             (7.80%)</span><br><span class="line">        11,563,328      node-stores                                                   (9.45%)</span><br><span class="line"></span><br><span class="line">      16.700731355 seconds time elapsed</span><br><span class="line">      </span><br><span class="line"># sudo perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores -a ./cpu_bound</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;system wide&apos;:</span><br><span class="line"></span><br><span class="line">    43,013,055,562      branch-instructions                                           (14.33%)</span><br><span class="line">       436,722,063      branch-misses             #    1.02% of all branches          (11.58%)</span><br><span class="line">     3,154,327,457      bus-cycles                                                    (14.31%)</span><br><span class="line">       306,977,772      cache-misses              #   17.837 % of all cache refs      (14.42%)</span><br><span class="line">     1,721,062,233      cache-references                                              (14.39%)</span><br><span class="line">   176,119,834,487      cpu-cycles                                                    (17.98%)</span><br><span class="line">   276,038,539,571      instructions              #    1.57  insn per cycle           (21.55%)</span><br><span class="line">   309,334,354,268      ref-cycles                                                    (22.31%)</span><br><span class="line">     2,551,915,790      L1-dcache-load-misses     #    6.78% of all L1-dcache hits    (13.12%)</span><br><span class="line">    37,638,319,334      L1-dcache-loads                                               (14.32%)</span><br><span class="line">    19,132,537,445      L1-dcache-stores                                              (15.73%)</span><br><span class="line">     1,834,976,400      L1-icache-load-misses                                         (18.90%)</span><br><span class="line">       131,307,343      LLC-load-misses           #   11.46% of all LL-cache hits     (19.94%)</span><br><span class="line">     1,145,964,874      LLC-loads                                                     (16.60%)</span><br><span class="line">        45,561,247      LLC-store-misses                                              (8.11%)</span><br><span class="line">       140,236,535      LLC-stores                                                    (9.60%)</span><br><span class="line">       423,294,349      branch-load-misses                                            (14.27%)</span><br><span class="line">    46,645,623,485      branch-loads                                                  (14.28%)</span><br><span class="line">        73,377,533      dTLB-load-misses          #    0.19% of all dTLB cache hits   (14.28%)</span><br><span class="line">    37,905,428,246      dTLB-loads                                                    (15.69%)</span><br><span class="line">         4,969,973      dTLB-store-misses                                             (17.21%)</span><br><span class="line">    18,729,947,580      dTLB-stores                                                   (19.71%)</span><br><span class="line">        72,073,313      iTLB-load-misses          #  167.86% of all iTLB cache hits   (20.60%)</span><br><span class="line">        42,935,532      iTLB-loads                                                    (19.16%)</span><br><span class="line">       112,306,453      node-load-misses                                              (15.35%)</span><br><span class="line">        37,239,267      node-loads                                                    (7.44%)</span><br><span class="line">        37,455,335      node-store-misses                                             (10.00%)</span><br><span class="line">         8,134,155      node-stores                                                   (8.87%)</span><br><span class="line"></span><br><span class="line">      10.838808208 seconds time elapsed</span><br></pre></td></tr></table></figure>
<h3 id="对比下飞腾芯片"><a href="#对比下飞腾芯片" class="headerlink" title="对比下飞腾芯片"></a>对比下飞腾芯片</h3><p>ipc 大概是intel的30%，加上主频也要差一些，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#time perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses -a ./cpu_bound</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;system wide&apos;:</span><br><span class="line"></span><br><span class="line">       10496356859      branch-misses                                                 (37.60%)</span><br><span class="line">     2813170983911      bus-cycles                                                    (37.58%)</span><br><span class="line">       17604745519      cache-misses              #    3.638 % of all cache refs      (37.55%)</span><br><span class="line">      483878256161      cache-references                                              (37.54%)</span><br><span class="line">     2818545529083      cpu-cycles                                                    (43.78%)</span><br><span class="line">     1280497827941      instructions              #    0.45  insns per cycle          (43.78%)</span><br><span class="line">       17623592806      L1-dcache-load-misses     #    3.65% of all L1-dcache hits    (43.78%)</span><br><span class="line">      482429613337      L1-dcache-loads                                               (41.83%)</span><br><span class="line">       17604561232      L1-dcache-store-misses                                        (37.53%)</span><br><span class="line">      484126081882      L1-dcache-stores                                              (37.52%)</span><br><span class="line">       17774514325      L1-icache-load-misses                                         (37.50%)</span><br><span class="line">      641046300400      L1-icache-loads                                               (37.50%)</span><br><span class="line">       10574973722      branch-load-misses                                            (39.45%)</span><br><span class="line">      273851009656      branch-loads                                                  (43.76%)</span><br><span class="line">        9457594390      dTLB-load-misses                                              (43.77%)</span><br><span class="line">        1813954093      iTLB-load-misses                                              (43.77%)</span><br><span class="line"></span><br><span class="line">      31.172754504 seconds time elapsed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real	0m31.284s</span><br><span class="line">user	0m31.096s</span><br><span class="line">sys	0m0.165s</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>对纯CPU 运算场景，并发不超过物理core时，比如Prime运算，比如DRDS(CPU bound，IO在网络，可以加并发弥补)<ul>
<li>海光的IPC能保持稳定；</li>
<li>intel的IPC有所下降，但是QPS在IPC下降后还能完美线性</li>
</ul>
</li>
<li>在openssl和MySQL oltp_read_only场景下<ul>
<li>如果并发没超过物理core数时，海光和Intel都能随着并发的翻倍性能能增加80%</li>
<li>如果并发超过物理core数后，Intel还能随着并发的翻倍性能增加50%，海光增加就只有20%了</li>
<li>简单理解在这两个场景下Intel的HT能发挥半个物理core的作用，海光的HT就只能发挥0.2个物理core的作用了</li>
</ul>
</li>
<li>海光zen1的AMD 架构，每个core只有一个fpu，综上在多个场景下HT基本上都可以忽略</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://dino.ciuffetti.info/2011/07/howto-java-huge-pages-linux/" target="_blank" rel="noopener">How to use Huge Pages with Java and Linux</a>这个资料中提到了Java使用HugePage的时候启动进程的用户权限问题，在我的docker容器中用的admin启动的进程，测试验证是不需要按资料中的设置。</p>
<p><a href="https://www.atatech.org/articles/157681" target="_blank" rel="noopener">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<p><a href="https://bbs.huaweicloud.com/blogs/146367" target="_blank" rel="noopener">华为TaiShan服务器ARMNginx应用调优案例 大量绑核、中断、Numa等相关调优信息</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/05/MacOS下如何使用iTerm2访问水木社区/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/05/MacOS下如何使用iTerm2访问水木社区/" itemprop="url">MacOS下如何使用iTerm2访问水木社区</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-05T17:30:03+08:00">
                2021-03-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/03/05/MacOS下如何使用iTerm2访问水木社区/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/03/05/MacOS下如何使用iTerm2访问水木社区/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MacOS下如何使用iTerm2访问水木社区"><a href="#MacOS下如何使用iTerm2访问水木社区" class="headerlink" title="MacOS下如何使用iTerm2访问水木社区"></a>MacOS下如何使用iTerm2访问水木社区</h1><p>关键字： MacOS、iTerm 、Dracula、ssh、bbs.newsmth.net</p>
<p>windows下有各种Term软件来帮助我们通过ssh访问bbs.newsmth.net, 但是工作环境切换到MacOS后发现FTerm、CTerm这样的工具都没有对应的了。但是term下访问 bbs.newsmth.net 简直是太爽了，所以本文希望解决这个问题。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>ssh 访问 bbs.newsmth.net 是没问题的，但是<strong>要解决配色和字符编码问题</strong></p>
<h3 id="解决编码"><a href="#解决编码" class="headerlink" title="解决编码"></a>解决编码</h3><p>在iTerm2的配置中增加一个profile，如下图 smth，主要是改字符编码集为 GB 18030，然后修改配色方案，我喜欢的Dracula不适合SMTH，十大完全看不了。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/c86ee5401de32e6692d3d65ccfe0041a.png" alt="image.png"></p>
<p>然后增加一个profile切换脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat ~/src/script/encode.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line"># 使用GBK Profile</span><br><span class="line">echo -e &quot;\033]50;SetProfile=smth\a&quot;</span><br><span class="line"># 更改当前 iTerm2 tab title</span><br><span class="line">echo -ne &quot;\033]0;&quot;$@&quot;\007&quot;</span><br><span class="line">$@</span><br><span class="line">echo -ne &quot;\033]0;&quot;$&#123;PWD/#$HOME/~&#125;&quot;\007&quot;</span><br><span class="line">echo -e &quot;\033]50;SetProfile=Default\a&quot;</span><br></pre></td></tr></table></figure>
<p>Encode.sh用来解决profile切换，连smth前切换成GB 18030，断开的时候恢复成UTF-8，要不然的话正常工作的命令行就乱码了。</p>
<p>这行命令保存为可执行文件smth, 用于通过 ssh连上 bbs.newsmth.net </p>
<blockquote>
<p>/Users/ren/src/script/encode.sh sshpass -p’密码’ ssh -o ServerAliveInterval=60 水木<a href="mailto:id@bbs.newsmth.net" target="_blank" rel="noopener">id@bbs.newsmth.net</a></p>
</blockquote>
<p>最终执行命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/bin/smth</span><br><span class="line">/Users/ren/src/script/encode.sh sshpass -p&apos;密码&apos; ssh -o ServerAliveInterval=60 水木id@bbs.newsmth.net</span><br></pre></td></tr></table></figure>
<h3 id="解决配色问题"><a href="#解决配色问题" class="headerlink" title="解决配色问题"></a>解决配色问题</h3><p>然后还是在profile里面把smth的配色方案改成：Tango Dark, 一切简直是完美，工作灌水两不误，别人还发现不了</p>
<h2 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h2><p>目录（右边是工作窗口）：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/0265ed7a728bfdd6be940d838fc1feaf.png" alt="image.png"></p>
<p>十大，这个十大颜色和右边工作模式的配色方案不一样</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/252b9295375f6e6078278a6e64e1d68c.png" alt="image.png"></p>
<p>断开后恢复成 Dracula 配色和UTF-8编码，不影响工作，别的工作tab也还是正常使用utf8</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/cf8912c0634182b44fa92eeb9f854362.png" alt="image.png"></p>
<p>别的term网站也是类似，比如小百合、byr、ptt等</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/14/TCP疑难问题案例汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/14/TCP疑难问题案例汇总/" itemprop="url">TCP疑难问题案例汇总</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-14T13:30:03+08:00">
                2021-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/02/14/TCP疑难问题案例汇总/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/02/14/TCP疑难问题案例汇总/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TCP疑难问题案例汇总"><a href="#TCP疑难问题案例汇总" class="headerlink" title="TCP疑难问题案例汇总"></a>TCP疑难问题案例汇总</h1><p>碰到各种奇葩的TCP相关问题，所以汇总记录一下。分析清楚这些问题的所有来龙去脉，就能帮你在TCP知识体系里建立几个坚固的抓手，让TCP知识慢慢在抓手之间生长和互通</p>
<h2 id="服务不响应的现象或者奇怪异常的原因分析"><a href="#服务不响应的现象或者奇怪异常的原因分析" class="headerlink" title="服务不响应的现象或者奇怪异常的原因分析"></a>服务不响应的现象或者奇怪异常的原因分析</h2><p> <a href="https://plantegg.github.io/2021/02/10/%E4%B8%80%E4%B8%AA%E9%BB%91%E7%9B%92%E7%A8%8B%E5%BA%8F%E5%A5%87%E6%80%AA%E7%9A%84%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">一个黑盒程序奇怪行为的分析</a> listen端口上很快就全连接队列溢出了，导致整个程序不响应了</p>
<p><a href="https://plantegg.github.io/2020/11/02/%E4%B8%BE%E4%B8%89%E5%8F%8D%E4%B8%80--%E4%BB%8E%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%88%B0%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%A8%E5%AF%BC/" target="_blank" rel="noopener">举三反一–从理论知识到实际问题的推导</a> 服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
<p><a href="https://plantegg.github.io/2020/11/18/TCP%E8%BF%9E%E6%8E%A5%E4%B8%BA%E5%95%A5%E4%BA%92%E4%B8%B2%E4%BA%86/" target="_blank" rel="noopener">活久见，TCP连接互串了</a>  应用每过一段时间总是会抛出几个连接异常的错误，需要查明原因。排查后发现是TCP连接互串了，这个案例实在是很珍惜，所以记录一下。</p>
<p> <a href="https://plantegg.github.io/2020/07/01/如何创建一个自己连自己的TCP连接/" target="_blank" rel="noopener">如何创建一个自己连自己的TCP连接</a></p>
<h2 id="传输速度分析"><a href="#传输速度分析" class="headerlink" title="传输速度分析"></a>传输速度分析</h2><p>案例：<a href="https://plantegg.github.io/2021/01/15/TCP%E4%BC%A0%E8%BE%93%E9%80%9F%E5%BA%A6%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">TCP传输速度案例分析</a>（长肥网络、rt升高、delay ack的影响等）</p>
<p>原理：<a href="https://plantegg.github.io/2019/09/28/就是要你懂TCP--性能和发送接收Buffer的关系/" target="_blank" rel="noopener">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></p>
<p><a href="https://plantegg.github.io/2018/06/14/就是要你懂TCP--最经典的TCP性能问题/" target="_blank" rel="noopener">就是要你懂TCP–最经典的TCP性能问题 Nagle和Delay ack</a></p>
<p><a href="https://plantegg.github.io/2019/06/21/就是要你懂TCP--性能优化大全/" target="_blank" rel="noopener">就是要你懂TCP–性能优化大全</a></p>
<h2 id="TCP队列问题以及连接数"><a href="#TCP队列问题以及连接数" class="headerlink" title="TCP队列问题以及连接数"></a>TCP队列问题以及连接数</h2><p> <a href="https://plantegg.github.io/2020/11/30/一台机器上最多能创建多少个TCP连接/" target="_blank" rel="noopener">到底一台服务器上最多能创建多少个TCP连接</a></p>
<p> <a href="https://plantegg.github.io/2019/08/31/就是要你懂TCP队列--通过实战案例来展示问题/" target="_blank" rel="noopener">就是要你懂TCP队列–通过实战案例来展示问题</a></p>
<p> <a href="https://plantegg.github.io/2017/06/07/就是要你懂TCP--半连接队列和全连接队列/" target="_blank" rel="noopener">就是要你懂TCP–半连接队列和全连接队列</a></p>
<p> <a href="https://plantegg.github.io/2017/06/02/就是要你懂TCP--连接和握手/" target="_blank" rel="noopener">就是要你懂TCP–握手和挥手</a></p>
<h2 id="防火墙和reset定位分析"><a href="#防火墙和reset定位分析" class="headerlink" title="防火墙和reset定位分析"></a>防火墙和reset定位分析</h2><p>对ttl、identification等的运用</p>
<p><a href="https://plantegg.github.io/2018/08/26/关于TCP连接的KeepAlive和reset/" target="_blank" rel="noopener">关于TCP连接的Keepalive和reset</a></p>
<p><a href="https://plantegg.github.io/2019/11/06/谁动了我的TCP连接/" target="_blank" rel="noopener">就是要你懂网络–谁动了我的TCP连接</a></p>
<h2 id="TCP相关参数"><a href="#TCP相关参数" class="headerlink" title="TCP相关参数"></a>TCP相关参数</h2><p> <a href="https://plantegg.github.io/2020/01/26/TCP相关参数解释/" target="_blank" rel="noopener">TCP相关参数解释</a></p>
<p><a href="https://plantegg.github.io/2019/05/16/网络通不通是个大问题--半夜鸡叫/" target="_blank" rel="noopener">网络通不通是个大问题–半夜鸡叫</a> </p>
<p><a href="https://plantegg.github.io/2018/12/26/网络丢包/" target="_blank" rel="noopener">网络丢包</a></p>
<h2 id="工具技巧篇"><a href="#工具技巧篇" class="headerlink" title="工具技巧篇"></a>工具技巧篇</h2><p> <a href="https://plantegg.github.io/2019/04/21/netstat定位性能案例/" target="_blank" rel="noopener">netstat定位性能案例</a></p>
<p> <a href="https://plantegg.github.io/2017/08/28/netstat --timer/" target="_blank" rel="noopener">netstat timer keepalive explain</a></p>
<p><a href="https://plantegg.github.io/2016/10/12/ss用法大全/" target="_blank" rel="noopener">就是要你懂网络监控–ss用法大全</a></p>
<p><a href="https://plantegg.github.io/2019/06/21/就是要你懂抓包--WireShark之命令行版tshark/" target="_blank" rel="noopener">就是要你懂抓包–WireShark之命令行版tshark</a></p>
<p><a href="https://plantegg.github.io/2018/01/01/通过tcpdump对Unix Socket 进行抓包解析/" target="_blank" rel="noopener">通过tcpdump对Unix Domain Socket 进行抓包解析</a></p>
<p><a href="https://plantegg.github.io/2017/12/07/如何追踪网络流量/" target="_blank" rel="noopener">如何追踪网络流量</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/10/一个黑盒程序奇怪的行为分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/10/一个黑盒程序奇怪的行为分析/" itemprop="url">一个黑盒程序奇怪行为的分析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-10T10:30:03+08:00">
                2021-02-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/02/10/一个黑盒程序奇怪的行为分析/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/02/10/一个黑盒程序奇怪的行为分析/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一个黑盒程序奇怪行为的分析"><a href="#一个黑盒程序奇怪行为的分析" class="headerlink" title="一个黑盒程序奇怪行为的分析"></a>一个黑盒程序奇怪行为的分析</h1><h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><blockquote>
<p>从金主baba手里拿到一个区块链程序，监听4000，在我们的环境中4000端口上很快就全连接队列溢出了，导致整个程序不响应了。这个程序是黑盒子，没有源代码，但是在金主baba自己的环境运行正常（一样的OS）</p>
</blockquote>
<p>如下图所示：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/623ca2f46084958efa447882cbb58e72.png" alt="image.png"></p>
<p>ss -lnt 看到全连接队列增长到了39，但是netstat -ant找不到这39个连接，本来是想看看队列堆了这么多连接，都是哪些ip连过来的，实际看不到这就奇怪了</p>
<p>同时验证过程发现我们的环境4000端口上开了slb，也就是slb会不停滴探活4000端口，关掉slb探活后一切正常了。</p>
<p>所以总结下来问题就是：</p>
<ol>
<li><p>为什么全连接队列里面的连接netstat看不到这些连接，但是ss能看到总数 </p>
</li>
<li><p>为什么关掉slb就正常了 </p>
</li>
<li><p>为什么应用不accept连接,也不close（应用是个黑盒子） </p>
</li>
</ol>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="为什么全连接队列里面的连接netstat-ss都看不到-ss能看到总数"><a href="#为什么全连接队列里面的连接netstat-ss都看不到-ss能看到总数" class="headerlink" title="为什么全连接队列里面的连接netstat/ss都看不到(ss能看到总数)"></a>为什么全连接队列里面的连接netstat/ss都看不到(ss能看到总数)</h3><p>这是因为这些连接都是探活连接，三次握手后很快被slb reset了，在OS层面这个连接已经被释放，所以肯定看不见。反过来想要是netstat能看见这个连接，那么它的状态是什么？ reset吗？tcp连接状态里肯定是没有reset状态的。</p>
<p>ss能看到总数是指只要这个连接没有被accept，那么连接队列里就还有这个连接，通过ss也能看到连接队列数量。</p>
<h4 id="为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？"><a href="#为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？" class="headerlink" title="为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？"></a>为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？</h4><p>那是因为正常情况都是能看到的，从没有考虑过握手后很快reset的情况。也没反问过如果能看到这个连接该是什么状态呢？</p>
<h4 id="这个连接被reset后，kernel会将全连接队列数量减1吗？"><a href="#这个连接被reset后，kernel会将全连接队列数量减1吗？" class="headerlink" title="这个连接被reset后，kernel会将全连接队列数量减1吗？"></a>这个连接被reset后，kernel会将全连接队列数量减1吗？</h4><p>不会，按照我们的理解连接被reset释放后，那么kernel要释放全连接队列里面的这个连接，因为这些动作都是kernel负责，上层没法处理这个reset。实际上内核认为所有 listen 到的连接, 必须要 accept 走, 用户有权利知道存在过这么一个连接。</p>
<p>也就是reset后，连接在内核层面释放了，所以netstat/ss看不到，但是全连接队列里面的应用数不会减1，只有应用accept后队列才会减1，accept这个空连接后读写会报错。基本可以认为全连接队列溢出了，主要是应用accept太慢导致的。</p>
<h4 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h4><p>三次握手成功就变成 ESTABLISHED 了，三次握手成功的第一是收到第三步的ack并且全连接队列没有满，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>下面是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -at |grep &quot;:12346 &quot;</span><br><span class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</span><br><span class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</span><br><span class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</span><br><span class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </span><br><span class="line">LISTEN      73     1024            *:terabase                 *:*</span><br></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -at |grep &quot;:12346 &quot;  </span><br><span class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</span><br><span class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</span><br><span class="line"># ss -lt</span><br><span class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </span><br><span class="line">LISTEN      1025   1024             *:terabase              *:*</span><br></pre></td></tr></table></figure>
<h3 id="为什么关掉slb就正常了"><a href="#为什么关掉slb就正常了" class="headerlink" title="为什么关掉slb就正常了"></a>为什么关掉slb就正常了</h3><p>slb探活逻辑是向监听端口发起三次握手，握手成功后立即发送一个reset断开连接</p>
<p>这是一个完整的探活过程：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b81dcbaea26a5130383d0bc8317fd3c5.png" alt="image.png"></p>
<p>关掉就正常后要结合第三个问题来讲</p>
<h3 id="为什么应用不accept连接-也不close（应用是个黑盒子）"><a href="#为什么应用不accept连接-也不close（应用是个黑盒子）" class="headerlink" title="为什么应用不accept连接,也不close（应用是个黑盒子）"></a>为什么应用不accept连接,也不close（应用是个黑盒子）</h3><p>因为应用是个黑盒子，看不到源代码，只能从行为来分析了</p>
<p>从行为来看，这个应用在三次握手后，会主动给client发送一个12字节的数据，但是这个逻辑写在了accept主逻辑内部，一旦主动给client发12字节数据失败（比如这个连接reset了）那么一直卡在这里导致应用不再accept也不再close。</p>
<p>正确的实现逻辑是，accept在一个单独的线程里，一旦accept到一个新连接，那么就开启一个新的线程来处理这个新连接的读写。accept线程专注accept。</p>
<p>关掉slb后应用有机会发出这12个字节，然后accept就能继续了，否则就卡死了。</p>
<h2 id="一些验证"><a href="#一些验证" class="headerlink" title="一些验证"></a>一些验证</h2><h3 id="nc测试连接4000端口"><a href="#nc测试连接4000端口" class="headerlink" title="nc测试连接4000端口"></a>nc测试连接4000端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># nc -p 12346 dcep-blockchain-1 4000</span><br><span class="line"> //握手后4000返回的内容</span><br><span class="line"></span><br><span class="line">抓包：</span><br><span class="line">11:03:16.762547 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [S], seq 397659761, win 43690, options [mss 65495,sackOK,TS val 2329725964 ecr 0,nop,wscale 7], length 0</span><br><span class="line">04:42:24.466211 IP dcep-blockchain-1.terabase &gt; dcep-blockchain-1.12346: Flags [S.], seq 4239354556, ack 397659762, win 43690, options [mss 65495,sackOK,TS val 2329725964 ecr 2329725964,nop,wscale 7], length 0</span><br><span class="line">11:03:16.762571 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [.], ack 1, win 342, options [nop,nop,TS val 2329725964 ecr 2329725964], length 0</span><br><span class="line"></span><br><span class="line">----到这三次握手完毕，下面是隔了大概1.5ms，4000发了12字节给nc</span><br><span class="line">11:03:16.763893 IP dcep-blockchain-1.terabase &gt; dcep-blockchain-1.12346: Flags [P.], seq 1:13, ack 1, win 342, options [nop,nop,TS val 2329725966 ecr 2329725964], length 12</span><br><span class="line">11:03:16.763904 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [.], ack 13, win 342, options [nop,nop,TS val 2329725966 ecr 2329725966], length 0</span><br></pre></td></tr></table></figure>
<p>如果在上面的1.5ms之间nc  reset了这个连接，那么这12字节就发不出来了</p>
<h3 id="握手后Server主动发数据的行为非常像MySQL-Server"><a href="#握手后Server主动发数据的行为非常像MySQL-Server" class="headerlink" title="握手后Server主动发数据的行为非常像MySQL Server"></a>握手后Server主动发数据的行为非常像MySQL Server</h3><p>MySQL Server在收到mysql client连接后会主动发送 Server Greeting、版本号、认证方式等给client</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#nc -p 12345 127.0.0.1 3306</span><br><span class="line">J</span><br><span class="line">5.6.29�CuaV9v0xo�!</span><br><span class="line">                  qCHRrGRIJqvzmysql_native_password  </span><br><span class="line">                  </span><br><span class="line">#tcpdump -i any port 12345 -ennt</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 76: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [S], seq 3186409724, win 43690, options [mss 65495,sackOK,TS val 3967896050 ecr 0,nop,wscale 7], length 0</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 76: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [S.], seq 4188709983, ack 3186409725, win 43690, options [mss 65495,sackOK,TS val 3967896051 ecr 3967896050,nop,wscale 7], length 0</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 1, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 0 // 握手完毕</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 146: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [P.], seq 1:79, ack 1, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 78 //Server 发出Greeting</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 79, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 0</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [F.], seq 79, ack 1, win 342, options [nop,nop,TS val 3967913551 ecr 3967896051], length 0</span><br><span class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 80, win 342, options [nop,nop,TS val 3967913591 ecr 3967913551], length 0</span><br></pre></td></tr></table></figure>
<p>如下是Server发出的长度为 78 的 Server Greeting信息内容</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/203c52d94018bbf72dfd4fc64d8a237b.png" alt="image.png"></p>
<p>理论上如果slb探活连接检查MySQL Server的状态的时候也是很快reset了，如果MySQL Server程序写得烂也会出现同样的情况。</p>
<p>但是比如我们有实验验证MySQL  Server 是否正常的时候会用 nc 去测试，一般以能看到</p>
<blockquote>
<p>5.6.29�CuaV9v0xo�!<br>                  qCHRrGRIJqvzmysql_native_password </p>
</blockquote>
<p>就认为MySQL Server是正常的。但是真的是这样吗？我们看看 nc 的如下案例</p>
<h4 id="nc-6-4-快速fin"><a href="#nc-6-4-快速fin" class="headerlink" title="nc 6.4 快速fin"></a>nc 6.4 快速fin</h4><blockquote>
<p>#nc –version<br>Ncat: Version 6.40 ( <a href="http://nmap.org/ncat" target="_blank" rel="noopener">http://nmap.org/ncat</a> )</p>
</blockquote>
<p>用 nc 测试发现有一定的概率没有出现上面的Server Greeting信息，那么这是因为MySQL Server服务不正常了吗？</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2020/png/33359/1607660605575-1305739f-1621-4a01-89ad-0f81eef94922.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<blockquote>
<p> nc -i 3 10.97.170.11 3306 -w 4 -p 1234</p>
</blockquote>
<p>-i 3 表示握手成功后 等三秒钟nc退出（发fin）</p>
<p>nc 6.4 握手后立即发fin断开连接，导致可能收不到Greeting，换成7.5或者mysql client就OK了</p>
<p>nc 7.5的抓包，明显可以看到nc在发fin前会先等4秒钟：</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2020/png/33359/1607660937618-d66c4074-9aa2-44cb-8054-f7d3680d1181.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<h3 id="tcpping-模拟slb-探活"><a href="#tcpping-模拟slb-探活" class="headerlink" title="tcpping 模拟slb 探活"></a>tcpping 模拟slb 探活</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tcpping.py -R -i 0.1 -t 1 dcep-blockchain-1 4000</span><br></pre></td></tr></table></figure>
<p>-i 间隔0.1秒 </p>
<p>-R reset断开连接</p>
<p>-t 超时时间1秒</p>
<p>执行如上代码，跟4000端口握手，然后立即发出reset断开连接（完全模拟slb探活行为），很快重现了问题</p>
<p>增加延时</p>
<p>-D 0.01表示握手成功后10ms后再发出reset（让应用有机会成功发出那12个字节），应用工作正常</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tcpping.py -R -i 0.1 -t 1 -D 0.01 dcep-blockchain-1 4000</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最大的错误认知就是 ss 看到的全连接队列数量，netstat也能看到。实际是不一定，而这个快速reset+应用不accept就导致了看不到这个现象</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/28/journald和rsyslog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/28/journald和rsyslog/" itemprop="url">journald和rsyslogd</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-28T17:30:03+08:00">
                2021-01-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/01/28/journald和rsyslog/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/01/28/journald和rsyslog/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="journald和rsyslogd"><a href="#journald和rsyslogd" class="headerlink" title="journald和rsyslogd"></a>journald和rsyslogd</h1><p>碰到rsyslog-8.24.0-34.1.al7.x86_64 的 rsyslogd 占用内存过高，于是分析了一下原因并学习了一下系统日志、rsyslog、journald之间的关系，流水账记录此文。</p>
<h2 id="rsyslogd-占用内存过高的分析"><a href="#rsyslogd-占用内存过高的分析" class="headerlink" title="rsyslogd 占用内存过高的分析"></a>rsyslogd 占用内存过高的分析</h2><p>rsyslogd使用了大概1.6-2G内存，不正常（正常情况下内存占用30-50M之间）</p>
<p>现象：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/12d137f9416d7935dbe6540c626ca8b4.png" alt="image.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">KiB Mem :  7971268 total,   131436 free,  7712020 used,   127812 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.    43484 avail Mem</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">24850 admin     20   0 8743896   5.1g      0 S   2.0 66.9   1413:55 java</span><br><span class="line"> 1318 root      20   0 2380404   1.6g    536 S   0.0 21.6 199:09.36 rsyslogd</span><br><span class="line"> </span><br><span class="line"># systemctl status rsyslog</span><br><span class="line">● rsyslog.service - System Logging Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Tue 2020-10-20 16:01:01 CST; 3 months 8 days ago</span><br><span class="line">     Docs: man:rsyslogd(8)</span><br><span class="line">           http://www.rsyslog.com/doc/</span><br><span class="line"> Main PID: 1318 (rsyslogd)</span><br><span class="line">   CGroup: /system.slice/rsyslog.service</span><br><span class="line">           └─1318 /usr/sbin/rsyslogd -n</span><br><span class="line"></span><br><span class="line">Jan 28 09:10:07 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</span><br><span class="line">Jan 28 09:10:07 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</span><br><span class="line">Jan 28 10:27:48 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</span><br><span class="line">Jan 28 10:27:49 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</span><br><span class="line">Jan 28 11:45:23 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</span><br><span class="line">Jan 28 11:45:24 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</span><br><span class="line">Jan 28 13:03:00 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</span><br><span class="line">Jan 28 13:03:01 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</span><br><span class="line">Jan 28 14:20:42 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</span><br><span class="line">Jan 28 14:20:42 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ] </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># grep HUPed /var/log/messages</span><br><span class="line">Jan 24 03:39:15 iZwz95gaul6x9167sqdqz4Z rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;8.24.0-34.1.al7&quot; x-pid=&quot;1318&quot; x-info=&quot;http://www.rsyslog.com&quot;] rsyslogd was HUPed</span><br><span class="line"></span><br><span class="line"># journalctl --verify</span><br><span class="line">PASS: /var/log/journal/20190829214900434421844640356160/system@efef6fd56e2e4c9f861d0be25c8c0781-0000000001567546-0005b9e2e02a0a4f.journal</span><br><span class="line">PASS: /var/log/journal/20190829214900434421844640356160/system@efef6fd56e2e4c9f861d0be25c8c0781-00000000015ae56b-0005b9ea76e922e9.journal</span><br><span class="line">1be1e0: Data object references invalid entry at 1d03018</span><br><span class="line">File corruption detected at /var/log/journal/20190829214900434421844640356160/system.journal:1d02d80 (of 33554432 bytes, 90%).</span><br><span class="line">FAIL: /var/log/journal/20190829214900434421844640356160/system.journal (Bad message)</span><br></pre></td></tr></table></figure>
<p><code>journalctl --verify</code>命令检查发现系统日志卷文件损坏</p>
<h3 id="问题根因"><a href="#问题根因" class="headerlink" title="问题根因"></a>问题根因</h3><p><a href="https://access.redhat.com/solutions/3705051" target="_blank" rel="noopener">来自redhat官网的描述</a></p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/e1a1cd75553b5cbe2a64e835ba9f99a7.png" alt="image.png"></p>
<p>以下是现场收集到的日志：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/cdfe3fb8d50ee148b816a82a432f1b88.png" alt="image.png"></p>
<p>主要是rsyslogd的sd_journal_get_cursor报错，然后导致内存泄露。</p>
<p>journald 报Bad message, 跟rsyslogd内存泄露完全没关系，实际上升级rsyslogd后也有journald bad message,但是rsyslogd的内存一直稳定在30M以内</p>
<p><a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="noopener">这个CSDN的文章中有完全一样的症状</a> 但是作者的结论是：这是systemd的bug，在journald需要压缩的时候就会发生这个问题。实际上我用的是 systemd-219-62.6.al7.9.x86_64 比他描述的已经修复的版本还要要新，也还是有这个问题，所以这个结论是不对的</p>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>1、重启rsyslog <code>systemctl restart rsyslog</code> 可以释放内存</p>
<p>2、升级rsyslog到rsyslog-8.24.0-38.1.al7.x86_64或更新的版本才能彻底修复这个问题</p>
<h3 id="一些配置方法"><a href="#一些配置方法" class="headerlink" title="一些配置方法"></a>一些配置方法</h3><p>修改配置/etc/rsyslog.conf，增加如下两行，然后重启<code>systemctl restart rsyslog</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$imjournalRatelimitInterval 0</span><br><span class="line">$imjournalRatelimitBurst 0</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<p>1、关掉journal压缩配置</p>
<p>vi /etc/systemd/journald.conf，把#Compress=yes改成Compress=no，之后systemctl restart systemd-journald即可</p>
<p>2、限制rsyslogd 内存大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/systemd/system/multi-user.target.wants/rsyslog.service</span><br><span class="line"></span><br><span class="line">在Service配置中添加MemoryAccounting=yes，MemoryMax=80M，MemoryHigh=8M三项如下所示。</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=-/etc/sysconfig/rsyslog</span><br><span class="line">ExecStart=/usr/sbin/rsyslogd -n $SYSLOGD_OPTIONS</span><br><span class="line">Restart=on-failure</span><br><span class="line">UMask=0066</span><br><span class="line">StandardOutput=null</span><br><span class="line">Restart=on-failure</span><br><span class="line">MemoryAccounting=yes</span><br><span class="line">MemoryMax=80M</span><br><span class="line">MemoryHigh=8M</span><br></pre></td></tr></table></figure>
<h2 id="OOM-kill"><a href="#OOM-kill" class="headerlink" title="OOM kill"></a>OOM kill</h2><p>rsyslogd内存消耗过高后导致了OOM Kill</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/c7332f5b48506ea1faa015cfc6ae1709.png" alt="image.png"></p>
<p><strong>RSS对应物理内存，单位是4K（page大小）</strong>，红框两个进程用了5G+2G，总内存是8G，所以触发OOM killer了</p>
<p>每次OOM Kill日志前后总带着systemd-journald的重启</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z journal: Permanent journal is using 520.0M (max allowed 500.0M, trying to leave 4.0G free of 83.7G available → current limit 520.0M).</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z journal: Journal started</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: AliYunDun invoked oom-killer: gfp_mask=0x6200ca(GFP_HIGHUSER_MOVABLE), nodemask=(null), order=0, oom_score_adj=0</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: AliYunDun cpuset=/ mems_allowed=0</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: CPU: 3 PID: 13296 Comm: AliYunDun Tainted: G           OE     4.19.57-15.1.al7.x86_64 #1</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/01/2014</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Call Trace:</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: dump_stack+0x5c/0x7b</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: dump_header+0x77/0x29f</span><br><span class="line">***</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: [  18118]     0 18118    28218      255   245760        0             0 sshd</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Out of memory: Kill process 18665 (java) score 617 or sacrifice child</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Killed process 18665 (java) total-vm:8446992kB, anon-rss:4905856kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: oom_reaper: reaped process 18665 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z systemd: systemd-journald.service watchdog timeout (limit 3min)!</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z rsyslogd: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</span><br><span class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z rsyslogd: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</span><br><span class="line">Jan 28 20:14:38 iZwz95gaul6x9167sqdqz5Z rsyslogd: imjournal: journal reloaded... [v8.24.0-57.1.al7 try http://www.rsyslog.com/e/0 ]</span><br></pre></td></tr></table></figure>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/45008a8323742fb7f145211a6281afbc.png" alt="image.png"></p>
<p>OOM kill前大概率伴随着systemd-journald 重启是因为watch dog timeout(limit 3min)，造成timeout的原因是journald定期要把日志刷到磁盘上，然后要么是内存不够，要么是io负载太重，导致刷磁盘这个过程非常慢，于是就timeout了。</p>
<p>当然systemd-journald 重启也不一定意味着OOM Killer，只是肯定是内存比较紧张了。</p>
<h2 id="rsyslog和journald的基础知识"><a href="#rsyslog和journald的基础知识" class="headerlink" title="rsyslog和journald的基础知识"></a>rsyslog和journald的基础知识</h2><p><code>systemd-journald</code>是用来协助<code>rsyslog</code>记录系统启动服务和服务启动失败的情况等等. <code>systemd-journald</code>使用内存保存记录, 系统重启记录会丢失. 所有还要用<code>rsyslog</code>来记录分类信息, 如上面<code>/etc/rsyslog.d/listen.conf</code>中的<code>syslog</code>分类.</p>
<p><code>systemd-journald</code>跟随systemd开机就启动，能及时记录所有日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># systemd-analyze critical-chain systemd-journald.service</span><br><span class="line">The time after the unit is active or started is printed after the &quot;@&quot; character.</span><br><span class="line">The time the unit takes to start is printed after the &quot;+&quot; character.</span><br><span class="line"></span><br><span class="line">systemd-journald.service +13ms</span><br><span class="line">└─system.slice</span><br><span class="line">  └─-.slice</span><br></pre></td></tr></table></figure>
<p>systemd-journald 由于是使用于内存的登录文件记录方式，因此重新开机过后，开机前的登录文件信息当然就不会被记载了。 为此，我们还是建议启动 rsyslogd 来协助分类记录！也就是说， systemd-journald 用来管理与查询这次开机后的登录信息，而 rsyslogd 可以用来记录以前及现在的所以数据到磁盘文件中，方便未来进行查询喔！</p>
<p><strong>Tips</strong> 虽然 systemd-journald 所记录的数据其实是在内存中，但是系统还是利用文件的型态将它记录到 /run/log/ 下面！ 不过我们从前面几章也知道， /run 在 CentOS 7 其实是内存内的数据，所以重新开机过后，这个 /run/log 下面的数据当然就被刷新，旧的当然就不再存在了！</p>
<blockquote>
<p>其实鸟哥是这样想的，既然我们还有 rsyslog.service 以及 logrotate 的存在，因此这个 systemd-journald.service 产生的登录文件， 个人建议最好还是放置到 /run/log 的内存当中，以加快存取的速度！而既然 rsyslog.service 可以存放我们的登录文件， 似乎也没有必要再保存一份 journal 登录文件到系统当中就是了。单纯的建议！如何处理，依照您的需求即可喔！</p>
</blockquote>
<p><strong><code>system-journal</code>服务监听 <code>/dev/log</code> socket获取日志, 保存在内存中, 并间歇性的写入<code>/var/log/journal</code>目录中.</strong></p>
<p><code>rsyslog</code>服务启动后监听<code>/run/systemd/journal/socket</code> 获取syslog类型日志, 并写入<code>/var/log/messages</code>文件中. </p>
<p>获取日志时需要记录日志条目的<code>position</code>到<code>/var/lib/rsyslog/imjournal.state</code>文件中.</p>
<p>比如haproxy日志配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line"># log发给journald(journald监听 /dev/log)</span><br><span class="line">        log /dev/log    local1 warning</span><br></pre></td></tr></table></figure>
<p>以下是drds 的iptables日志配置，将tcp reset包记录下来，默认iptable日志输出到/varlog/messages中（dmesg也能看到），然后可以通过rsyslog.d 配置将这部分日志输出到单独的文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 配置iptables 日志，增加 [drds] 标识</span><br><span class="line"># cat /home/admin/drds-worker/install/drds_filter.conf</span><br><span class="line"># Generated by iptables-save v1.4.21 on Wed Apr  1 11:39:31 2020</span><br><span class="line">*filter</span><br><span class="line">:INPUT ACCEPT [557:88127]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [527:171711]</span><br><span class="line">-A INPUT -p tcp -m tcp ! --sport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line"># -A INPUT -p tcp -m tcp ! --dport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line">-A OUTPUT -p tcp -m tcp ! --sport 3406 --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line">COMMIT</span><br><span class="line"># Completed on Wed Apr  1 11:39:31 2020</span><br><span class="line"></span><br><span class="line">#通过rsyslogd将日志写出到指定位置(不配置的话默认输出到 dmesg)</span><br><span class="line"># cat /etc/rsyslog.d/drds_filter_log.conf</span><br><span class="line">:msg, startswith, &quot;[drds]&quot; -/home/admin/logs/tcp-rt/drds-tcp.log</span><br></pre></td></tr></table></figure>
<h3 id="journald-log持久化"><a href="#journald-log持久化" class="headerlink" title="journald log持久化"></a>journald log持久化</h3><p>创建 /var/log/journal 文件夹后默认会持久化，设置持久化后 /run/log 里面就没有日志了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/systemd/journald.conf</span><br><span class="line">#  This file is part of systemd.</span><br><span class="line">#</span><br><span class="line">#  systemd is free software; you can redistribute it and/or modify it</span><br><span class="line">#  under the terms of the GNU Lesser General Public License as published by</span><br><span class="line">#  the Free Software Foundation; either version 2.1 of the License, or</span><br><span class="line">#  (at your option) any later version.</span><br><span class="line">#</span><br><span class="line"># Entries in this file show the compile time defaults.</span><br><span class="line"># You can change settings by editing this file.</span><br><span class="line"># Defaults can be restored by simply deleting this file.</span><br><span class="line">#</span><br><span class="line"># See journald.conf(5) for details.</span><br><span class="line"></span><br><span class="line">[Journal]</span><br><span class="line">#Storage=auto  //默认如果有 /var/log/journal 目录就会持久化到这里</span><br><span class="line">Compress=no</span><br><span class="line">#Seal=yes</span><br><span class="line">#SplitMode=uid</span><br><span class="line">#SyncIntervalSec=5m</span><br><span class="line">#RateLimitInterval=30s</span><br><span class="line">#RateLimitBurst=1000</span><br><span class="line">SystemMaxUse=500M   //最多保留500M日志文件，免得撑爆磁盘</span><br><span class="line">#SystemKeepFree=</span><br><span class="line">#SystemMaxFileSize=</span><br><span class="line">#RuntimeMaxUse=</span><br><span class="line">#RuntimeKeepFree=</span><br><span class="line">#RuntimeMaxFileSize=</span><br><span class="line">#MaxRetentionSec=</span><br><span class="line">#MaxFileSec=1month</span><br><span class="line">#ForwardToSyslog=yes</span><br><span class="line">#ForwardToKMsg=no</span><br><span class="line">#ForwardToConsole=no</span><br><span class="line">#ForwardToWall=yes</span><br><span class="line">#TTYPath=/dev/console</span><br><span class="line">#MaxLevelStore=debug</span><br><span class="line">#MaxLevelSyslog=debug</span><br><span class="line">#MaxLevelKMsg=notice</span><br><span class="line">#MaxLevelConsole=info</span><br><span class="line">#MaxLevelWall=emerg</span><br><span class="line">#LineMax=48K</span><br></pre></td></tr></table></figure>
<p>清理日志保留1M：journalctl –vacuum-size=1M </p>
<p>设置最大保留500M日志： journalctl –vacuum-size=500</p>
<h3 id="rsyslogd"><a href="#rsyslogd" class="headerlink" title="rsyslogd"></a>rsyslogd</h3><p>以下内容来自鸟哥的书：</p>
<p>CentOS 7 除了保有既有的 rsyslog.service 之外，其实最上游还使用了 systemd 自己的登录文件日志管理功能喔！他使用的是 systemd-journald.service 这个服务来支持的。基本上，系统由 systemd 所管理，那所有经由 systemd 启动的服务，如果再启动或结束的过程中发生一些问题或者是正常的讯息， 就会将该讯息由 systemd-journald.service 以二进制的方式记录下来，之后再将这个讯息发送给 rsyslog.service 作进一步的记载。</p>
<p>基本上， rsyslogd 针对各种服务与讯息记录在某些文件的配置文件就是 /etc/rsyslog.conf， 这个文件规定了“（1）什么服务 （2）的什么等级讯息 （3）需要被记录在哪里（设备或文件）” 这三个咚咚，所以设置的语法会是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$cat /etc/rsyslog.conf</span><br><span class="line">服务名称[.=!]讯息等级        讯息记录的文件名或设备或主机</span><br><span class="line"># 下面以 mail 这个服务产生的 info 等级为例：</span><br><span class="line">mail.info            /var/log/maillog_info</span><br><span class="line"># 这一行说明：mail 服务产生的大于等于 info 等级的讯息，都记录到</span><br><span class="line"># /var/log/maillog_info 文件中的意思。</span><br></pre></td></tr></table></figure>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/1cce7612a84cf1a1addceeff6032cb5c.png" alt="syslog 所制订的服务名称与软件调用的方式"></p>
<p> CentOS 7.x 默认的 rsyslogd 本身就已经具有远程日志服务器的功能了， 只是默认并没有启动该功能而已。你可以通过 man rsyslogd 去查询一下相关的选项就能够知道啦！ 既然是远程日志服务器，那么我们的 Linux 主机当然会启动一个端口来监听了，那个默认的端口就是 UDP 或 TCP 的 port 514 </p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/40740cd5cfc8896c07c15b959420646f.png" alt="image.png"></p>
<p>Server配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/rsyslog.conf</span><br><span class="line"># 找到下面这几行：</span><br><span class="line"># Provides UDP syslog reception</span><br><span class="line">#$ModLoad imudp</span><br><span class="line">#$UDPServerRun 514</span><br><span class="line"></span><br><span class="line"># Provides TCP syslog reception</span><br><span class="line">#$ModLoad imtcp</span><br><span class="line">#$InputTCPServerRun 514</span><br><span class="line"># 上面的是 UDP 端口，下面的是 TCP 端口！如果你的网络状态很稳定，就用 UDP 即可。</span><br><span class="line"># 不过，如果你想要让数据比较稳定传输，那么建议使用 TCP 啰！所以修改下面两行即可！</span><br><span class="line">$ModLoad imtcp</span><br><span class="line">$InputTCPServerRun 514</span><br><span class="line"></span><br><span class="line"># 2\. 重新启动与观察 rsyslogd 喔！</span><br><span class="line">[root@study ~]# systemctl restart rsyslog.service</span><br><span class="line">[root@study ~]# netstat -ltnp &amp;#124; grep syslog</span><br><span class="line">Proto Recv-Q Send-Q Local Address  Foreign Address   State    PID/Program name</span><br><span class="line">tcp        0      0 0.0.0.0:514    0.0.0.0:*         LISTEN   2145/rsyslogd</span><br><span class="line">tcp6       0      0 :::514         :::*              LISTEN   2145/rsyslogd</span><br><span class="line"># 嘿嘿！你的登录文件主机已经设置妥当啰！很简单吧！</span><br></pre></td></tr></table></figure>
<p>client配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/rsyslog.conf</span><br><span class="line">*.*       @@192.168.1.100</span><br><span class="line">#*.*       @192.168.1.100  # 若用 UDP 传输，设置要变这样！</span><br></pre></td></tr></table></figure>
<p>常见的几个系统日志有哪些呢？一般而言，有下面几个：</p>
<ul>
<li>/var/log/boot.log： 开机的时候系统核心会去侦测与启动硬件，接下来开始各种核心支持的功能启动等。这些流程都会记录在 /var/log/boot.log 里面哩！ 不过这个文件只会存在这次开机启动的信息，前次开机的信息并不会被保留下来！</li>
<li>/var/log/cron： 还记得<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html" target="_blank" rel="noopener">第十五章例行性工作调度</a>吧？你的 crontab 调度有没有实际被进行？ 进行过程有没有发生错误？你的 /etc/crontab 是否撰写正确？在这个登录文件内查询看看。</li>
<li>/var/log/dmesg： 记录系统在开机的时候核心侦测过程所产生的各项信息。由于 CentOS 默认将开机时核心的硬件侦测过程取消显示， 因此额外将数据记录一份在这个文件中；</li>
<li>/var/log/lastlog： 可以记录系统上面所有的帐号最近一次登陆系统时的相关信息。<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#uselinux_find" target="_blank" rel="noopener">第十三章讲到的 lastlog</a> 指令就是利用这个文件的记录信息来显示的。</li>
<li>/var/log/maillog 或 /var/log/mail/*： 记录邮件的往来信息，其实主要是记录 postfix （SMTP 协定提供者） 与 dovecot （POP3 协定提供者） 所产生的讯息啦。 SMTP 是发信所使用的通讯协定， POP3 则是收信使用的通讯协定。 postfix 与 dovecot 则分别是两套达成通讯协定的软件。</li>
<li>/var/log/messages： 这个文件相当的重要，几乎系统发生的错误讯息 （或者是重要的信息） 都会记录在这个文件中； 如果系统发生莫名的错误时，这个文件是一定要查阅的登录文件之一。</li>
<li>/var/log/secure： 基本上，只要牵涉到“需要输入帐号密码”的软件，那么当登陆时 （不管登陆正确或错误） 都会被记录在此文件中。 包括系统的 login 程序、图形接口登陆所使用的 gdm 程序、 su, sudo 等程序、还有网络连线的 ssh, telnet 等程序， 登陆信息都会被记载在这里；</li>
<li>/var/log/wtmp, /var/log/faillog： 这两个文件可以记录正确登陆系统者的帐号信息 （wtmp） 与错误登陆时所使用的帐号信息 （faillog） ！ 我们在<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#last" target="_blank" rel="noopener">第十章谈到的 last</a> 就是读取 wtmp 来显示的， 这对于追踪一般帐号者的使用行为很有帮助！</li>
<li>/var/log/httpd/<em>, /var/log/samba/</em>： 不同的网络服务会使用它们自己的登录文件来记载它们自己产生的各项讯息！上述的目录内则是个别服务所制订的登录文件。</li>
</ul>
<h2 id="journalctl-常用参数"><a href="#journalctl-常用参数" class="headerlink" title="journalctl 常用参数"></a><a href="https://linuxhint.com/journalctl-tail-and-cheatsheet/" target="_blank" rel="noopener">journalctl 常用参数</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-n or –lines= Show the most recent **n** number of log lines</span><br><span class="line"></span><br><span class="line">-f or –follow Like a tail operation for viewing live updates</span><br><span class="line"></span><br><span class="line">-S, –since=, -U, –until= Search based on a date. “2019-07-04 13:19:17”, “00:00:00”, “yesterday”, “today”, “tomorrow”, “now” are valid formats. For complete time and date specification, see systemd.time(7)</span><br><span class="line"></span><br><span class="line">-u service unit</span><br></pre></td></tr></table></figure>
<p>清理journald日志</p>
<blockquote>
<p> journalctl –vacuum-size=1M &amp;&amp; journalctl –vacuum-size=500</p>
</blockquote>
<h2 id="logrotate"><a href="#logrotate" class="headerlink" title="logrotate"></a>logrotate</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/var/log/cron</span><br><span class="line">&#123;</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">        /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="kill-HUP"><a href="#kill-HUP" class="headerlink" title="kill -HUP"></a><a href="https://unix.stackexchange.com/questions/440004/why-is-kill-hup-used-in-logrotate-in-rhel-is-it-necessary-in-all-cases" target="_blank" rel="noopener">kill -HUP</a></h3><p>Generally services keep the log files opened while they are running. This mean that they do not care if the log files are renamed/moved or deleted they will continue to write to the open file handled.</p>
<p>When logrotate move the files, the services keep writing to the same file.</p>
<p>Example: syslogd will write to /var/log/cron.log. Then logrotate will rename the file to /var/log/cron.log.1, so syslogd will keep writing to the open file /var/log/cron.log.1.</p>
<p>Sending the HUP signal to syslogd will force him to close existing file handle and open new file handle to the original path /var/log/cron.log which will create a new file.</p>
<p>The use of the HUP signal instead of another one is at the discretion of the program. Some services like php-fpm will listen to the USR1 signal to reopen it’s file handle without terminating itself.</p>
<p>不过还得看应用是否屏蔽了 HUP 信号</p>
<h2 id="systemd"><a href="#systemd" class="headerlink" title="systemd"></a>systemd</h2><p>sudo systemctl list-unit-files –type=service | grep enabled //列出启动项</p>
<p> journalctl -b -1 //复审前一次启动， -2 复审倒数第 2 次启动. 重演你的系统启动的所有消息</p>
<p>sudo systemd-analyze blame   <strong>sudo systemd-analyze critical-chain</strong></p>
<p>systemd-analyze critical-chain –fuzz 1h</p>
<p>sudo systemd-analyze blame networkd</p>
<p>systemd-analyze critical-chain network.target local-fs.target</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/neweditor/bb21293e-9b52-40f9-9ab2-7c5aeb7beca1.png" alt="img"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>一模一样的症状，但是根因找错了：<a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="noopener">rsyslog占用内存高</a> </p>
<p><a href="https://access.redhat.com/solutions/3705051" target="_blank" rel="noopener">https://access.redhat.com/solutions/3705051</a></p>
<p><a href="https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html" target="_blank" rel="noopener">https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html</a></p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html" target="_blank" rel="noopener">鸟哥 journald 介绍</a></p>
<p><a href="https://linuxhint.com/journalctl-tail-and-cheatsheet/" target="_blank" rel="noopener">journalctl tail and cheatsheet</a></p>
<p><a href="https://lp007819.wordpress.com/2015/01/17/systemd-journal-介绍/" target="_blank" rel="noopener">Journal的由来</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/15/TCP传输速度案例分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/15/TCP传输速度案例分析/" itemprop="url">TCP传输速度案例分析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-15T17:30:03+08:00">
                2021-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/TCP/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/01/15/TCP传输速度案例分析/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/01/15/TCP传输速度案例分析/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TCP传输速度案例分析"><a href="#TCP传输速度案例分析" class="headerlink" title="TCP传输速度案例分析"></a>TCP传输速度案例分析</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>TCP传输速度受网络带宽和传输窗口的影响（接收、发送、拥塞窗口），带宽我们没办法改变，以下案例主要是讨论rt、窗口如何影响速度。</p>
<p>详细的buffer、rt对TCP传输速度的影响请看这篇：</p>
<p> <a href="https://plantegg.github.io/2019/09/28/就是要你懂TCP--性能和发送接收Buffer的关系/" target="_blank" rel="noopener">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></p>
<p>以及 <a href="https://plantegg.github.io/2018/06/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%9C%80%E7%BB%8F%E5%85%B8%E7%9A%84TCP%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">就是要你懂TCP–最经典的TCP性能问题 Nagle和Delay ack</a></p>
<p>上面两篇以及下面几个案例读完，应该所有TCP传输速度问题都能解决了，Good Luck！</p>
<h2 id="前后端rtt差异大-vip下载慢的案例"><a href="#前后端rtt差异大-vip下载慢的案例" class="headerlink" title="前后端rtt差异大+vip下载慢的案例"></a>前后端rtt差异大+vip下载慢的案例</h2><p>来源：<a href="https://mp.weixin.qq.com/s/er8vTKZUcahA6-Pf8DZBng" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/er8vTKZUcahA6-Pf8DZBng</a> 文章中的trace-cmd工具也不错</p>
<p>如下三个链路，有一个不正常了</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/2422ae219d3b27cfe8c799642662d5b2.png" alt="image.png"></p>
<p>首先通过 ss -it dst “ip:port” 来分析cwnd、ssthresh、buffer，到底是什么导致了传输慢</p>
<h3 id="原因TCPLossProbe："><a href="#原因TCPLossProbe：" class="headerlink" title="原因TCPLossProbe："></a>原因TCPLossProbe：</h3><p>如果尾包发生了丢包，没有新包可发送触发多余的dup ack来实现快速重传，如果完全依赖RTO超时来重传，代价太大，那如何能优化解决这种尾丢包的情况。也就是在某些情况下一个可以的重传包就能触发ssthresh减半，从而导致传输速度上不来。</p>
<p>本案例中，因为client到TGW跨了地域，导致rtt增大，但是TGW和STGW之间的rtt很小，导致握手完毕后STGW认为和client的rtt很小，所以很快就触发了丢包重传，实际没有丢包，只是rtt变大了，所以触发了如上的TLP( PTO=max(2rtt, 10ms) ， 因为只有一次重传并收到了 dup，还是不应该触发TLP，但是因为老版本kernel bug导致，4.0的kernel修复了这个问题， 函数 is_tlp_dupack)</p>
<p>握手完毕后第七号包很快重传了</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/2867daa600363af61f8f971479246858.png" alt="image.png"></p>
<h3 id="观察："><a href="#观察：" class="headerlink" title="观察："></a>观察：</h3><p>netstat -s |grep TCPLossProbes</p>
<h3 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h3><p>tcp_early_retrans可用于开启和关闭ER和TLP，默认是3（enable TLP and delayed ER），sysctl -w net.ipv4.tcp_early_retrans=2 关掉TLP</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>kernel版本小于4.0+TLP开启+VIP代理导致RS认为rtt很小，实际比较大，这两个条件下就会出现如上问题。</p>
<p>这个问题一看就是跟client和VIP代理之间的rtt扩大有关系，不过不是因为扩大后发送窗口不够之类导致的。</p>
<h2 id="长肥网络（高rtt）场景下tcp-metrics记录的ssthresh太小导致传输慢的案例"><a href="#长肥网络（高rtt）场景下tcp-metrics记录的ssthresh太小导致传输慢的案例" class="headerlink" title="长肥网络（高rtt）场景下tcp_metrics记录的ssthresh太小导致传输慢的案例"></a>长肥网络（高rtt）场景下tcp_metrics记录的ssthresh太小导致传输慢的案例</h2><p><a href="https://www.atatech.org/articles/109967" target="_blank" rel="noopener">https://www.atatech.org/articles/109967</a></p>
<blockquote>
<p>tcp_metrics会记录下之前已关闭tcp 连接的状态，包括发送端拥塞窗口和拥塞控制门限，如果之前网络有一段时间比较差或者丢包比较严重，就会导致tcp 的拥塞控制门限ssthresh降低到一个很低的值，这个值在连接结束后会被tcp_metrics cache 住，在新连接建立时，即使网络状况已经恢复，依然会继承 tcp_metrics 中cache 的一个很低的ssthresh 值，在长肥管道情况下，新连接经历短暂的“慢启动”后，随即进入缓慢的拥塞控制阶段, 导致连接速度很难在短时间内上去。而后面的连接，需要很特殊的场景之下才能将ssthresh 再次推到一个比较高的值缓存下来，因此很有很能在接下来的很长一段时间，连接的速度都会处于一个很低的水平</p>
</blockquote>
<p>因为 tcp_metrics记录的ssthresh非常小，导致后面新的tcp连接传输数据时很快进入拥塞控制阶段，如果传输的文件不大的话就没有机会将ssthresh撑大。除非传输一个特别大的文件，忍受拥塞控制阶段的慢慢增长，最后tcp_metrics记录下撑大后的ssthresh，整个网络才会恢复正常。</p>
<p>所以关闭 tcp_metrics其实是个不错的选择： net.ipv4.tcp_no_metrics_save = 1 </p>
<p>或者清除： sudo ip tcp_metrics flush all</p>
<h3 id="从系统cache中查看-tcp-metrics-item"><a href="#从系统cache中查看-tcp-metrics-item" class="headerlink" title="从系统cache中查看 tcp_metrics item"></a>从系统cache中查看 tcp_metrics item</h3><pre><code>$sudo ip tcp_metrics show | grep  100.118.58.7
100.118.58.7 age 1457674.290sec tw_ts 3195267888/5752641sec ago rtt 1000us rttvar 1000us ssthresh 361 cwnd 40 ----这两个值对传输性能很重要

192.168.1.100 age 1051050.859sec ssthresh 4 cwnd 2 rtt 4805us rttvar 4805us source 192.168.0.174 ---这条记录有问题，缓存的ssthresh 4 cwnd 2都太小，传输速度一定慢 

清除 tcp_metrics, sudo ip tcp_metrics flush all 
关闭 tcp_metrics 功能，net.ipv4.tcp_no_metrics_save = 1
sudo ip tcp_metrics delete 100.118.58.7
</code></pre><p>每个连接的ssthresh默认是个无穷大的值，但是内核会cache对端ip上次的ssthresh（大部分时候两个ip之间的拥塞窗口大小不会变），这样大概率到达ssthresh之后就基本拥塞了，然后进入cwnd的慢增长阶段。</p>
<h2 id="长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响"><a href="#长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响" class="headerlink" title="长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响"></a>长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响</h2><p>最后通过一个实际碰到的案例，涉及到了接收窗口、发送Buffer以及高延时情况下的性能问题</p>
<p>案例描述：从中国访问美国的服务器下载图片，只能跑到220K，远远没有达到带宽能力，其中中美之间的网络延时时150ms，这个150ms已经不能再优化了。业务结构是：</p>
<p>client ——150ms—–&gt;&gt;&gt;LVS—1ms–&gt;&gt;&gt;美国的统一接入server—–1ms—–&gt;&gt;&gt;nginx</p>
<p>通过下载一个4M的文件大概需要20秒，分别在client和nginx上抓包来分析这个问题（统一接入server没权限上去）</p>
<h3 id="Nginx上抓包"><a href="#Nginx上抓包" class="headerlink" title="Nginx上抓包"></a>Nginx上抓包</h3><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/259767fb17f7dbffe7f77ab059c47dbd.png" alt="image.png"></p>
<p>从这里可以看到Nginx大概在60ms内就将4M的数据都发完了</p>
<h3 id="client上抓包"><a href="#client上抓包" class="headerlink" title="client上抓包"></a>client上抓包</h3><p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/466fba92829f6a922ccd2d57a7e3fdac.png" alt="image.png"></p>
<p>从这个图上可以清楚看到大概每传输大概30K数据就有一个150ms的等待平台，这个150ms基本是client到美国的rt。</p>
<p>从我们前面的阐述可以清楚了解到因为rt比较高，统一接入server每发送30K数据后要等150ms才能收到client的ack，然后继续发送，猜是因为上面设置的发送buffer大概是30K。</p>
<p>检查统一接入server的配置，可以看到接入server的配置里面果然有个32K buffer设置</p>
<h3 id="将buffer改大"><a href="#将buffer改大" class="headerlink" title="将buffer改大"></a>将buffer改大</h3><p>速度可以到420K，但是还没有跑满带宽：</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/93e254c5154ce2e065bec9fb34f3db2b.png" alt="image.png"></p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/0a8c68a58da6f169573b57cde0ffba93.png" alt="image.png"></p>
<p>接着看一下client上的抓包</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/822737a4ed6ffe6b920d4b225a1be5bf.png" alt="image.png"></p>
<p>可以清楚看到 client的接收窗口是64K， 64K*1000/150=426K 这个64K很明显是16位的最大值，应该是TCP握手有一方不支持window scaling factor</p>
<p>那么继续分析一下握手包，syn：</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/004886698ddbaa1cbc8342a9cd667c76.png" alt="image.png"></p>
<p>说明client是支持的，再看 syn+ack：</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/70155e021390cb1ee07091c306c375f4.png" alt="image.png"></p>
<p>可以看到服务端不支持，那就最大只能用到64K。需要修改服务端代理程序，这主要是LVS或者代理的锅。</p>
<p>如果内网之间rt很小这个锅不会爆发，一旦网络慢一点就把问题恶化了</p>
<p>比如这是这个应用的开发人员的反馈：</p>
<p><img src="https://ata2-img.cn-hangzhou.oss-pub.aliyun-inc.com/a08a204ec7ad4bba7867dacea1668322.png" alt="image.png"></p>
<p>长肥网络就像是很长很宽的高速公路，上面可以同时跑很多车，而如果发车能力不够，就容易跑不满高速公路。<br>在rt很短的时候可以理解为高速公路很短，所以即使发车慢也还好，因为车很快就到了，到了后就又能发新车了。rt很长的话就要求更大的仓库了。</p>
<p>整个这个问题，我最初拿到的问题描述结构是这样的（不要笑用户连自己的业务结构都描述不清）：</p>
<p>client ——150ms—–&gt;&gt;&gt;nginx</p>
<p>实际开发人员也不能完全描述清楚结构，从抓包中慢慢分析反推他们的结构，到最后问题的解决。</p>
<p>这个案例综合了发送窗口（32K）、接收窗口（64K，因为握手LVS不支持window scale）、rt很大将问题暴露出来（跨国网络，rt没法优化）。</p>
<p>nginx buffer 分析参考案例：<a href="https://club.perfma.com/article/433792?from=timeline" target="_blank" rel="noopener">https://club.perfma.com/article/433792?from=timeline</a></p>
<h2 id="delay-ack拉高实际rt的案例"><a href="#delay-ack拉高实际rt的案例" class="headerlink" title="delay ack拉高实际rt的案例"></a>delay ack拉高实际rt的案例</h2><p><strong>这个案例跟速度没有关系，只是解析监控图表上的rt为什么不符合逻辑地偏高了。</strong></p>
<p>如下业务监控图：实际处理时间（逻辑服务时间1ms，rtt2.4ms，加起来3.5ms），但是系统监控到的rt（蓝线）是6ms，如果一个请求分很多响应包串行发给client，这个6ms是正常的（1+2.4*N），但实际上如果send buffer足够的话，按我们前面的理解多个响应包会并发发出去，所以如果整个rt是3.5ms才是正常的。</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d56f87a19a10b0ac9a3b7009641247a0.png" alt="image.png"></p>
<p>抓包来分析原因：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/d5e2e358dd1a24e104f54815c84875c9.png" alt="image.png"></p>
<p>实际看到大量的response都是3.5ms左右，符合我们的预期，但是有少量rt被delay ack严重影响了</p>
<p>从下图也可以看到有很多rtt超过3ms的，这些超长时间的rtt会最终影响到整个服务rt</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/48eae3dcd7c78a68b0afd5c66f783f23.png" alt="image.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/25/一个有意思的问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/25/一个有意思的问题/" itemprop="url">一个有意思的问题</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-25T17:30:03+08:00">
                2020-12-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/12/25/一个有意思的问题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/12/25/一个有意思的问题/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一个有意思的问题"><a href="#一个有意思的问题" class="headerlink" title="一个有意思的问题"></a>一个有意思的问题</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$mysql -N -h127.0.0.1 -e &quot;select id from sbtest1 limit 1&quot;</span><br><span class="line">+--------+</span><br><span class="line">| 100024 |</span><br><span class="line">+--------+</span><br><span class="line"></span><br><span class="line">$mysql -N -h127.0.0.1 -e &quot;select id from sbtest1 limit 1&quot; | cat</span><br><span class="line">100024</span><br><span class="line"></span><br><span class="line">$mysql -t -N -h127.0.0.1 -e &quot;select id from sbtest1 limit 1&quot; | cat</span><br><span class="line">+--------+</span><br><span class="line">| 100024 |</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure>
<p>如上第一和第二个语句，<strong>为什么mysql client的输出重定向后就没有ascii制表符了呢</strong>？ 语句三加上 -t后再经过管道，也有制表符了。</p>
<p><a href="https://stackoverflow.com/questions/15640287/change-output-format-for-mysql-command-line-results-to-csv/17910254" target="_blank" rel="noopener">这里也有很多人有同样的疑问</a>，不过不但没有给出第三行的解法，更没有人讲清楚这个里面的原理</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>strace看看第一个语句：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/086f6cd952d2b91eae7eda6d576765f8.png" alt="image.png"></p>
<p>再对比下第二个语句的strace：</p>
<p><img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/984bcce23ff8766b52fdede8ff3eadec.png" alt="image.png"></p>
<p>从上面两个strace比较来看，似乎mysql client能检测到要输出到命名管道（S_IFIFO ）还是character device（S_IFCHR），如果是命名管道的话就不要输出制表符了，如果是character device那么就输出ascii制表符。</p>
<p><a href="https://linux.die.net/man/2/fstat64" target="_blank" rel="noopener">fstats里面对不同输出目标的说明</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;File type:                &quot;);</span><br><span class="line">   switch (sb.st_mode &amp; S_IFMT) &#123;</span><br><span class="line">    case S_IFBLK:  printf(&quot;block device\n&quot;);            break;</span><br><span class="line">    case S_IFCHR:  printf(&quot;character device\n&quot;);        break;</span><br><span class="line">    case S_IFDIR:  printf(&quot;directory\n&quot;);               break;</span><br><span class="line">    case S_IFIFO:  printf(&quot;FIFO/pipe\n&quot;);               break;</span><br><span class="line">    case S_IFLNK:  printf(&quot;symlink\n&quot;);                 break;</span><br><span class="line">    case S_IFREG:  printf(&quot;regular file\n&quot;);            break;</span><br><span class="line">    case S_IFSOCK: printf(&quot;socket\n&quot;);                  break;</span><br><span class="line">    default:       printf(&quot;unknown?\n&quot;);                break;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>第4行和第6行两个类型就是导致mysql client选择了不同的输出内容</p>
<h2 id="误解"><a href="#误解" class="headerlink" title="误解"></a>误解</h2><p>所以这个问题不是： </p>
<blockquote>
<p>​       <strong>为什么mysql client的输出重定向后就没有ascii制表符了呢</strong>？</p>
</blockquote>
<p>而是：</p>
<blockquote>
<p>​        <strong>mysql client 可以检测到不同的输出目标然后输出不同的内容吗？</strong> 管道或者重定向是一个应用能感知的输出目标吗？</p>
</blockquote>
<p>误解：觉得管道写在后面，mysql client不应该知道后面是管道，mysql client输出内容到stdout，然后os将stdout的内容重定向给管道。</p>
<p>实际上mysql是可以检测（detect）输出目标的，如果是管道类的非交互输出那么没必要徒增一些制表符；如果是交互式界面那么就输出一些制表符好看一些。</p>
<p>要是想想在Unix下一切皆文件就更好理解了，输出到管道这个管道也是个文件，所以mysql client是可以感知各种输出文件的属性的。</p>
<p>背后的<a href="https://stackoverflow.com/questions/1312922/detect-if-stdin-is-a-terminal-or-pipe" target="_blank" rel="noopener">实现</a>大概是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;io.h&gt;</span><br><span class="line">...    </span><br><span class="line">if (isatty(fileno(stdout)))</span><br><span class="line">    printf( &quot;stdout is a terminal\n&quot; );      // 输出制表符</span><br><span class="line">else</span><br><span class="line">    printf( &quot;stdout is a file or a pipe\n&quot;); // 不输出制表符</span><br></pre></td></tr></table></figure>
<p><a href="https://linux.die.net/man/3/isatty" target="_blank" rel="noopener">isatty的解释</a></p>
<p>结论就是 mysql client根据输出目标的不同（stdout、重定向）输出不同的内容，不过这种做法对用户体感上不是太好。</p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>Linux管道居然不是按顺序，而是并发执行的：<a href="https://unix.stackexchange.com/questions/37508/in-what-order-do-piped-commands-run" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/37508/in-what-order-do-piped-commands-run</a>  掉坑里了，并发问题就多了，实际测试也发现跑几千次 ps |grep 会出现，ps看不到后面的grep进程</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.pyrosoft.co.uk/blog/2014/09/08/how-to-stop-mysql-ascii-tables-column-separators-from-being-lost-when-redirecting-bash-output/" target="_blank" rel="noopener">https://www.pyrosoft.co.uk/blog/2014/09/08/how-to-stop-mysql-ascii-tables-column-separators-from-being-lost-when-redirecting-bash-output/</a></p>
<p><a href="https://www.oreilly.com/library/view/mysql-cookbook/0596001452/ch01s22.html" target="_blank" rel="noopener">https://www.oreilly.com/library/view/mysql-cookbook/0596001452/ch01s22.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/30/一台机器上最多能创建多少个TCP连接/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/30/一台机器上最多能创建多少个TCP连接/" itemprop="url">到底一台服务器上最多能创建多少个TCP连接</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-30T10:30:03+08:00">
                2020-11-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/11/30/一台机器上最多能创建多少个TCP连接/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/11/30/一台机器上最多能创建多少个TCP连接/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="到底一台服务器上最多能创建多少个TCP连接"><a href="#到底一台服务器上最多能创建多少个TCP连接" class="headerlink" title="到底一台服务器上最多能创建多少个TCP连接"></a>到底一台服务器上最多能创建多少个TCP连接</h1><blockquote>
<p>经常听到有同学说一台机器最多能创建65535个TCP连接，这其实是错误的理解，为什么会有这个错误的理解呢？</p>
</blockquote>
<h2 id="port-range"><a href="#port-range" class="headerlink" title="port range"></a>port range</h2><p>我们都知道linux下本地随机端口范围由参数控制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /proc/sys/net/ipv4/ip_local_port_range </span><br><span class="line">10000	65535</span><br></pre></td></tr></table></figure>
<p>port range的上限是65535，所以也经常看到这个<strong>误解</strong>：一台机器上最多能创建65535个TCP连接</p>
<h2 id="到底一台机器上最多能创建多少个TCP连接"><a href="#到底一台机器上最多能创建多少个TCP连接" class="headerlink" title="到底一台机器上最多能创建多少个TCP连接"></a>到底一台机器上最多能创建多少个TCP连接</h2><p>先说<strong>结论</strong>：在内存、文件句柄足够的话可以创建的连接是没有限制的（每个TCP连接至少要消耗一个文件句柄）。</p>
<p>那么/proc/sys/net/ipv4/ip_local_port_range指定的端口范围到底是什么意思呢？</p>
<p>核心规则：<strong>一个TCP连接只要保证四元组(src-ip src-port dest-ip dest-port)唯一就可以了，而不是要求src port唯一</strong></p>
<p>后面所讲都遵循这个规则，所以在心里反复默念：<strong>四元组唯一</strong> 五个大字，就能分析出来到底能创建多少TCP连接了。</p>
<p>比如如下这个机器上的TCP连接实际状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 18089</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:22         ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.0.79:18089      192.168.0.79:22         TIME_WAIT </span><br><span class="line">tcp        0      0 192.168.1.79:22         192.168.1.79:18089      ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.1.79:18080      192.168.1.79:18089      ESTABLISHED</span><br></pre></td></tr></table></figure>
<p>从前三行可以清楚地看到18089被用了三次，第一第二行src-ip、dest-ip也是重复的，但是dest port不一样，第三行的src-port还是18089，但是src-ip变了。他们的四元组均不相同。</p>
<p>所以一台机器能创建的TCP连接是没有限制的，而ip_local_port_range是指没有bind的时候OS随机分配端口的范围，但是分配到的端口要同时满足五元组唯一，这样 ip_local_port_range 限制的是连同一个目标（dest-ip和dest-port一样）的port的数量（请忽略本地多网卡的情况，因为dest-ip为以后route只会选用一个本地ip）。</p>
<p><strong>那么为什么大家有这样的误解呢？</strong></p>
<ul>
<li>如果是listen服务，那么肯定端口不能重复使用，这样就跟我们的误解对应上了，一个服务器上最多能监听65535个端口。比如nginx监听了80端口，那么tomcat就没法再监听80端口了，这里的80端口只能监听一次。</li>
<li>另外如果我们要连的server只有一个，比如：1.1.1.1:80 ，同时本机只有一个ip的话，那么这个时候即使直接调connect 也只能创建出65535个连接，因为四元组中的三个是固定的了。</li>
</ul>
<p>这也就是65535错误理解的两个主要原因吧。</p>
<p>我们在创建连接前，经常会先调bind，bind后可以调listen当做服务端监听，也可以直接调connect当做client来连服务端。</p>
<p>bind(ip,port=0) 的时候是让系统绑定到某个网卡和自动分配的端口，此时系统没有办法确定接下来这个socket是要去connect还是listen. 如果是listen的话，那么肯定是不能出现端口冲突的，如果是connect的话，只要满足4元组唯一即可。在这种情况下，系统只能尽可能满足更强的要求，就是先要求端口不能冲突，即使之后去connect的时候四元组是唯一的。</p>
<p>但如果我只是个client端，只需要连接server建立连接，也就不需要bind，直接调connect就可以了，这个时候只要保证四元组唯一就行。</p>
<p>bind()的时候内核是还不知道四元组的，只知道src_ip、src_port，所以这个时候单网卡下src_port是没法重复的，但是connect()的时候已经知道了四元组的全部信息，所以只要保证四元组唯一就可以了，那么这里的src_port完全是可以重复使用的。</p>
<p><strong>是不是加上了 SO_REUSEADDR、SO_REUSEPORT 就能重用端口了呢？</strong></p>
<h2 id="TCP-SO-REUSEADDR"><a href="#TCP-SO-REUSEADDR" class="headerlink" title="TCP SO_REUSEADDR"></a>TCP SO_REUSEADDR</h2><p>SO_REUSEADDR 主要解决的是重用TIME_WAIT状态的port, 在程序崩溃后之前的TCP连接会进入到TIME_WAIT状态，需要一段时间才能释放，如果立即重启就会抛出Address Already in use的错误导致启动失败。可以通过在调用bind函数之前设置SO_REUSEADDR来解决。</p>
<blockquote>
<p>What exactly does SO_REUSEADDR do?</p>
<p>This socket option tells the kernel that even if this port is busy (in the TIME_WAIT state), go ahead and reuse it anyway. If it is busy, but with another state, you will still get an address already in use error. It is useful if your server has been shut down, and then restarted right away while sockets are still active on its port. You should be aware that if any unexpected data comes in, it may confuse your server, but while this is possible, it is not likely.</p>
<p>It has been pointed out that “A socket is a 5 tuple (proto, local addr, local port, remote addr, remote port). SO_REUSEADDR just says that you can reuse local addresses. The 5 tuple still must be unique!” This is true, and this is why it is very unlikely that unexpected data will ever be seen by your server. The danger is that such a 5 tuple is still floating around on the net, and while it is bouncing around, a new connection from the same client, on the same system, happens to get the same remote port. </p>
</blockquote>
<p>By setting <code>SO_REUSEADDR</code> user informs the kernel of an intention to share the bound port with anyone else, but only if it doesn’t cause a conflict on the protocol layer. There are at least three situations when this flag is useful:</p>
<ol>
<li>Normally after binding to a port and stopping a server it’s neccesary to wait for a socket to time out before another server can bind to the same port. With <code>SO_REUSEADDR</code> set it’s possible to rebind immediately, even if the socket is in a <code>TIME_WAIT</code> state.</li>
<li>When one server binds to <code>INADDR_ANY</code>, say <code>0.0.0.0:1234</code>, it’s impossible to have another server binding to a specific address like <code>192.168.1.21:1234</code>. With <code>SO_REUSEADDR</code> flag this behaviour is allowed.</li>
<li>When using the bind before connect trick only a single connection can use a single outgoing source port. With this flag, it’s possible for many connections to reuse the same source port, given that they connect to different destination addresses.</li>
</ol>
<h2 id="TCP-SO-REUSEPORT"><a href="#TCP-SO-REUSEPORT" class="headerlink" title="TCP SO_REUSEPORT"></a>TCP SO_REUSEPORT</h2><p>SO_REUSEPORT主要用来解决惊群、性能等问题。</p>
<blockquote>
<p>SO_REUSEPORT is also useful for eliminating the try-10-times-to-bind hack in ftpd’s data connection setup routine.  Without SO_REUSEPORT, only one ftpd thread can bind to TCP (lhost, lport, INADDR_ANY, 0) in preparation for connecting back to the client.  Under conditions of heavy load, there are more threads colliding here than the try-10-times hack can accomodate.  With SO_REUSEPORT, things  work nicely and the hack becomes unnecessary.</p>
</blockquote>
<p>SO_REUSEPORT使用场景：linux kernel 3.9 引入了最新的SO_REUSEPORT选项，使得多进程或者多线程创建多个绑定同一个ip:port的监听socket，提高服务器的接收链接的并发能力,程序的扩展性更好；此时需要设置SO_REUSEPORT（<strong>注意所有进程都要设置才生效</strong>）。</p>
<p>setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&amp;reuse , sizeof(int));</p>
<p>目的：每一个进程有一个独立的监听socket，并且bind相同的ip:port，独立的listen()和accept()；提高接收连接的能力。（例如nginx多进程同时监听同一个ip:port）</p>
<blockquote>
<p>(a) on Linux SO_REUSEPORT is meant to be used <em>purely</em> for load balancing multiple incoming UDP packets or incoming TCP connection requests across multiple sockets belonging to the same app.  ie. it’s a work around for machines with a lot of cpus, handling heavy load, where a single listening socket becomes a bottleneck because of cross-thread contention on the in-kernel socket lock (and state).</p>
<p>(b) set IP_BIND_ADDRESS_NO_PORT socket option for tcp sockets before binding to a specific source ip<br>with port 0 if you’re going to use the socket for connect() rather then listen() this allows the kernel<br>to delay allocating the source port until connect() time at which point it is much cheaper</p>
</blockquote>
<h2 id="The-Ephemeral-Port-Range"><a href="#The-Ephemeral-Port-Range" class="headerlink" title="The Ephemeral Port Range"></a><a href="http://www.ncftp.com/ncftpd/doc/misc/ephemeral_ports.html" target="_blank" rel="noopener">The Ephemeral Port Range</a></h2><p>Ephemeral Port Range就是我们前面所说的Port Range（/proc/sys/net/ipv4/ip_local_port_range）</p>
<blockquote>
<p>A TCP/IPv4 connection consists of two endpoints, and each endpoint consists of an IP address and a port number. Therefore, when a client user connects to a server computer, an established connection can be thought of as the 4-tuple of (server IP, server port, client IP, client port).</p>
<p>Usually three of the four are readily known – client machine uses its own IP address and when connecting to a remote service, the server machine’s IP address and service port number are required.</p>
<p>What is not immediately evident is that when a connection is established that the client side of the connection uses a port number. Unless a client program explicitly requests a specific port number, the port number used is an ephemeral port number.</p>
<p>Ephemeral ports are temporary ports assigned by a machine’s IP stack, and are assigned from a designated range of ports for this purpose. When the connection terminates, the ephemeral port is available for reuse, although most IP stacks won’t reuse that port number until the entire pool of ephemeral ports have been used.</p>
<p>So, if the client program reconnects, it will be assigned a different ephemeral port number for its side of the new connection.</p>
</blockquote>
<h2 id="linux-如何选择Ephemeral-Port"><a href="#linux-如何选择Ephemeral-Port" class="headerlink" title="linux 如何选择Ephemeral Port"></a>linux 如何选择Ephemeral Port</h2><p>有资料说是随机从Port Range选择port，有的说是顺序选择，那么实际验证一下。</p>
<p>如下测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;      // printf</span><br><span class="line">#include &lt;stdlib.h&gt;     // atoi</span><br><span class="line">#include &lt;unistd.h&gt;     // close</span><br><span class="line">#include &lt;arpa/inet.h&gt;  // ntohs</span><br><span class="line">#include &lt;sys/socket.h&gt; // connect, socket</span><br><span class="line"></span><br><span class="line">void sample() &#123;</span><br><span class="line">    // Create socket</span><br><span class="line">    int sockfd;</span><br><span class="line">    if (sockfd = socket(AF_INET, SOCK_STREAM, 0), -1 == sockfd) &#123;</span><br><span class="line">        perror(&quot;socket&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Connect to remote. This does NOT actually send a packet.</span><br><span class="line">    const struct sockaddr_in raddr = &#123;</span><br><span class="line">        .sin_family = AF_INET,</span><br><span class="line">        .sin_port   = htons(8080),     // arbitrary remote port</span><br><span class="line">        .sin_addr   = htonl(INADDR_ANY)  // arbitrary remote host</span><br><span class="line">    &#125;;</span><br><span class="line">    if (-1 == connect(sockfd, (const struct sockaddr *)&amp;raddr, sizeof(raddr))) &#123;</span><br><span class="line">        perror(&quot;connect&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Display selected ephemeral port</span><br><span class="line">    const struct sockaddr_in laddr;</span><br><span class="line">    socklen_t laddr_len = sizeof(laddr);</span><br><span class="line">    if (-1 == getsockname(sockfd, (struct sockaddr *)&amp;laddr, &amp;laddr_len)) &#123;</span><br><span class="line">        perror(&quot;getsockname&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;local port: %i\n&quot;, ntohs(laddr.sin_port));</span><br><span class="line"></span><br><span class="line">    // Close socket</span><br><span class="line">    close(sockfd);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">        sample();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-10-0-327-ali2017-alios7-x86-64"><a href="#3-10-0-327-ali2017-alios7-x86-64" class="headerlink" title="3.10.0-327.ali2017.alios7.x86_64"></a>3.10.0-327.ali2017.alios7.x86_64</h3><p>编译后，执行(3.10.0-327.ali2017.alios7.x86_64)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 ; echo &quot;-------&quot; &amp;&amp; ./client &amp;&amp; sleep 10; date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 &amp;&amp; echo &quot;******&quot;; ./client;</span><br><span class="line">Fri Nov 27 10:52:52 CST 2020</span><br><span class="line">local port: 17448</span><br><span class="line">local port: 17449</span><br><span class="line">local port: 17451</span><br><span class="line">local port: 17452</span><br><span class="line">local port: 17453</span><br><span class="line">+++++++</span><br><span class="line">local port: 17455</span><br><span class="line">local port: 17456</span><br><span class="line">local port: 17457</span><br><span class="line">local port: 17458</span><br><span class="line">local port: 17460</span><br><span class="line">-------</span><br><span class="line">local port: 17475</span><br><span class="line">local port: 17476</span><br><span class="line">local port: 17477</span><br><span class="line">local port: 17478</span><br><span class="line">local port: 17479</span><br><span class="line">Fri Nov 27 10:53:02 CST 2020</span><br><span class="line">local port: 17997</span><br><span class="line">local port: 17998</span><br><span class="line">local port: 17999</span><br><span class="line">local port: 18000</span><br><span class="line">local port: 18001</span><br><span class="line">+++++++</span><br><span class="line">local port: 18002</span><br><span class="line">local port: 18003</span><br><span class="line">local port: 18004</span><br><span class="line">local port: 18005</span><br><span class="line">local port: 18006</span><br><span class="line">******</span><br><span class="line">local port: 18010</span><br><span class="line">local port: 18011</span><br><span class="line">local port: 18012</span><br><span class="line">local port: 18013</span><br><span class="line">local port: 18014</span><br></pre></td></tr></table></figure>
<p>从测试看起来linux下端口选择跟时间有关系，起始端口肯定是顺序增加，起始端口应该是在Ephemeral Port范围内并且和时间戳绑定的某个值（也是递增的），即使没有使用任何端口，起始端口也会随时间增加而增加。</p>
<h3 id="4-19-91-19-1-al7-x86-64"><a href="#4-19-91-19-1-al7-x86-64" class="headerlink" title="4.19.91-19.1.al7.x86_64"></a>4.19.91-19.1.al7.x86_64</h3><p>换个内核版本编译后，执行(4.19.91-19.1.al7.x86_64)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 ; echo &quot;-------&quot; &amp;&amp; ./client &amp;&amp; sleep 10; date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 &amp;&amp; echo &quot;******&quot;; ./client;</span><br><span class="line">Fri Nov 27 14:10:47 CST 2020</span><br><span class="line">local port: 7890</span><br><span class="line">local port: 7892</span><br><span class="line">local port: 7894</span><br><span class="line">local port: 7896</span><br><span class="line">local port: 7898</span><br><span class="line">+++++++</span><br><span class="line">local port: 7900</span><br><span class="line">local port: 7902</span><br><span class="line">local port: 7904</span><br><span class="line">local port: 7906</span><br><span class="line">local port: 7908</span><br><span class="line">-------</span><br><span class="line">local port: 7910</span><br><span class="line">local port: 7912</span><br><span class="line">local port: 7914</span><br><span class="line">local port: 7916</span><br><span class="line">local port: 7918</span><br><span class="line">Fri Nov 27 14:10:57 CST 2020</span><br><span class="line">local port: 7966</span><br><span class="line">local port: 7968</span><br><span class="line">local port: 7970</span><br><span class="line">local port: 7972</span><br><span class="line">local port: 7974</span><br><span class="line">+++++++</span><br><span class="line">local port: 7976</span><br><span class="line">local port: 7978</span><br><span class="line">local port: 7980</span><br><span class="line">local port: 7982</span><br><span class="line">local port: 7984</span><br><span class="line">******</span><br><span class="line">local port: 7988</span><br><span class="line">local port: 7990</span><br><span class="line">local port: 7992</span><br><span class="line">local port: 7994</span><br><span class="line">local port: 7996</span><br></pre></td></tr></table></figure>
<p>之所以都是偶数端口，是因为port_range 从偶数开始：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</span><br><span class="line">1024    65535</span><br></pre></td></tr></table></figure>
<p>将1024改成1025后，分配出来的都是奇数端口了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</span><br><span class="line">1025    1034</span><br><span class="line"></span><br><span class="line">$./client</span><br><span class="line">local port: 1033</span><br><span class="line">local port: 1025</span><br><span class="line">local port: 1027</span><br><span class="line">local port: 1029</span><br><span class="line">local port: 1031</span><br><span class="line">local port: 1033</span><br><span class="line">local port: 1025</span><br><span class="line">local port: 1027</span><br><span class="line">local port: 1029</span><br><span class="line">local port: 1031</span><br><span class="line">local port: 1033</span><br><span class="line">local port: 1025</span><br><span class="line">local port: 1027</span><br><span class="line">local port: 1029</span><br><span class="line">local port: 1031</span><br></pre></td></tr></table></figure>
<p>可见4.19内核下每次port是+2，在3.10内核版本中是+1. 并且都是递增的，同时即使port不使用，也会随着时间的变化这个起始port增大。</p>
<p>Port Range有点像雷达转盘数字，时间就像是雷达上的扫描指针，这个指针不停地旋转，如果这个时候刚好有应用要申请Port，那么就从指针正好指向的Port开始向后搜索可用port</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>在内存、文件句柄足够的话一台服务器上可以创建的TCP连接数量是没有限制的</li>
<li>SO_REUSEADDR 主要用于快速重用 TIME_WAIT状态的TCP端口，避免服务重启就会抛出Address Already in use的错误</li>
<li>SO_REUSEPORT主要用来解决惊群、性能等问题</li>
<li>local port的选择是递增搜索的，搜索起始port随时间增加也变大</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://segmentfault.com/a/1190000002396411" target="_blank" rel="noopener">https://segmentfault.com/a/1190000002396411</a></p>
<p><a href="https://blog.csdn.net/a364572/article/details/40628171" target="_blank" rel="noopener">linux中TCP的socket、bind、listen、connect和accept的实现</a></p>
<p><a href="https://ops.tips/blog/how-linux-tcp-introspection/" target="_blank" rel="noopener">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
<p><a href="https://idea.popcount.org/2014-04-03-bind-before-connect/" target="_blank" rel="noopener">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="weibo @plantegg">
          <p class="site-author-name" itemprop="name">weibo @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">117</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">213</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">weibo @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    

  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
