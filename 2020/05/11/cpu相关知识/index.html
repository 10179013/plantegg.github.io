<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Linux,CPU,perf,IPC,arm,x86,">





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1">






<meta name="description" content="cpu相关知识指令集美国的Sun（1982年成立，早期使用摩托罗拉公司芯片，现已被Oracle收购）、日本Fujitsu（富士通）等公司的小型机是基于SPARC处理器架构（该处理器由1985年Sun公司研制，现在Oracle已放弃了SPARCE转用Intel Xeon）,而美国HP公司的则是基于PA-RISC架构，后基于Itanium ，而最新的SuperdomeX也基于Intel Xeon；Co">
<meta name="keywords" content="Linux,CPU,perf,IPC,arm,x86">
<meta property="og:type" content="article">
<meta property="og:title" content="cpu相关知识">
<meta property="og:url" content="http://yoursite.com/2020/05/11/cpu相关知识/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="cpu相关知识指令集美国的Sun（1982年成立，早期使用摩托罗拉公司芯片，现已被Oracle收购）、日本Fujitsu（富士通）等公司的小型机是基于SPARC处理器架构（该处理器由1985年Sun公司研制，现在Oracle已放弃了SPARCE转用Intel Xeon）,而美国HP公司的则是基于PA-RISC架构，后基于Itanium ，而最新的SuperdomeX也基于Intel Xeon；Co">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/35161ebe1d0b571845c1904abaafa604.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/3e9074d16ebaf8d21d7d5cc64ea9e955.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/ac0bac75ae745316e0c011ffdc5a78a5.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/750px-cascade_lake_naming_scheme.svg.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/950px-skylake_server_block_diagram.svg.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/cache-ht-hierarchy-2.jpg">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/numa-fsb-3.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/numa-imc-iio-smb-4.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/ka02R000000oNBPQA2_en_US_1.jpeg">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/e475286bef0d734feca8fba300a501e6.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/v2-22ac73d34a31de0c98d773199448be24_1440w.jpg">
<meta property="og:image" content="http://yoursite.com/images/oss/16b271c8-5132-4273-a26a-4b35e8f92882.png">
<meta property="og:image" content="http://yoursite.com/images/oss/e177902c-73b2-4535-9c1f-2726451820db.png">
<meta property="og:image" content="http://yoursite.com/images/oss/5a19ff61-68db-4c65-be4c-6b6c155a8a29.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210422095217195.png">
<meta property="og:image" content="http://yoursite.com/images/oss/4d4fdebb-6146-407e-881d-19170fbfd82b.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210425092158127.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210425091727122.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210425091557750.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210425093630438.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427093424116.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427164953340.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427093625575.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427095130343.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427172612685.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427173047815.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210427173417673.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210425180535225.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210425180518926.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210426083033989.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/image-20210426085534983.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/latency.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/app0610-node0.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/app0610-pagescans.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/usr-sys-elapsed-times-app0610.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/perf1.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/perf2.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/db243cb4b3fb9d40902e193283aca1aa.png">
<meta property="og:image" content="http://yoursite.com/images/951413iMgBlog/802b8f4607f1addf17ad24747fda7fb6.png">
<meta property="og:updated_time" content="2021-05-11T10:22:03.626Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cpu相关知识">
<meta name="twitter:description" content="cpu相关知识指令集美国的Sun（1982年成立，早期使用摩托罗拉公司芯片，现已被Oracle收购）、日本Fujitsu（富士通）等公司的小型机是基于SPARC处理器架构（该处理器由1985年Sun公司研制，现在Oracle已放弃了SPARCE转用Intel Xeon）,而美国HP公司的则是基于PA-RISC架构，后基于Itanium ，而最新的SuperdomeX也基于Intel Xeon；Co">
<meta name="twitter:image" content="http://yoursite.com/images/951413iMgBlog/35161ebe1d0b571845c1904abaafa604.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/05/11/cpu相关知识/">





  <title>cpu相关知识 | plantegg</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/11/cpu相关知识/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">cpu相关知识</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-05-11T17:30:03+08:00">
                2020-05-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/05/11/cpu相关知识/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/05/11/cpu相关知识/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="cpu相关知识"><a href="#cpu相关知识" class="headerlink" title="cpu相关知识"></a>cpu相关知识</h1><h2 id="指令集"><a href="#指令集" class="headerlink" title="指令集"></a>指令集</h2><p>美国的Sun（1982年成立，早期使用摩托罗拉公司芯片，现已被Oracle收购）、日本Fujitsu（富士通）等公司的小型机是基于SPARC处理器架构（该处理器由1985年Sun公司研制，现在Oracle已放弃了SPARCE转用Intel Xeon）,而美国HP公司的则是基于PA-RISC架构，后基于Itanium ，而最新的SuperdomeX也基于Intel Xeon；Compaq公司是Alpha架构。</p>
<p>精简指令集RISC(Reduced Instruction Set Computer)和复杂指令集CISC(Complex Instruction Set Computer)的区分是从上个世纪70年代开始的，IBM研究关于CPU如何高效的运行，发现有些常用的指令占比很高。20%的指令完成了80%的工作。</p>
<p><img src="/images/951413iMgBlog/35161ebe1d0b571845c1904abaafa604.png" alt="img"></p>
<p>把CPU从硬件上设计简单一点，从而使得软件上高效一点，这样就提出了精简指令集这个概念，其最大的特点就是它的<strong>指令宽度是相等的</strong>，每个指令执行的周期几乎也相同，这样把复杂的指令简单化，最后用简单的操作去完成一件复杂的任务。</p>
<p>而复杂指令集(CISC)每一个<strong>指令的长度是不同的，导致机器码、指令码不同，导致每条指令的执行周期不同，从而使得软件流水操作上处理的步骤不一样的</strong>。这样的一个好处是一个指令就能完成一个比较复杂的事情，对上层的程序员来讲，会容易理解一些，特别是汇编程序员。</p>
<h3 id="RISC特点"><a href="#RISC特点" class="headerlink" title="RISC特点"></a>RISC特点</h3><p>​    RISC的主要特点如下:</p>
<ul>
<li>简单、基本的指令:通过简单、基本的指令，组合成复杂指令。</li>
<li>同样长度的指令：每条指令的长度都是相同的，可以在一个单独操作里完成。</li>
<li>单机器周期指令(易流水线)：大多数的指令都可以在一个机器周期里完成，并且允许处理器在同一时间内执行一系列的指令。便于流水线操作执行。</li>
<li>更多的通用寄存器:例如ARM处理器具有31个通用寄存器。大多数数据操作都在寄存器中完成</li>
<li>寻址方式简化:由于指令长度固定，指令格式和寻址方式种类减少。</li>
<li>Load/Store结构:使用load/store指令批量从内存中读写数据，数据传输效率高；</li>
<li>体积小，低功耗，低成本;</li>
</ul>
<p>​    从RISC的特点，我们可以得到RISC体系的优缺点,其实当前很多底层技术相互之间在不断融合，所以以下也只能参考了:</p>
<p>​     优点：在使用相同的芯片技术和相同运行时钟下，RISC 系统的运行速度将是 CISC 的2～4倍。由于RISC处理器的指令集是精简的，所以内存管理单元、浮点单元等都能更容易的设计在同一块芯片上。RISC处理器比相对应的 CISC处理器设计更简单，开发设计周期更短，可以比CISC处理器应用更多先进的技术，更快迭代的下一代处理器。</p>
<p>​     缺点：更多指令的操作使得程序开发者必须小心地选用合适的编译器，编写的代码量会变得非常大。另外RISC体系的处理器需要更快的存储器，这通常都集成于处理器内部(现在处理器当前都有CACHE的)。</p>
<h3 id="CISC-Complex-instruction-set-computer"><a href="#CISC-Complex-instruction-set-computer" class="headerlink" title="CISC(Complex instruction set computer)"></a>CISC(Complex instruction set computer)</h3><p>​    复杂指令集(CISC)是伴随着计算机诞生便存在的指令集，拥有较强的处理高级语言的能力，对于提高计算机性能有一定好处。但是日趋复杂的指令系统带来了效率的低下，使系统结构的复杂性增加，也将导致了CISC的通用性不佳。从指令集架构来看，Intel也承认，CISC架构确实限制了CPU的发展。</p>
<p>​    CISC体系的指令特征如下，</p>
<ul>
<li>使用微代码，指令集可以直接在微代码存储器(比主存储器的速度快很多)里执行。</li>
<li>庞大的指令集，可以减少编程所需要的代码行数，减轻程序员的负担。包括双运算元格式、寄存器到寄存器、寄存器到存储器以及存储器到寄存器的指令。</li>
</ul>
<p>​    指令特征也直接显现了CISC体系的优缺点:</p>
<p>​    <strong>优点：</strong>可有效缩短新指令的微代码设计时间，允许设计师实现 CISC 体系机器的向上兼容。新的系统可以使用一个包含早期系统的指令超集合。另外微程序指令的格式与高级语言相匹配，因而编译器并不一定要重新编写。</p>
<p>​    <strong>缺点：</strong>指令集以及芯片的设计比上一代产品更复杂，不同的指令，需要不同的时钟周期来完成，执行较慢的指令，将影响整台机器的执行效率。</p>
<p>目前X86指令集通用寄存器组(CPU的内核)有16个通用寄存器(rax, rbx, rcx, rdx, rbp, rsp, rsi, rdi, r8, r9, r10, r11, r12, r13, r14, r15),与ARM的31个通用寄存器比起来是少了近一半。X86 CPU复杂指令执行时大多数时间是访问存储器中的数据，会直接拖慢指令执行速度。</p>
<p>X86处理器特有解码器(Decode Unit)，把长度不定的x86指令转换为长度固定的类似于RISC的指令，并交给RISC内核。解码分为硬件解码和微解码，<strong>对于简单的x86指令只要硬件解码即可，速度较快，但复杂的x86指令则需要进行微解码，并把它分成若干条简单指令，速度较慢且很复杂。X86指令集严重制约了性能表现</strong>。</p>
<p> x86 需要将一些工位拆开（这意味着流水线工位更多或者流水线长度更深）。流水线设计可以让指令完成时间更短（理论上受限于流水线执行时间最长的工位），因此将一些工位再拆开的话，虽然依然是每个周期完成一条指令，但是“周期”更短意味着指令吞吐时间进一步缩短，每秒能跑出来的指令数更多，这就是超级流水线的初衷。</p>
<h4 id="AMD64"><a href="#AMD64" class="headerlink" title="AMD64"></a>AMD64</h4><p>因为AMD知道自己造不出能与IA64兼容的处理器，于是它把x86扩展一下，加入了64位寻址和64位寄存器。最终出来的架构，就是 AMD64，成为了64位版本的x86处理器的标准。这一次AMD又交了一个满分答卷。</p>
<p>也就是64位的x86是AMD做出来的，所以经常看到AMD64、X86_64这种叫法。Intel最早是和惠普合作的安腾64位芯片 IA64，后放弃</p>
<h3 id="RISC和CISC比较"><a href="#RISC和CISC比较" class="headerlink" title="RISC和CISC比较"></a>RISC和CISC比较</h3><p><strong>大量的复杂指令、可变的指令长度、多种的寻址方式这些是CISC的特点，也是启缺点。因为这都大大增加了解码的难度，在现在的高速硬件发展下，复杂指令所带来的速度提升已不及在解码上浪费的时间</strong></p>
<p><strong>而RISC体系的指令格式种类少，寻址方式种类少，大多数是简单指令且都能在一个时钟周期内完成，易于设计超标量与流水线，寄存器数量多，大量操作在寄存器之间进行，其优点是不言而喻的。</strong></p>
<p><img src="/images/951413iMgBlog/3e9074d16ebaf8d21d7d5cc64ea9e955.png" alt="img"></p>
<p> 总体上来看：</p>
<p>​    执行时间和空间:CISC 因指令复杂，故采用微指令码控制单元的设计，而RISC的指令90%是由硬件直接完成，只有10%的指令是由软件以组合的方式完成，因此指令执行时间上RISC较短，但RISC所须ROM空间相对的比较大。</p>
<p>​    寻址方面：CISC的需要较多的寻址模式，而RISC只有少数的寻址模式，因此CPU在计算存储器有效位址时，CISC占用的周期较多。</p>
<p>​    指令执行：CISC指令的格式长短不一，执行时的周期次数也不统一，而RISC结构刚好相反，适合采用流水线处理架构的设计，进而可以达到平均一周期完成一指令。</p>
<p>​    设计上:RISC较CISC简单，同时因为CISC的执行步骤过多，闲置的单元电路等待时间增长，不利于平行处理的设计，所以就效能而言RISC较CISC还是占了上风，但<strong>RISC因指令精简化后造成应用程式码变大，需要较大的存储器空间</strong>。</p>
<p>​    RISC是为了提高处理器运行速度而设计的芯片设计体系,关键技术在于流水线操作(Pipelining)：在一个时钟周期里完成多条指令。目前，超流水线以及超标量设计技术已普遍在芯片设计中使用。</p>
<p>​    而ARM的优势不在于性能强大而在于效率，ARM采用RISC流水线指令集，在<strong>完成综合性工作方面可能就处于劣势，而在一些任务相对固定的应用场合其优势就能发挥得淋漓尽致</strong>。</p>
<p><a href="https://topic.atatech.org/articles/176546" target="_blank" rel="noopener">ARM和X86的系统架构差异分析——中篇(狭路相逢)</a></p>
<h4 id="能耗"><a href="#能耗" class="headerlink" title="能耗"></a>能耗</h4><p>X86为了保持高性能，使用乱序执行，这样会让大部分的模块都保持开启，并且时钟也保持切换，直接后果就是耗电高。而ARM的指令确定执行顺序(移动设备)，并且依靠多核而不是单核多线程来执行，容易保持子模块和时钟信号的关闭，显然就会更省电一点(当然目前ARM也是支持乱序的)。</p>
<p>一条指令被解码并准备执行时，Intel和ARM的处理器都使用流水线，就是说解码的过程是并行的。为了更快地执行指令，这些流水线可以被设计成允许指令们不按照程序的顺序被执行（乱序执行）。一些巧逻辑结构可以判断下一条指令是否依赖于当前的指令执行的结果。Intel和ARM都提供乱序执行逻辑结构，由于这结构复杂，直接会导致更多的功耗。</p>
<p>在服务器领域，ARM为追求性能，其功耗优势应该会渐渐消失。</p>
<p>能效方面ARM还是相比Intel 的X86也是优势很多，给出的是64core期soc功率是105w(安培80核心的官方数据是210W)，而Intel 8163的 TDP是165W。</p>
<h3 id="RISC-V指令架构，开源"><a href="#RISC-V指令架构，开源" class="headerlink" title="RISC-V指令架构，开源"></a>RISC-V指令架构，开源</h3><p>平头哥玄铁系列CPU，基于RISC-V指令架构</p>
<p>RISC-V目前主要是布局大数据、人工智能等领域，从ARM和X86尚未完全占领的市场起步。以目前RISC-V在业界掀起的巨大波澜来看，将来很可能足以挑战x86和ARM的地位。</p>
<p>RISC-V指令集由一个非常小的基础指令集和一系列可选的扩展指令集。最基础的指令集只包含40条指令，通过扩展还支持64位和128位的运算以及变长指令，扩展包括了乘除运算、原子操作、浮点运算等，以及开发中的指令集包括压缩指令、位运算、事务存储、矢量计算等。指令集的开发也遵循开源软件的开发方式。</p>
<p>RISC-V架构精简，现阶段已经可以对应执行64位元运算模式，相比ARM Cortex-A5架构设计的处理器，RISC-V架构打造的处理器约可在运算效能提升10%，并且在占用面积精简49%，用于嵌入式装置可带来不少竞争优势。</p>
<p>在商业授权方面，通过指令集扩展，任何企业都可以构建适用于任何领域的微处理器，比如云计算、存储、并行计算、虚拟化/容器、MCU、应用处理器、DSP处理器等等。目前Berkeley开发了多款开源的处理器，可覆盖从高性能计算到嵌入式等应用领域，并孵化出了初创公司SiFive并获得了风投。</p>
<h2 id="ARM"><a href="#ARM" class="headerlink" title="ARM"></a>ARM</h2><p> ARM公司最早是由赫尔曼·豪泽（Hermann Hauser）和工程师Chris Curry在1978年创立（早期全称是 Acorn RISC Machine），后来改名为现在的ARM公司（Advanced RISC Machine）</p>
<p><img src="/images/951413iMgBlog/ac0bac75ae745316e0c011ffdc5a78a5.png" alt="img"></p>
<h3 id="ARM-芯片厂家"><a href="#ARM-芯片厂家" class="headerlink" title="ARM 芯片厂家"></a>ARM 芯片厂家</h3><p>查看厂家</p>
<blockquote>
<p>cat /proc/cpuinfo |grep implementer</p>
<p>CPU implementer    : 0x70</p>
<p>#cat /sys/devices/system/cpu/cpu0/regs/identification/midr_el1<br>0x00000000701f6633  // 70 表示厂家</p>
</blockquote>
<p>vendor id对应厂家</p>
<table>
<thead>
<tr>
<th style="text-align:left">Vendor Name</th>
<th style="text-align:left">Vendor ID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ARM</td>
<td style="text-align:left">0x41</td>
</tr>
<tr>
<td style="text-align:left">Broadcom</td>
<td style="text-align:left">0x42</td>
</tr>
<tr>
<td style="text-align:left">Cavium</td>
<td style="text-align:left">0x43</td>
</tr>
<tr>
<td style="text-align:left">DigitalEquipment</td>
<td style="text-align:left">0x44</td>
</tr>
<tr>
<td style="text-align:left">HiSilicon</td>
<td style="text-align:left">0x48</td>
</tr>
<tr>
<td style="text-align:left">Infineon</td>
<td style="text-align:left">0x49</td>
</tr>
<tr>
<td style="text-align:left">Freescale</td>
<td style="text-align:left">0x4D</td>
</tr>
<tr>
<td style="text-align:left">NVIDIA</td>
<td style="text-align:left">0x4E</td>
</tr>
<tr>
<td style="text-align:left">APM</td>
<td style="text-align:left">0x50</td>
</tr>
<tr>
<td style="text-align:left">Qualcomm</td>
<td style="text-align:left">0x51</td>
</tr>
<tr>
<td style="text-align:left">Marvell</td>
<td style="text-align:left">0x56</td>
</tr>
<tr>
<td style="text-align:left">Intel</td>
<td style="text-align:left">0x69</td>
</tr>
<tr>
<td style="text-align:left">飞腾</td>
<td style="text-align:left">0x70</td>
</tr>
</tbody>
</table>
<h2 id="X86"><a href="#X86" class="headerlink" title="X86"></a>X86</h2><p>Intel x86从开始就定位于PC机，应用多是计算密集型的，如多媒体、科研计算、模拟等(在1999年市值突破了5000亿美金，当然没有一家企业是顺风顺水的，在Intel 奔腾四（高频、高压、高功耗）年代(2005年+)，基本是被AMD的速龙XP摁在地上打，而且后来的奔腾四双核为了追进度也是直接是<strong>胶水双核，就是将两个DIE直接封装在一起，没有专用总线</strong>，成为其历史上最愚蠢的决定之一)。从2006年的酷睿架构开始搭载笔记本电脑，Intel才开始再次的腾飞，并开始甩开AMD。</p>
<p>CISC架构也会使得硬件的逻辑复杂，晶体管数量庞大。为了进一步高效地进行运算，x86架构会有较长的流水线以达到指令级并行(ILP)，而长流水线也会带来了弊端，当遇到分支时，如果预载入分支指令不是未来真实的分支，那么要清空整个流水。因此，x86有引入了复杂的分支预测机构，来确保流水线的效率。再加上多级cache，超线程、虚拟化等等技术，使得x86的复杂度越来越高，而向后兼容性也使得Intel历史包袱越来越大。</p>
<p><img src="/images/951413iMgBlog/750px-cascade_lake_naming_scheme.svg.png" alt="cascade lake naming scheme.svg"></p>
<p>Intel skylake 架构图</p>
<p><img src="/images/951413iMgBlog/950px-skylake_server_block_diagram.svg.png" alt="skylake server block diagram.svg"></p>
<p>iTLB:instruct TLB </p>
<p>dTLB:data TLB</p>
<p><img src="/images/951413iMgBlog/cache-ht-hierarchy-2.jpg" alt="img"></p>
<p>uma下cpu访问内存：</p>
<p><img src="/images/951413iMgBlog/numa-fsb-3.png" alt="x86 UMA"></p>
<h3 id="numa下："><a href="#numa下：" class="headerlink" title="numa下："></a>numa下：</h3><p><img src="/images/951413iMgBlog/numa-imc-iio-smb-4.png" alt="img"></p>
<p>在两路及以上的服务器，远程 DRAM 的访问延迟，远远高于本地 DRAM 的访问延迟，有些系统可以达到 2 倍的差异。即使服务器 BIOS 里关闭了 NUMA 特性，也只是对 OS 内核屏蔽了这个特性，这种延迟差异还是存在的</p>
<p>如果 BIOS 打开了 NUMA 支持，Linux 内核则会根据 ACPI 提供的表格，针对 NUMA 节点做一系列的 NUMA 亲和性的优化。</p>
<p><a href="https://www.nas.nasa.gov/hecc/support/kb/cascade-lake-processors_579.html" target="_blank" rel="noopener">Like Skylake, there are two sub-NUMA clusters in each Cascade Lake socket</a>, creating two localization domains. There are three memory channels per sub-NUMA cluster. Each channel can be connected with up to two memory DIMMs. For the Aitken Cascade Lake configuration, there is one 16-gigabyte (GB) dual rank DDR4 DIMM with error correcting code (ECC) support per channel. In total, the amount of memory is 48 GB per sub-NUMA cluster, 96 GB per socket, and 192 GB per node.</p>
<p>The speed of each memory channel is increased from 2,666 MHz in Skylake to 2,933 MHz in Cascade Lake. An 8-byte read or write can take place per cycle per channel. With a total of six memory channels, the total half-duplex memory bandwidth is approximately 141 GB/s per socket.</p>
<p><img src="/images/951413iMgBlog/ka02R000000oNBPQA2_en_US_1.jpeg" alt="SLN316864_en_US__1image001(1)"></p>
<p><a href="https://www.dell.com/support/kbdoc/en-sg/000176921/bios-characterization-for-hpc-with-intel-cascade-lake-processors" target="_blank" rel="noopener">性能测试数据</a></p>
<p>ob在鲲鹏920上测试，如果BIOS 里enable NUMA，NUMA-Aware 的就近内存分配和使用能提升性能，在我们的测试中，性能提高8%</p>
<p><img src="/images/951413iMgBlog/e475286bef0d734feca8fba300a501e6.png" alt="img"></p>
<p>sysctl -w kernel.numa_balancing=0 TPS是72W，sysctl -w kernel.numa_balancing=1 TPS为57W</p>
<h2 id="不同CPU的一些性能数据对比"><a href="#不同CPU的一些性能数据对比" class="headerlink" title="不同CPU的一些性能数据对比"></a>不同CPU的一些性能数据对比</h2><p>以Skylake的IPC（图中表示为GPC）为1.0，那么Apple的M1或者A14的Firestorm大核心，大致上是Skylake的两倍，而Intel Tiger Lake U的最好表现是1.3最差1.20，综合下表现应该是符合Willow Cove是Skylake 1.25X IPC附近的条件的。所以说Firestorm也是Intel Willow Cove的1.5~1.6X的IPC的，排除Geekbench的权威性不足以及测试误差，也可以说苹果Firestorm显著超过现有X86的。</p>
<p>对于处理器来说，跑高频的能力固然重要，但是高频也会造成功耗的快速上升，一个跑4.8G的CPU只比跑3.2G时高了50%性能，但是功耗可能是3倍乃至更多（比如你看AMD）。</p>
<p><img src="/images/951413iMgBlog/v2-22ac73d34a31de0c98d773199448be24_1440w.jpg" alt="img"></p>
<p>接下来通过实际测试来探讨一下不同CPU架构的性能，以及NUMA和绑核对性能的影响。</p>
<h2 id="飞腾ARM芯片案例"><a href="#飞腾ARM芯片案例" class="headerlink" title="飞腾ARM芯片案例"></a>飞腾ARM芯片案例</h2><p>ARMv64 架构CPU，基本都是运行网络瓶颈的业务逻辑</p>
<p><img src="/images/oss/16b271c8-5132-4273-a26a-4b35e8f92882.png" alt="img"></p>
<p>cpu详细信息：</p>
<p><img src="/images/oss/e177902c-73b2-4535-9c1f-2726451820db.png" alt="img"></p>
<p>飞腾芯片，按如下distance绑核基本没区别！展示出来的distance是假的一样</p>
<p><img src="/images/oss/5a19ff61-68db-4c65-be4c-6b6c155a8a29.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/image-20210422095217195.png" alt="image-20210422095217195"></p>
<p>绑核后对性能提升非常明显：</p>
<p><img src="/images/oss/4d4fdebb-6146-407e-881d-19170fbfd82b.png" alt="img"></p>
<p>点查场景：</p>
<p><img src="/images/951413iMgBlog/image-20210425092158127.png" alt="image-20210425092158127"></p>
<p>如上是绑48-63号核</p>
<p><img src="/images/951413iMgBlog/image-20210425091727122.png" alt="image-20210425091727122"></p>
<p><img src="/images/951413iMgBlog/image-20210425091557750.png" alt="image-20210425091557750"></p>
<p><img src="/images/951413iMgBlog/image-20210425093630438.png" alt="image-20210425093630438"></p>
<p>绑不同的核性能差异比较大，比如同样绑第一个socket最后16core和绑第二个socket最后16core，第二个socket的最后16core性能要好25-30%—<strong>这是因为网卡软中断，如果将软中断绑定到0-4号cpu后差异基本消失</strong>,因为网卡队列设置的是60，基本跑在前60core上，也就是第一个socket上。</p>
<p>点查场景绑核和不绑核性能能差1倍, 将table分表后，物理rt稳定了(<strong>截图中物理rt下降是因为压力小了</strong>)</p>
<h3 id="点查场景压测16个core的节点"><a href="#点查场景压测16个core的节点" class="headerlink" title="点查场景压测16个core的节点"></a>点查场景压测16个core的节点</h3><p>一个节点16core，16个core绑定到14、15号NUMA上，然后压测</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">#perl numa-maps-summary.pl &lt;/proc/79694/numa_maps //16core</span><br><span class="line">N0        :         1103 (  0.00 GB)</span><br><span class="line">N1        :       107368 (  0.41 GB)</span><br><span class="line">N10       :       144736 (  0.55 GB)</span><br><span class="line">N11       :        16919 (  0.06 GB)</span><br><span class="line">N12       :       551987 (  2.11 GB)</span><br><span class="line">N13       :        59499 (  0.23 GB)</span><br><span class="line">N14       :      5621573 ( 21.44 GB)  //内存就近分配</span><br><span class="line">N15       :      6200398 ( 23.65 GB)</span><br><span class="line">N2        :          700 (  0.00 GB)</span><br><span class="line">N3        :           89 (  0.00 GB)</span><br><span class="line">N4        :         5784 (  0.02 GB)</span><br><span class="line">N5        :           77 (  0.00 GB)</span><br><span class="line">N6        :          426 (  0.00 GB)</span><br><span class="line">N7        :          472 (  0.00 GB)</span><br><span class="line">N8        :          107 (  0.00 GB)</span><br><span class="line">N9        :         6137 (  0.02 GB)</span><br><span class="line">active    :           85 (  0.00 GB)</span><br><span class="line">anon      :     12712675 ( 48.50 GB)</span><br><span class="line">dirty     :     12712679 ( 48.50 GB)</span><br><span class="line">kernelpagesize_kB:        17444 (  0.07 GB)</span><br><span class="line">mapmax    :         1598 (  0.01 GB)</span><br><span class="line">mapped    :         4742 (  0.02 GB)</span><br><span class="line"></span><br><span class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses -a -p 79694</span><br><span class="line">^C</span><br><span class="line"> Performance counter stats for process id &apos;79694&apos;:</span><br><span class="line"></span><br><span class="line">        1719788217      branch-misses                                                 (39.70%)</span><br><span class="line">      311069393237      bus-cycles                                                    (38.07%)</span><br><span class="line">        2021349865      cache-misses              #    6.669 % of all cache refs      (38.32%)</span><br><span class="line">       30308501243      cache-references                                              (39.67%)</span><br><span class="line">      310980728138      cpu-cycles                                                    (46.46%)</span><br><span class="line">       67298903097      instructions              #    0.22  insns per cycle          (47.63%)</span><br><span class="line">        1983728595      L1-dcache-load-misses     #    6.62% of all L1-dcache hits    (48.76%)</span><br><span class="line">       29943167305      L1-dcache-loads                                               (47.89%)</span><br><span class="line">        1957152091      L1-dcache-store-misses                                        (46.14%)</span><br><span class="line">       29572767575      L1-dcache-stores                                              (44.91%)</span><br><span class="line">        4223808613      L1-icache-load-misses                                         (43.08%)</span><br><span class="line">       49122358099      L1-icache-loads                                               (38.15%)</span><br><span class="line">        1724605628      branch-load-misses                                            (37.63%)</span><br><span class="line">       15225535577      branch-loads                                                  (36.61%)</span><br><span class="line">         997458038      dTLB-load-misses                                              (35.81%)</span><br><span class="line">         542426693      iTLB-load-misses                                              (34.98%)</span><br><span class="line"></span><br><span class="line">      10.489297296 seconds time elapsed</span><br><span class="line">      </span><br><span class="line">[  29s] threads: 160, tps: 0.00, reads/s: 15292.01, writes/s: 0.00, response time: 25.82ms (95%)</span><br><span class="line">[  30s] threads: 160, tps: 0.00, reads/s: 16399.99, writes/s: 0.00, response time: 23.58ms (95%)</span><br><span class="line">[  31s] threads: 160, tps: 0.00, reads/s: 17025.00, writes/s: 0.00, response time: 20.73ms (95%)</span><br><span class="line">[  32s] threads: 160, tps: 0.00, reads/s: 16991.01, writes/s: 0.00, response time: 22.83ms (95%)</span><br><span class="line">[  33s] threads: 160, tps: 0.00, reads/s: 18400.94, writes/s: 0.00, response time: 21.29ms (95%)</span><br><span class="line">[  34s] threads: 160, tps: 0.00, reads/s: 17760.05, writes/s: 0.00, response time: 20.69ms (95%)</span><br><span class="line">[  35s] threads: 160, tps: 0.00, reads/s: 17935.00, writes/s: 0.00, response time: 20.23ms (95%)</span><br><span class="line">[  36s] threads: 160, tps: 0.00, reads/s: 18296.98, writes/s: 0.00, response time: 20.10ms (95%)</span><br><span class="line">[  37s] threads: 160, tps: 0.00, reads/s: 18111.02, writes/s: 0.00, response time: 20.56ms (95%)</span><br><span class="line">[  38s] threads: 160, tps: 0.00, reads/s: 17782.99, writes/s: 0.00, response time: 20.54ms (95%)</span><br><span class="line">[  38s] threads: 160, tps: 0.00, reads/s: 21412.13, writes/s: 0.00, response time: 11.96ms (95%)</span><br><span class="line">[  40s] threads: 160, tps: 0.00, reads/s: 18027.85, writes/s: 0.00, response time: 20.18ms (95%)</span><br><span class="line">[  41s] threads: 160, tps: 0.00, reads/s: 17907.04, writes/s: 0.00, response time: 20.02ms (95%)</span><br><span class="line">[  42s] threads: 160, tps: 0.00, reads/s: 13860.96, writes/s: 0.00, response time: 23.58ms (95%)</span><br><span class="line">[  43s] threads: 160, tps: 0.00, reads/s: 18491.02, writes/s: 0.00, response time: 20.18ms (95%)</span><br><span class="line">[  44s] threads: 160, tps: 0.00, reads/s: 17673.02, writes/s: 0.00, response time: 20.85ms (95%)</span><br><span class="line">[  45s] threads: 160, tps: 0.00, reads/s: 18048.96, writes/s: 0.00, response time: 21.47ms (95%)</span><br><span class="line">[  46s] threads: 160, tps: 0.00, reads/s: 18130.03, writes/s: 0.00, response time: 22.13ms (95%)</span><br></pre></td></tr></table></figure>
<h3 id="点查场景压测8个core的节点"><a href="#点查场景压测8个core的节点" class="headerlink" title="点查场景压测8个core的节点"></a>点查场景压测8个core的节点</h3><p>因为每个NUMA才8个core，所以测试一下8core的节点绑核前后性能对比。实际结果看起来和16core节点绑核性能提升差不多。</p>
<p>绑核前后对比：绑核后QPS翻倍，DRDS上的rt从7.5降低到了2.2，rt下降非常明显，可以看出主要是绑核前跨numa访问慢。实际这个测试是先跑的不绑核，内存分布在所有NUMA上，没有重启再绑核就直接测试了，所以性能提升不明显，因为内存已经跨NUMA分配完毕了。</p>
<p><img src="/images/951413iMgBlog/image-20210427093424116.png" alt="image-20210427093424116"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">#perl numa-maps-summary.pl &lt;/proc/33727/numa_maps //绑定8core后，在如下内存分配下QPS能到11000，但是抖动略大，应该是一个numa内存不够了</span><br><span class="line">N0        :          551 (  0.00 GB)</span><br><span class="line">N1        :      1023418 (  3.90 GB)</span><br><span class="line">N10       :        52065 (  0.20 GB)</span><br><span class="line">N11       :       190737 (  0.73 GB)</span><br><span class="line">N12       :       516115 (  1.97 GB)</span><br><span class="line">N13       :       186556 (  0.71 GB)</span><br><span class="line">N14       :      1677489 (  6.40 GB)</span><br><span class="line">N15       :       324531 (  1.24 GB)</span><br><span class="line">N2        :          397 (  0.00 GB)</span><br><span class="line">N3        :            8 (  0.00 GB)</span><br><span class="line">N4        :          398 (  0.00 GB)</span><br><span class="line">N6        :          349 (  0.00 GB)</span><br><span class="line">N7        :          437 (  0.00 GB)</span><br><span class="line">N8        :       108508 (  0.41 GB)</span><br><span class="line">N9        :        69162 (  0.26 GB)</span><br><span class="line">active    :         2296 (  0.01 GB)</span><br><span class="line">anon      :      4144997 ( 15.81 GB)</span><br><span class="line">dirty     :      4145002 ( 15.81 GB)</span><br><span class="line">kernelpagesize_kB:         7508 (  0.03 GB)</span><br><span class="line">mapmax    :         1548 (  0.01 GB)</span><br><span class="line">mapped    :         5724 (  0.02 GB)</span><br><span class="line"></span><br><span class="line">[ 349s] threads: 100, tps: 0.00, reads/s: 11088.99, writes/s: 0.00, response time: 20.18ms (95%)</span><br><span class="line">[ 350s] threads: 100, tps: 0.00, reads/s: 8778.98, writes/s: 0.00, response time: 26.20ms (95%)</span><br><span class="line">[ 351s] threads: 100, tps: 0.00, reads/s: 7995.01, writes/s: 0.00, response time: 31.79ms (95%)</span><br><span class="line">[ 352s] threads: 100, tps: 0.00, reads/s: 9549.01, writes/s: 0.00, response time: 23.90ms (95%)</span><br><span class="line">[ 353s] threads: 100, tps: 0.00, reads/s: 8757.99, writes/s: 0.00, response time: 24.60ms (95%)</span><br><span class="line">[ 354s] threads: 100, tps: 0.00, reads/s: 10288.02, writes/s: 0.00, response time: 21.85ms (95%)</span><br><span class="line">[ 355s] threads: 100, tps: 0.00, reads/s: 11003.97, writes/s: 0.00, response time: 18.90ms (95%)</span><br><span class="line">[ 356s] threads: 100, tps: 0.00, reads/s: 11111.01, writes/s: 0.00, response time: 20.51ms (95%)</span><br><span class="line">[ 357s] threads: 100, tps: 0.00, reads/s: 11426.00, writes/s: 0.00, response time: 17.98ms (95%)</span><br><span class="line">[ 358s] threads: 100, tps: 0.00, reads/s: 11007.01, writes/s: 0.00, response time: 19.35ms (95%)</span><br><span class="line">[ 359s] threads: 100, tps: 0.00, reads/s: 10425.00, writes/s: 0.00, response time: 20.92ms (95%)</span><br><span class="line">[ 360s] threads: 100, tps: 0.00, reads/s: 10024.00, writes/s: 0.00, response time: 23.17ms (95%)</span><br><span class="line">[ 361s] threads: 100, tps: 0.00, reads/s: 10100.98, writes/s: 0.00, response time: 22.94ms (95%)</span><br><span class="line">[ 362s] threads: 100, tps: 0.00, reads/s: 8164.01, writes/s: 0.00, response time: 27.48ms (95%)</span><br><span class="line">[ 363s] threads: 100, tps: 0.00, reads/s: 6593.00, writes/s: 0.00, response time: 37.10ms (95%)</span><br><span class="line">[ 364s] threads: 100, tps: 0.00, reads/s: 7008.00, writes/s: 0.00, response time: 32.32ms (95%)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#调整这个实例到内存充足的NUMA7上 QPS峰值能到14000，稳定在11000-13000之间，RT明显更稳定了</span><br><span class="line">#perl numa-maps-summary.pl &lt;/proc/78245/numa_maps</span><br><span class="line">N0        :          551 (  0.00 GB)</span><br><span class="line">N1        :          115 (  0.00 GB)</span><br><span class="line">N11       :          695 (  0.00 GB)</span><br><span class="line">N12       :          878 (  0.00 GB)</span><br><span class="line">N13       :         2019 (  0.01 GB)</span><br><span class="line">N14       :           25 (  0.00 GB)</span><br><span class="line">N15       :           60 (  0.00 GB)</span><br><span class="line">N2        :          394 (  0.00 GB)</span><br><span class="line">N3        :            8 (  0.00 GB)</span><br><span class="line">N4        :       197713 (  0.75 GB)</span><br><span class="line">N6        :          349 (  0.00 GB)</span><br><span class="line">N7        :      3957844 ( 15.10 GB)</span><br><span class="line">N8        :            1 (  0.00 GB)</span><br><span class="line">active    :           10 (  0.00 GB)</span><br><span class="line">anon      :      4154693 ( 15.85 GB)</span><br><span class="line">dirty     :      4154698 ( 15.85 GB)</span><br><span class="line">kernelpagesize_kB:         7452 (  0.03 GB)</span><br><span class="line">mapmax    :         1567 (  0.01 GB)</span><br><span class="line">mapped    :         5959 (  0.02 GB)</span><br><span class="line"></span><br><span class="line">[ 278s] threads: 100, tps: 0.00, reads/s: 13410.99, writes/s: 0.00, response time: 15.36ms (95%)</span><br><span class="line">[ 279s] threads: 100, tps: 0.00, reads/s: 14049.99, writes/s: 0.00, response time: 15.54ms (95%)</span><br><span class="line">[ 280s] threads: 100, tps: 0.00, reads/s: 13107.02, writes/s: 0.00, response time: 16.72ms (95%)</span><br><span class="line">[ 281s] threads: 100, tps: 0.00, reads/s: 12431.99, writes/s: 0.00, response time: 17.79ms (95%)</span><br><span class="line">[ 282s] threads: 100, tps: 0.00, reads/s: 13164.01, writes/s: 0.00, response time: 16.33ms (95%)</span><br><span class="line">[ 283s] threads: 100, tps: 0.00, reads/s: 13455.01, writes/s: 0.00, response time: 16.19ms (95%)</span><br><span class="line">[ 284s] threads: 100, tps: 0.00, reads/s: 12932.01, writes/s: 0.00, response time: 16.22ms (95%)</span><br><span class="line">[ 285s] threads: 100, tps: 0.00, reads/s: 12790.99, writes/s: 0.00, response time: 17.00ms (95%)</span><br><span class="line">[ 286s] threads: 100, tps: 0.00, reads/s: 12706.00, writes/s: 0.00, response time: 17.88ms (95%)</span><br><span class="line">[ 287s] threads: 100, tps: 0.00, reads/s: 11886.00, writes/s: 0.00, response time: 19.43ms (95%)</span><br><span class="line">[ 288s] threads: 100, tps: 0.00, reads/s: 12700.00, writes/s: 0.00, response time: 16.97ms (95%)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#perl numa-maps-summary.pl &lt;/proc/54723/numa_maps  //54723绑定在NUMA6上</span><br><span class="line">N0        :          551 (  0.00 GB)</span><br><span class="line">N1        :          115 (  0.00 GB)</span><br><span class="line">N11       :          682 (  0.00 GB)</span><br><span class="line">N12       :          856 (  0.00 GB)</span><br><span class="line">N13       :         2018 (  0.01 GB)</span><br><span class="line">N14       :           25 (  0.00 GB)</span><br><span class="line">N15       :           60 (  0.00 GB)</span><br><span class="line">N2        :      1270166 (  4.85 GB) //不应该分配这里的内存，实际是因为N6内存被PageCache使用掉了</span><br><span class="line"></span><br><span class="line">N3        :            8 (  0.00 GB)</span><br><span class="line">N4        :          398 (  0.00 GB)</span><br><span class="line">N6        :      3662400 ( 13.97 GB)</span><br><span class="line">N7        :          460 (  0.00 GB)</span><br><span class="line">N8        :            1 (  0.00 GB)</span><br><span class="line">active    :            9 (  0.00 GB)</span><br><span class="line">anon      :      4931796 ( 18.81 GB)</span><br><span class="line">dirty     :      4931801 ( 18.81 GB)</span><br><span class="line">kernelpagesize_kB:         7920 (  0.03 GB)</span><br><span class="line">mapmax    :         1580 (  0.01 GB)</span><br><span class="line">mapped    :         5944 (  0.02 GB)</span><br><span class="line"></span><br><span class="line">#cat /proc/meminfo | grep -i active</span><br><span class="line">Active:         22352360 kB</span><br><span class="line">Inactive:       275173756 kB</span><br><span class="line">Active(anon):      16984 kB</span><br><span class="line">Inactive(anon): 240344208 kB</span><br><span class="line">Active(file):   22335376 kB</span><br><span class="line">Inactive(file): 34829548 kB</span><br><span class="line"></span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line">#cat /proc/meminfo | grep -i active</span><br><span class="line">Active:          1865724 kB</span><br><span class="line">Inactive:       242335632 kB</span><br><span class="line">Active(anon):       7108 kB</span><br><span class="line">Inactive(anon): 240199020 kB</span><br><span class="line">Active(file):    1858616 kB  //回收了大量PageCache内存</span><br><span class="line">Inactive(file):  2136612 kB</span><br><span class="line">#perl numa-maps-summary.pl &lt;/proc/54723/numa_maps</span><br><span class="line">N0        :          552 (  0.00 GB)</span><br><span class="line">N1        :          115 (  0.00 GB)</span><br><span class="line">N11       :          682 (  0.00 GB)</span><br><span class="line">N12       :          856 (  0.00 GB)</span><br><span class="line">N13       :         2018 (  0.01 GB)</span><br><span class="line">N14       :           25 (  0.00 GB)</span><br><span class="line">N15       :           60 (  0.00 GB)</span><br><span class="line">N2        :         1740 (  0.01 GB)</span><br><span class="line">N3        :            8 (  0.00 GB)</span><br><span class="line">N4        :          398 (  0.00 GB)</span><br><span class="line">N6        :      4972492 ( 18.97 GB)</span><br><span class="line">N7        :          459 (  0.00 GB)</span><br><span class="line">N8        :            1 (  0.00 GB)</span><br><span class="line">active    :           16 (  0.00 GB)</span><br><span class="line">anon      :      4973486 ( 18.97 GB)</span><br><span class="line">dirty     :      4973491 ( 18.97 GB)</span><br><span class="line">kernelpagesize_kB:         8456 (  0.03 GB)</span><br><span class="line">mapmax    :         1564 (  0.01 GB)</span><br><span class="line">mapped    :         5920 (  0.02 GB)</span><br></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20210427164953340.png" alt="image-20210427164953340"></p>
<p>绑核前的IPC：</p>
<p><img src="/images/951413iMgBlog/image-20210427093625575.png" alt="image-20210427093625575"></p>
<p>绑核后的IPC：</p>
<p><img src="/images/951413iMgBlog/image-20210427095130343.png" alt="image-20210427095130343"></p>
<p><strong>如果是两个8core对一个16core在都最优绑核场景下从上面的数据来看能有40-50%的性能提升，并且RT抖动更小</strong>，这两个8core绑定在同一个Socket下，验证是否争抢，同时可以看到<strong>绑核后性能可以随着加节点线性增加</strong></p>
<p><img src="/images/951413iMgBlog/image-20210427172612685.png" alt="image-20210427172612685"></p>
<p><img src="/images/951413iMgBlog/image-20210427173047815.png" alt="image-20210427173047815"></p>
<p><img src="/images/951413iMgBlog/image-20210427173417673.png" alt="image-20210427173417673"></p>
<p>结论：不绑核一个FT2500的core点查只有500 QPS，绑核后能到1500QPS, 在Intel 8263下一个core能到6000以上(开日志、没开协程)</p>
<h3 id="RDS-数据库场景绑核"><a href="#RDS-数据库场景绑核" class="headerlink" title="RDS 数据库场景绑核"></a>RDS 数据库场景绑核</h3><p>通过同一台物理上6个drds节点，总共96个core，压6台RDS，RDS基本快打挂了。sysbench 点查，32个分表，增加drds节点进来物理rt就增加，从最初的的1.2ms加到6个drds节点后变成8ms。</p>
<p><img src="/images/951413iMgBlog/image-20210425180535225.png" alt="image-20210425180535225"></p>
<p>RDS没绑好核，BIOS默认关闭了NUMA，外加12个rds分布在物理机上不均匀，3个节点3个rds，剩下的物理机上只有一个RDS实例。</p>
<p>RDS每个实例32core，管控默认已经做了绑核，但是如果两个RDS绑在了一个socket上竞争会很激烈，ipc比单独的降一半。</p>
<p>比如这三个rds，qps基本均匀，上面两个cpu高，但是没效率，每个rds绑了32core，上面两个绑在一个socket上，下面的RDS绑在另一个socket上，第一个socket还有网络软中断在争抢cpu，飞腾环境下性能真要冲高还有很大空间。</p>
<p><img src="/images/951413iMgBlog/image-20210425180518926.png" alt="image-20210425180518926"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">#第二个RDS IPC只有第三个的30%多点，这就是为什么CPU高这么多，但是QPS差不多</span><br><span class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses  -a -p 61238</span><br><span class="line">^C</span><br><span class="line"> Performance counter stats for process id &apos;61238&apos;:</span><br><span class="line"></span><br><span class="line">        86,491,052      branch-misses                                                 (58.55%)</span><br><span class="line">    98,481,418,793      bus-cycles                                                    (55.64%)</span><br><span class="line">       113,095,618      cache-misses              #    6.169 % of all cache refs      (53.20%)</span><br><span class="line">     1,833,344,484      cache-references                                              (52.00%)</span><br><span class="line">   101,516,165,898      cpu-cycles                                                    (57.09%)</span><br><span class="line">     4,229,190,014      instructions              #    0.04  insns per cycle          (55.91%)</span><br><span class="line">       111,780,025      L1-dcache-load-misses     #    6.34% of all L1-dcache hits    (55.40%)</span><br><span class="line">     1,764,421,570      L1-dcache-loads                                               (52.62%)</span><br><span class="line">       112,261,128      L1-dcache-store-misses                                        (49.34%)</span><br><span class="line">     1,814,998,338      L1-dcache-stores                                              (48.51%)</span><br><span class="line">       219,372,119      L1-icache-load-misses                                         (49.56%)</span><br><span class="line">     2,816,279,627      L1-icache-loads                                               (49.15%)</span><br><span class="line">        85,321,093      branch-load-misses                                            (50.38%)</span><br><span class="line">     1,038,572,653      branch-loads                                                  (50.65%)</span><br><span class="line">        45,166,831      dTLB-load-misses                                              (51.98%)</span><br><span class="line">        29,892,473      iTLB-load-misses                                              (52.56%)</span><br><span class="line"></span><br><span class="line">       1.163750756 seconds time elapsed</span><br><span class="line"></span><br><span class="line">#第三个RDS</span><br><span class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses  -a -p 53400</span><br><span class="line">^C</span><br><span class="line"> Performance counter stats for process id &apos;53400&apos;:</span><br><span class="line"></span><br><span class="line">       295,575,513      branch-misses                                                 (40.51%)</span><br><span class="line">   110,934,600,206      bus-cycles                                                    (39.30%)</span><br><span class="line">       537,938,496      cache-misses              #    8.310 % of all cache refs      (38.99%)</span><br><span class="line">     6,473,688,885      cache-references                                              (39.80%)</span><br><span class="line">   110,540,950,757      cpu-cycles                                                    (46.10%)</span><br><span class="line">    14,766,013,708      instructions              #    0.14  insns per cycle          (46.85%)</span><br><span class="line">       538,521,226      L1-dcache-load-misses     #    8.36% of all L1-dcache hits    (48.00%)</span><br><span class="line">     6,440,728,959      L1-dcache-loads                                               (46.69%)</span><br><span class="line">       533,693,357      L1-dcache-store-misses                                        (45.91%)</span><br><span class="line">     6,413,111,024      L1-dcache-stores                                              (44.92%)</span><br><span class="line">       673,725,952      L1-icache-load-misses                                         (42.76%)</span><br><span class="line">     9,216,663,639      L1-icache-loads                                               (38.27%)</span><br><span class="line">       299,202,001      branch-load-misses                                            (37.62%)</span><br><span class="line">     3,285,957,082      branch-loads                                                  (36.10%)</span><br><span class="line">       149,348,740      dTLB-load-misses                                              (35.20%)</span><br><span class="line">       102,444,469      iTLB-load-misses                                              (34.78%)</span><br><span class="line"></span><br><span class="line">       8.080841166 seconds time elapsed</span><br></pre></td></tr></table></figure>
<p>12个RDS流量基本均匀：</p>
<p><img src="/images/951413iMgBlog/image-20210426083033989.png" alt="image-20210426083033989"></p>
<h3 id="网卡队列调整"><a href="#网卡队列调整" class="headerlink" title="网卡队列调整"></a>网卡队列调整</h3><p>这批机器默认都是双网卡做bond，但是两块网卡是HA，默认网卡队列是60，基本都跑在前面60个core上</p>
<p>将RDS网卡队列从60个改成6个后RDS性能提升大概10%</p>
<p><img src="/images/951413iMgBlog/image-20210426085534983.png" alt="image-20210426085534983"></p>
<p>默认第一个RDS都绑在0-31号核上,其实改少队列加大了0-5号core的压力，但是实际数据表现要好。</p>
<h3 id="查看网卡和numa的关系"><a href="#查看网卡和numa的关系" class="headerlink" title="查看网卡和numa的关系"></a>查看网卡和numa的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#yum install lshw -y</span><br><span class="line">#lshw -C network -short</span><br><span class="line">H/W path               Device          Class      Description</span><br><span class="line">=============================================================</span><br><span class="line">/0/100/0/9/0           eth0            network    MT27710 Family [ConnectX-4 Lx]</span><br><span class="line">/0/100/0/9/0.1         eth1            network    MT27710 Family [ConnectX-4 Lx]</span><br><span class="line">/1                     e41358fae4ee_h  network    Ethernet interface</span><br><span class="line">/2                     86b0637ef1e1_h  network    Ethernet interface</span><br><span class="line">/3                     a6706e785f53_h  network    Ethernet interface</span><br><span class="line">/4                     d351290e50a0_h  network    Ethernet interface</span><br><span class="line">/5                     1a9e5df98dd1_h  network    Ethernet interface</span><br><span class="line">/6                     766ec0dab599_h  network    Ethernet interface</span><br><span class="line">/7                     bond0.11        network    Ethernet interface</span><br><span class="line">/8                     ea004888c217_h  network    Ethernet interface</span><br></pre></td></tr></table></figure>
<p>以及：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lscpu | grep -i numa</span><br><span class="line">numactl --hardware</span><br><span class="line">cat /proc/interrupts | egrep -i &quot;CPU|rx&quot;</span><br></pre></td></tr></table></figure>
<p><a href="https://ixnfo.com/en/how-to-find-out-on-which-numa-node-network-interfaces.html" target="_blank" rel="noopener">Check if the network interfaces are tied to Numa</a> (if -1 means not tied, if 0, then to numa0):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/class/net/eth0/device/numa_node</span><br></pre></td></tr></table></figure>
<p>You can see which NAMA the network card belongs to, for example, using lstopo:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install hwloc -y</span><br><span class="line">lstopo</span><br><span class="line">lstopo --logical</span><br><span class="line">lstopo --logical --output-format png &gt; lstopo.png</span><br></pre></td></tr></table></figure>
<p>如果cpu core太多, interrupts 没法看的话，通过cut只看其中一部分core</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/interrupts | grep -i &apos;eth4\|CPU&apos; | cut -c -8,865-995,1425-</span><br></pre></td></tr></table></figure>
<h2 id="内存和cache的latency对比"><a href="#内存和cache的latency对比" class="headerlink" title="内存和cache的latency对比"></a>内存和cache的latency对比</h2><p><img src="/images/951413iMgBlog/latency.png" alt="latency"></p>
<h2 id="NUMA"><a href="#NUMA" class="headerlink" title="NUMA"></a>NUMA</h2><p>事实上Linux识别到NUMA架构后，默认的内存分配方案就是：优先尝试在请求线程当前所处的CPU的Local内存上分配空间。如果local内存不足，优先淘汰local内存中无用的Page（Inactive，Unmapped）。</p>
<p>intel 芯片跨node延迟远低于其他家，所以跨node性能损耗不大</p>
<p>zone_reclaim_mode，它用来管理当一个内存区域(zone)内部的内存耗尽时，是从其内部进行内存回收还是可以从其他zone进行回收的选项：</p>
<ul>
<li>0 关闭zone_reclaim模式，可以从其他zone或NUMA节点回收内存</li>
<li>1 打开zone_reclaim模式，这样内存回收只会发生在本地节点内</li>
<li>2 在本地回收内存时，可以将cache中的脏数据写回硬盘，以回收内存</li>
<li>4 在本地回收内存时，表示可以用Swap 方式回收内存</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /proc/sys/vm/zone_reclaim_mode</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<p><strong>开启NUMA亲和性后让内存访问速度大大加快了，但是带来的最大问题是容易造成某个NUMA下内存水位太高，进而经常触发Pages 回收，造成系统卡顿。</strong></p>
<h3 id="Understanding-what-triggers-zone-reclaim"><a href="#Understanding-what-triggers-zone-reclaim" class="headerlink" title="Understanding what triggers zone reclaim"></a><a href="https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases" target="_blank" rel="noopener">Understanding what triggers zone reclaim</a></h3><p>When a process requests a page, the kernel checks whether its preferred NUMA zone has enough free memory and if more than 1% of its pages are reclaimable. The percentage is tunable and is determined by the <a href="http://lxr.free-electrons.com/source/Documentation/sysctl/vm.txt#L412" target="_blank" rel="noopener">vm.min_unmapped_ratio sysctl</a>. Reclaimable pages are file backed pages (ie. pages which are allocated through mmapped files) which are not currently mapped to any process. In particular, from <code>/proc/meminfo</code>, the ‘reclaimable pages’ are ‘Active(file)+Inactive(file)-Mapped’ (<a href="http://lxr.free-electrons.com/source/mm/vmscan.c#L3433" target="_blank" rel="noopener">source</a>).</p>
<p>How does the kernel determine how much free memory is enough? The kernel uses zone ‘watermarks’ (<a href="http://lxr.free-electrons.com/source/include/linux/mmzone.h#L235" target="_blank" rel="noopener">source</a>) which are determined through the value in <code>/proc/sys/vm/min_free_kbytes</code> (<a href="http://lwn.net/Articles/422291/" target="_blank" rel="noopener">source</a>). They are also determined by <code>/proc/sys/vm/lowmem_reserve_ratio</code> value (<a href="http://lxr.free-electrons.com/source/Documentation/sysctl/vm.txt#L238" target="_blank" rel="noopener">source</a>). The computed values on a given host can be found from <code>/proc/zoneinfo</code> under the ‘low/min/high’ labels as seen below:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Node 1, zone Normal</span><br><span class="line"> pages free 17353</span><br><span class="line"> min 11284</span><br><span class="line"> low 14105</span><br><span class="line"> high 16926</span><br><span class="line"> scanned 0</span><br><span class="line"> spanned 6291456</span><br><span class="line"> present 6205440</span><br></pre></td></tr></table></figure>
<p>The kernel reclaims pages when the number of free pages in a zone falls below the low water mark. The page reclaim stops when the number of free pages rise above the ‘low’ watermark. <em>Further, these computations are per-zone: a zone reclaim can be triggered on a particular zone even if other zones on the host have plenty of free memory.</em></p>
<p>Here is a graph that demonstrates this behavior from our experiment. Some points worth noting:</p>
<ul>
<li>the black line denotes page scans on the zone and is plotted against the y-axis on the right.</li>
<li>the red line denotes the number of free pages in the zone.</li>
<li>the ‘low’ watermark for the zone is in green.</li>
</ul>
<p><img src="/images/951413iMgBlog/app0610-node0.png" alt="img"></p>
<h4 id="Characteristics-of-systems-experiencing-zone-reclaims"><a href="#Characteristics-of-systems-experiencing-zone-reclaims" class="headerlink" title="Characteristics of systems experiencing zone reclaims"></a>Characteristics of systems experiencing zone reclaims</h4><p>We observed similar patterns on our production hosts. In all cases, we see that the page scan plot is virtually a mirror image of the free pages plot. In other words, Linux predictably triggers zone reclaims when the free pages fall below the ‘low’ watermark of the zone.</p>
<p>Our first observation was that, with zone reclaim enabled, Linux performed mostly direct reclaims (ie. reclaims performed in the context of application threads and counted as direct page scans). Once zone reclaim was disabled, the direct reclaims stopped, but the number of reclaims performed by kswapd increased. This would explain the high pgscand/s we observed from sar:</p>
<p><img src="/images/951413iMgBlog/app0610-pagescans.png" alt="img"></p>
<p>we observed that the number of expensive memory accesses in the program dropped significantly once we disabled zone reclaim mode. The graph below shows the memory access latency in milliseconds, along with the amount of time spent in system and user CPU. We see that the program spent most of its time waiting for I/O while occasionally being blocked in system CPU.</p>
<p><img src="/images/951413iMgBlog/usr-sys-elapsed-times-app0610.png" alt="img"></p>
<h4 id="How-zone-reclaim-impacts-read-performance"><a href="#How-zone-reclaim-impacts-read-performance" class="headerlink" title="How zone reclaim impacts read performance"></a>How zone reclaim impacts read performance</h4><p>Based on the evidence above, it seems that the direct reclaim path triggered by zone reclaim is too aggressive in removing pages from the active list and adding them to the inactive list. In particular, with zone reclaim enabled, active pages seem to wind up on the inactive list, and then are subsequently paged out. Consequently, reads suffer a higher rate of major faults, and hence are more expensive.</p>
<h3 id="NUMA的“七宗罪”"><a href="#NUMA的“七宗罪”" class="headerlink" title="NUMA的“七宗罪”"></a><a href="http://cenalulu.github.io/linux/numa/" target="_blank" rel="noopener">NUMA的“七宗罪”</a></h3><p>几乎所有的运维都会多多少少被NUMA坑害过，让我们看看究竟有多少种在NUMA上栽的方式：</p>
<ul>
<li><a href="http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="noopener">MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture</a></li>
<li><a href="http://frosty-postgres.blogspot.com/2012/08/postgresql-numa-and-zone-reclaim-mode.html" target="_blank" rel="noopener">PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux</a></li>
<li><a href="http://blog.yannickjaquier.com/hpux/non-uniform-memory-access-numa-architecture-with-oracle-database-by-examples.html" target="_blank" rel="noopener">Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples</a></li>
<li><a href="http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases" target="_blank" rel="noopener">Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases</a></li>
</ul>
<p>究其原因几乎都和：“因为CPU亲和策略导致的内存分配不平均”及“NUMA Zone Claim内存回收”有关，而和数据库种类并没有直接联系。所以下文我们就拿MySQL为例，来看看重内存操作应用在NUMA架构下到底会出现什么问题。</p>
<p>MySQL “swap insanity” problem and the effects of the NUMA architecture 总结：</p>
<ul>
<li>CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核CPU通过一条总线共享内存成为瓶颈</li>
<li>于是NUMA出现了，CPU平均划分为若干个Chip（不多于4个），每个Chip有自己的内存控制器及内存插槽</li>
<li>CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存（下文称Remote Access）的速度相较慢三倍左右</li>
<li>于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中</li>
<li>由于内存页没有动态调整策略，使得大部分内存页都集中在<code>CPU 0</code>上</li>
<li>又因为<code>Reclaim</code>默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出</li>
<li>当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现了</li>
</ul>
<p>怎么解决这个问题，文章给出的方案：</p>
<ul>
<li><code>numactl --interleave=all</code></li>
<li>在MySQL进程启动前，使用<code>sysctl -q -w vm.drop_caches=3</code>清空文件缓存所占用的空间</li>
<li>Innodb在启动时，就完成整个<code>Innodb_buffer_pool_size</code>的内存分配</li>
</ul>
<p>或者另外几个进阶方案</p>
<ul>
<li>配置<code>vm.zone_reclaim_mode = 0</code>使得内存不足时<code>去remote memory分配</code>优先于<code>swap out local page</code>   //默认配置</li>
<li><code>echo -15 &gt; /proc/&lt;pid_of_mysqld&gt;/oom_adj</code>调低MySQL进程被<code>OOM_killer</code>强制Kill的可能</li>
<li><a href="http://dev.mysql.com/doc/refman/5.6/en/server-options.html#option_mysqld_memlock" target="_blank" rel="noopener">memlock</a></li>
<li>对MySQL使用Huge Page（黑魔法，<strong>巧用了Huge Page不会被swap的特性</strong>）</li>
</ul>
<h3 id="NUMA-Interleave真的好么？"><a href="#NUMA-Interleave真的好么？" class="headerlink" title="NUMA Interleave真的好么？"></a>NUMA Interleave真的好么？</h3><p><strong>为什么<code>Interleave</code>的策略就解决了问题？</strong> 借用两张 <a href="https://www.cs.sfu.ca/~fedorova/papers/asplos284-dashti.pdf" target="_blank" rel="noopener">Carrefour性能测试</a> 的结果图，可以看到几乎所有情况下<code>Interleave</code>模式下的程序性能都要比默认的亲和模式要高，有时甚至能高达30%。究其根本原因是Linux服务器的大多数workload分布都是随机的：即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分布的。而<code>Interleave</code>模式就恰恰是针对这种特性将内存page随机打散到各个CPU Core上，使得每个CPU的负载和<code>Remote Access</code>的出现频率都均匀分布。相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多。 </p>
<p><img src="/images/951413iMgBlog/perf1.png" alt="perf1"> <img src="/images/951413iMgBlog/perf2.png" alt="perf2"></p>
<p>也就是说，像MySQL这种外部请求随机性强，各个线程访问内存在地址上平均分布的这种应用，<code>Interleave</code>的内存分配模式相较默认模式可以带来一定程度的性能提升。 此外 <a href="https://www.cs.sfu.ca/~fedorova/papers/asplos284-dashti.pdf" target="_blank" rel="noopener">各种</a> <a href="http://www.lst.inf.ethz.ch/people/alumni/zmajo/publications/11-systor.pdf" target="_blank" rel="noopener">论文</a> 中也都通过实验证实，真正造成程序在NUMA系统上性能瓶颈的并不是<code>Remote Acess</code>带来的响应时间损耗，而是内存的不合理分布导致<code>Remote Access</code>将inter-connect这个小水管塞满所造成的结果。而<code>Interleave</code>恰好，把这种不合理分布情况下的Remote Access请求平均分布在了各个小水管中。所以这也是<code>Interleave</code>效果奇佳的一个原因。</p>
<p>这是说Numa默认优先本地分配后，会导致某个NUMA内存使用过高，进而使得NUMA到这些内存访问频率高，最终导致内存访问带宽不够。</p>
<h3 id="centos-NUMA-内存分配策略"><a href="#centos-NUMA-内存分配策略" class="headerlink" title="centos NUMA 内存分配策略"></a><a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-allocation_policy" target="_blank" rel="noopener">centos NUMA 内存分配策略</a></h3><p>以下三种策略定义了系统中节点对内存的分配：</p>
<ul>
<li><p><em><code>Strict</code></em></p>
<p>目标节点中不能分配内存时，分配将被默认操作转进至其他节点。严格的策略意味着，当目标节点中不能分配内存时，分配将会失效。</p>
</li>
<li><p><em><code>Interleave</code></em></p>
<p>内存页面将被分配至一项节点掩码指定的节点，但将以轮循机制的方式分布。</p>
</li>
<li><p><em><code>Preferred</code></em></p>
<p>内存将从单一最优内存节点分配。如果内存并不充足，内存可以从其他节点分配。</p>
</li>
</ul>
<h3 id="NUMA下内存分布查看"><a href="#NUMA下内存分布查看" class="headerlink" title="NUMA下内存分布查看"></a>NUMA下内存分布查看</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#perl numa-maps-summary.pl &lt; /proc/110609/numa_maps //已经绑核后的DRDS节点,绑在8、9号NUMA上</span><br><span class="line">N0        :          723 (  0.00 GB)</span><br><span class="line">N1        :          534 (  0.00 GB)</span><br><span class="line">N10       :          346 (  0.00 GB)</span><br><span class="line">N11       :       317175 (  1.21 GB)</span><br><span class="line">N12       :       394490 (  1.50 GB)</span><br><span class="line">N13       :       740577 (  2.83 GB)</span><br><span class="line">N14       :         1528 (  0.01 GB)</span><br><span class="line">N15       :        12540 (  0.05 GB)</span><br><span class="line">N2        :          185 (  0.00 GB)</span><br><span class="line">N3        :          975 (  0.00 GB)</span><br><span class="line">N4        :          759 (  0.00 GB)</span><br><span class="line">N5        :       203231 (  0.78 GB)</span><br><span class="line">N6        :          143 (  0.00 GB)</span><br><span class="line">N7        :          433 (  0.00 GB)</span><br><span class="line">N8        :      6900962 ( 26.33 GB)</span><br><span class="line">N9        :      6164248 ( 23.51 GB)</span><br><span class="line">active    :          218 (  0.00 GB)</span><br><span class="line">anon      :     14734638 ( 56.21 GB)</span><br><span class="line">dirty     :     14734640 ( 56.21 GB)</span><br><span class="line">kernelpagesize_kB:        17452 (  0.07 GB)</span><br><span class="line">mapmax    :         1347 (  0.01 GB)</span><br><span class="line">mapped    :         4211 (  0.02 GB)</span><br><span class="line"></span><br><span class="line">#perl numa-maps-summary.pl &lt;/proc/61238/numa_maps //同样的物理机上，BIOS中关闭了NUMA，RDS绑了32core</span><br><span class="line">N0        :     24239251 ( 92.47 GB)</span><br><span class="line">active    :         1199 (  0.00 GB)</span><br><span class="line">anon      :     24231673 ( 92.44 GB)</span><br><span class="line">dirty     :     24231673 ( 92.44 GB)</span><br><span class="line">kernelpagesize_kB:        11036 (  0.04 GB)</span><br><span class="line">mapmax    :          608 (  0.00 GB)</span><br><span class="line">mapped    :         7775 (  0.03 GB)</span><br><span class="line">#perl numa-maps-summary.pl &lt;/proc/53400/numa_maps //和61238在同一个物理机上，但是绑定的是另外一个socket，性能好</span><br><span class="line">N0        :     18301536 ( 69.81 GB)</span><br><span class="line">active    :         1171 (  0.00 GB)</span><br><span class="line">anon      :     18293958 ( 69.79 GB)</span><br><span class="line">dirty     :     18293958 ( 69.79 GB)</span><br><span class="line">kernelpagesize_kB:        10988 (  0.04 GB)</span><br><span class="line">mapmax    :          582 (  0.00 GB)</span><br><span class="line">mapped    :         7775 (  0.03 GB)</span><br></pre></td></tr></table></figure>
<p>看起来<strong>飞腾在同一个socket下的跨NUMA内存访问性能跟distence差别不大，是本地访问的80%，同一个socket下的不同NUMA之间的代价基本一样。但是跨socket的远程代价就非常大了，也就是本地的30%左右，抖动还大。</strong></p>
<p>或者<a href="https://www.kernel.org/doc/html/latest/admin-guide/numastat.html?spm=ata.21736010.0.0.65b56bbdVhT9AI" target="_blank" rel="noopener">通过numastat来查看本地内存是否充足</a>:</p>
<p><img src="/images/951413iMgBlog/db243cb4b3fb9d40902e193283aca1aa.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/802b8f4607f1addf17ad24747fda7fb6.png" alt="img"></p>
<h3 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h3><p><a href="https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases" target="_blank" rel="noopener">In a nutshell, Linux maintains a set of 3 lists of pages per NUMA zone</a>: the active list, the inactive list, and the free list. New page allocations move pages from the free list onto the active list. An LRU algorithm moves pages from the active list to the inactive list, and then from the inactive list to the free list. The following is the best place to learn about Linux’s management of the page cache:</p>
<ul>
<li>Mel Gorman: <a href="https://www.kernel.org/doc/gorman/html/understand/" target="_blank" rel="noopener">Understanding the Linux Virtual Memory Manager</a>, chapter 13: <a href="https://www.kernel.org/doc/gorman/html/understand/understand013.html" target="_blank" rel="noopener">Page Frame Reclamation</a>.</li>
</ul>
<h4 id="Linux内存分配模式-mode"><a href="#Linux内存分配模式-mode" class="headerlink" title="Linux内存分配模式(mode)"></a>Linux内存分配模式(mode)</h4><ul>
<li>NODE LOCAL （系统默认）<br>在当前代码执行的地方分配内存</li>
<li>Interleave<br>第一页分配在node 0,下一页在node 1，再下一页node 0，如此轮换。适合被多个线程访问的数据</li>
</ul>
<p>进程的Numa分配可通过<code>/proc/&lt;pid&gt;/numa_maps</code>查看，单个node分配查看<code>/sys/devices/system/node/node&lt;X&gt;/meminfo</code></p>
<h4 id="关于NUMA内存回收"><a href="#关于NUMA内存回收" class="headerlink" title="关于NUMA内存回收"></a>关于NUMA内存回收</h4><p>当内存很低时，内核会考虑回收pages。具体回收哪些页取决于page类别，如mlock的page就不能回收。对于NUMA内存来讲，不同node剩余的内存很有可能不一样，当当前node不能分配所需内存而远程node可分配时，有两种策略：</p>
<ul>
<li>在local node上做回收<br>但这会增加内核处理开销</li>
<li>从远程分配</li>
</ul>
<p>对于小型的numa系统，如2 nodes，内核默认第二种策略，对于大型numa系统(4 nodes以上)，选择第一种。（这一点未在linux内核上确认，但存在这种可选策略是可以理解的）</p>
<p>通过查看<code>/proc/sys/vm/zone_reclaim</code>可知系统是否支持回收，它的值可以在启动时测量NUMA nodes之间的距离得到；如果被enabled，内核会尽可能少地执行此过程，且默认只回收<strong>unmapped page-cache pages</strong>,用户还可以通过<code>/proc/sys/vm/min_unmapped_ratio</code>来设置最小的回收比例来进一步限制reclaim频次；当然，也可以设置地更加aggressive ，比如允许剔除dirty pages, annoymous pages</p>
<h2 id="lmbench-测试"><a href="#lmbench-测试" class="headerlink" title="lmbench 测试"></a>lmbench 测试</h2><p><a href="https://topic.atatech.org/articles/175032" target="_blank" rel="noopener">内存测试命令，以及华为海思arm处理器：­Hisilicon Hi1620， 海光处理器Hygon C86 7185，Intel处理器8163性能比较</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">总带宽：</span><br><span class="line">#taskset -c 0-96 -W 5 -N 5 -M 64M //其中a表示系统中逻辑核总数。</span><br><span class="line"></span><br><span class="line">Die2Die测试命令, 其中j表示遍历Dide的内存节点，nodenum表示die中所包含的逻辑cpu数量。</span><br><span class="line">numactl -C j ./stream -v1 -P $nodenum -W 5 -N 5 -M 64M</span><br><span class="line"></span><br><span class="line">socket2socket测试命令, 其中cpusocket[表示第个中的逻辑数量，j]表示遍历所有socket的内存节点。</span><br><span class="line">numactl -C &#123;cpusocket[i]&#125; -m &#123;memsocket[j]&#125; ./stream -v1 -P &#123;numsocket[i]&#125; -W 5 -N 5 -M 64M</span><br></pre></td></tr></table></figure>
<p>内存延时测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">延时测试命令, 其中cpus为die所在的某个cpu, j表示遍历所有die。</span><br><span class="line">numactl -C j ./lat_mem_rd -P 1 -W 5 -N 5 -t 1024M</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://winddoing.github.io/post/54953.html" target="_blank" rel="noopener">https://winddoing.github.io/post/54953.html</a></p>
<p><a href="https://topic.atatech.org/articles/107682" target="_blank" rel="noopener">内存延时测试</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#./bin/lat_mem_rd 1 16 //1(范围, 单位M) 16(步长)</span><br><span class="line">&quot;stride=16</span><br><span class="line">0.00049 1.540</span><br><span class="line">0.00098 1.540</span><br><span class="line">0.00195 1.540</span><br><span class="line">0.00293 1.540</span><br></pre></td></tr></table></figure>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Linux/" rel="tag"># Linux</a>
          
            <a href="/tags/CPU/" rel="tag"># CPU</a>
          
            <a href="/tags/perf/" rel="tag"># perf</a>
          
            <a href="/tags/IPC/" rel="tag"># IPC</a>
          
            <a href="/tags/arm/" rel="tag"># arm</a>
          
            <a href="/tags/x86/" rel="tag"># x86</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/07/就是要你懂TCP--半连接队列和全连接队列--阿里技术公众号版本/" rel="next" title="就是要你懂TCP--半连接队列和全连接队列">
                <i class="fa fa-chevron-left"></i> 就是要你懂TCP--半连接队列和全连接队列
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/05/24/程序员如何学习和构建网络知识体系/" rel="prev" title="程序员如何学习和构建网络知识体系">
                程序员如何学习和构建网络知识体系 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="weibo @plantegg">
          <p class="site-author-name" itemprop="name">weibo @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">99</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">215</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#cpu相关知识"><span class="nav-number">1.</span> <span class="nav-text">cpu相关知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#指令集"><span class="nav-number">1.1.</span> <span class="nav-text">指令集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RISC特点"><span class="nav-number">1.1.1.</span> <span class="nav-text">RISC特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CISC-Complex-instruction-set-computer"><span class="nav-number">1.1.2.</span> <span class="nav-text">CISC(Complex instruction set computer)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AMD64"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">AMD64</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RISC和CISC比较"><span class="nav-number">1.1.3.</span> <span class="nav-text">RISC和CISC比较</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#能耗"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">能耗</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RISC-V指令架构，开源"><span class="nav-number">1.1.4.</span> <span class="nav-text">RISC-V指令架构，开源</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ARM"><span class="nav-number">1.2.</span> <span class="nav-text">ARM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ARM-芯片厂家"><span class="nav-number">1.2.1.</span> <span class="nav-text">ARM 芯片厂家</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#X86"><span class="nav-number">1.3.</span> <span class="nav-text">X86</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#numa下："><span class="nav-number">1.3.1.</span> <span class="nav-text">numa下：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不同CPU的一些性能数据对比"><span class="nav-number">1.4.</span> <span class="nav-text">不同CPU的一些性能数据对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#飞腾ARM芯片案例"><span class="nav-number">1.5.</span> <span class="nav-text">飞腾ARM芯片案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#点查场景压测16个core的节点"><span class="nav-number">1.5.1.</span> <span class="nav-text">点查场景压测16个core的节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#点查场景压测8个core的节点"><span class="nav-number">1.5.2.</span> <span class="nav-text">点查场景压测8个core的节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDS-数据库场景绑核"><span class="nav-number">1.5.3.</span> <span class="nav-text">RDS 数据库场景绑核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网卡队列调整"><span class="nav-number">1.5.4.</span> <span class="nav-text">网卡队列调整</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看网卡和numa的关系"><span class="nav-number">1.5.5.</span> <span class="nav-text">查看网卡和numa的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存和cache的latency对比"><span class="nav-number">1.6.</span> <span class="nav-text">内存和cache的latency对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NUMA"><span class="nav-number">1.7.</span> <span class="nav-text">NUMA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-what-triggers-zone-reclaim"><span class="nav-number">1.7.1.</span> <span class="nav-text">Understanding what triggers zone reclaim</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Characteristics-of-systems-experiencing-zone-reclaims"><span class="nav-number">1.7.1.1.</span> <span class="nav-text">Characteristics of systems experiencing zone reclaims</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-zone-reclaim-impacts-read-performance"><span class="nav-number">1.7.1.2.</span> <span class="nav-text">How zone reclaim impacts read performance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NUMA的“七宗罪”"><span class="nav-number">1.7.2.</span> <span class="nav-text">NUMA的“七宗罪”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NUMA-Interleave真的好么？"><span class="nav-number">1.7.3.</span> <span class="nav-text">NUMA Interleave真的好么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#centos-NUMA-内存分配策略"><span class="nav-number">1.7.4.</span> <span class="nav-text">centos NUMA 内存分配策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NUMA下内存分布查看"><span class="nav-number">1.7.5.</span> <span class="nav-text">NUMA下内存分布查看</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内存分配"><span class="nav-number">1.7.6.</span> <span class="nav-text">内存分配</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linux内存分配模式-mode"><span class="nav-number">1.7.6.1.</span> <span class="nav-text">Linux内存分配模式(mode)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#关于NUMA内存回收"><span class="nav-number">1.7.6.2.</span> <span class="nav-text">关于NUMA内存回收</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lmbench-测试"><span class="nav-number">1.8.</span> <span class="nav-text">lmbench 测试</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">weibo @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2020/05/11/cpu相关知识/';
          this.page.identifier = '2020/05/11/cpu相关知识/';
          this.page.title = 'cpu相关知识';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
