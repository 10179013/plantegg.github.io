<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/3/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/page/3/"/>





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/05/16/Perf_IPC以及CPU利用率/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/16/Perf_IPC以及CPU利用率/" itemprop="url">Perf IPC以及CPU性能</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-16T12:30:03+08:00">
                2021-05-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Perf-IPC以及CPU性能"><a href="#Perf-IPC以及CPU性能" class="headerlink" title="Perf IPC以及CPU性能"></a>Perf IPC以及CPU性能</h1><p>为了让程序能快点，特意了解了CPU的各种原理，比如多核、超线程、NUMA、睿频、功耗、GPU、大小核再到分支预测、cache_line失效、加锁代价、IPC等各种指标（都有对应的代码和测试数据）都会在这系列文章中得到答案。当然一定会有程序员最关心的分支预测案例、Disruptor无锁案例、cache_line伪共享案例等等。</p>
<p>这次让我们从最底层的沙子开始用8篇文章来回答各种疑问以及大量的实验对比案例和测试数据。</p>
<p>大的方面主要是从这几个疑问来写这些文章：</p>
<ul>
<li>同样程序为什么CPU跑到800%还不如CPU跑到200%快？</li>
<li>IPC背后的原理和和程序效率的关系？</li>
<li>为什么数据库领域都爱把NUMA关了，这对吗？</li>
<li>几个国产芯片的性能到底怎么样？</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU的制造和概念/">CPU的制造和概念</a></p>
<p><a href="/2021/05/16/Perf IPC以及CPU利用率/">Perf IPC以及CPU性能</a></p>
<p><a href="https://plantegg.github.io/2021/07/19/CPU性能和CACHE/">CPU性能和CACHE</a></p>
<p><a href="/2021/05/16/CPU Cache Line 和性能/">CPU 性能和Cache Line</a></p>
<p><a href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<p><a href="/2021/06/18/几款CPU性能对比/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/03/07/一次海光物理机资源竞争压测的记录/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2021/05/15/飞腾ARM芯片-FT2500的性能测试/">飞腾ARM芯片(FT2500)的性能测试</a></p>
<p><img src="/images/951413iMgBlog/image-20210802161455950.png" alt="image-20210802161455950"></p>
<h2 id="程序性能"><a href="#程序性能" class="headerlink" title="程序性能"></a>程序性能</h2><blockquote>
<p> 程序的 CPU 执行时间 = 指令数/(主频*IPC)</p>
</blockquote>
<p>IPC: insns per cycle  insn/cycles</p>
<h2 id="CPU-流水线工作原理"><a href="#CPU-流水线工作原理" class="headerlink" title="CPU 流水线工作原理"></a>CPU 流水线工作原理</h2><p>cycles：CPU时钟周期。CPU从它的指令集(instruction set)中选择指令执行。</p>
<p>一个指令包含以下的步骤，每个步骤由CPU的一个叫做功能单元(functional unit)的组件来进行处理，每个步骤的执行都至少需要花费一个时钟周期。</p>
<ul>
<li>指令读取(instruction fetch， IF)</li>
<li>指令解码(instruction decode， ID)</li>
<li>执行(execute， EXE)</li>
<li>内存访问(memory access，MEM)</li>
<li>寄存器回写(register write-back， WB)</li>
</ul>
<p><img src="/images/951413iMgBlog/950px-skylake_server_block_diagram.svg.png" alt="skylake server block diagram.svg"></p>
<p>以上结构简化成流水线就是：</p>
<p><img src="/images/951413iMgBlog/image-20210511154816751.png" alt="image-20210511154816751"></p>
<p>IF/ID 就是我们常说的前端，他负责不停地取指和译指，然后为后端提供译指之后的指令，最核心的优化就是要做好<strong>分支预测</strong>，终归取指是要比执行慢，只有提前做好预测才能尽量匹配上后端。后端核心优化是要做好执行单元的并发量，以及乱序执行能力，最终要将乱序执行结果正确组合并输出。</p>
<p>在流水线指令之前是单周期处理器：也就是一个周期完成一条指令。每个时钟周期必须完成取指、译码、读寄存器、 执行、访存等很多组合逻辑工作，为了保证在下一个时钟上升沿到来之前准备好寄存器堆的写数 据，需要将每个时钟周期的间隔拉长，导致处理器的主频无法提高。</p>
<p>使用流水线技术可以提高处 理器的主频。五个步骤只能串行，<strong>但是可以做成pipeline提升效率</strong>，也就是第一个指令做第二步的时候，指令读取单元可以去读取下一个指令了，如果有一个指令慢就会造成stall，也就是pipeline有地方卡壳了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">$sudo perf stat -a -- sleep 10</div><div class="line"></div><div class="line">Performance counter stats for &apos;system wide&apos;:</div><div class="line"></div><div class="line"> 239866.330098      task-clock (msec)         #   23.985 CPUs utilized    /10*1000        (100.00%)</div><div class="line">        45,709      context-switches          #    0.191 K/sec                    (100.00%)</div><div class="line">         1,715      cpu-migrations            #    0.007 K/sec                    (100.00%)</div><div class="line">        79,586      page-faults               #    0.332 K/sec</div><div class="line"> 3,488,525,170      cycles                    #    0.015 GHz                      (83.34%)</div><div class="line"> 9,708,140,897      stalled-cycles-frontend   #  278.29% /cycles frontend cycles idle     (83.34%)</div><div class="line"> 9,314,891,615      stalled-cycles-backend    #  267.02% /cycles backend  cycles idle     (66.68%)</div><div class="line"> 2,292,955,367      instructions              #    0.66  insns per cycle  insn/cycles</div><div class="line">                                              #    4.23  stalled cycles per insn stalled-cycles-frontend/insn (83.34%)</div><div class="line">   447,584,805      branches                  #    1.866 M/sec                    (83.33%)</div><div class="line">     8,470,791      branch-misses             #    1.89% of all branches          (83.33%)</div></pre></td></tr></table></figure>
<p>stalled-cycles，则是指令管道未能按理想状态发挥并行作用，发生停滞的时钟周期。stalled-cycles-frontend指指令读取或解码的指令步骤，而stalled-cycles-backend则是指令执行步骤。第二列中的cycles idle其实意思跟stalled是一样的，由于指令执行停滞了，所以指令管道也就空闲了，千万不要误解为CPU的空闲率。这个数值是由stalled-cycles-frontend或stalled-cycles-backend除以上面的cycles得出的。</p>
<p>另外cpu可以同时有多条pipeline，这就是理论上最大的IPC.</p>
<h3 id="pipeline效率和IPC"><a href="#pipeline效率和IPC" class="headerlink" title="pipeline效率和IPC"></a><a href="https://www.hikunpeng.com/document/detail/zh/kunpenggrf/progtuneg/kunpengprogramming_05_0009.html" target="_blank" rel="external">pipeline效率和IPC</a></h3><p>虽然一个指令需要5个步骤，也就是完全执行完需要5个cycles，这样一个时钟周期最多能执行0.2条指令，IPC就是0.2，显然太低了。</p>
<ul>
<li>非流水线：</li>
</ul>
<p><img src="/images/951413iMgBlog/image-20210511154859711.png" alt="image-20210511154859711"></p>
<p>如果把多个指令的五个步骤用pipeline流水线跑起来，在理想情况下一个时钟周期就能跑完一条指令了，这样IPC就能达到1了。</p>
<p>这种非流水线的方式将一个指令分解成多个步骤后，能提升主频，但是一个指令执行需要的时间基本没变</p>
<ul>
<li>标量流水线, 标量（Scalar）流水计算机是<strong>只有一条指令流水线</strong>的计算机:</li>
</ul>
<p><img src="/images/951413iMgBlog/image-20210511155530477.png" alt="image-20210511155530477"></p>
<p>进一步优化，如果我们加大流水线的条数，让多个指令并行执行，就能得到更高的IPC了，但是这种并行必然会有指令之间的依赖，比如第二个指令依赖第一个的结果，所以多个指令并行更容易出现互相等待(stall).</p>
<p><img src="/images/951413iMgBlog/58c7dc9084fa648f204a6468209ca788.png" alt="img"></p>
<p>在每个时钟周期的开始，指令的部分数据和控制信息都保存在流水线锁存器中，并且该信息形成了下一个流水线的逻辑电路输入。在时钟周期内，信号通过组合逻辑传播，从而在时钟周期结束时及时产生输出，以供下一个pipeline锁存器捕获。</p>
<p>早期的RISC处理器，例如IBM的801原型，MIPS R2000（基于斯坦福MIPS机器）和原始的SPARC（来自Berkeley RISC项目），都实现了一个简单的5阶段流水线，与上面所示的流水线不同（ 额外的阶段是内存访问，在执行后存放结果）。在那个时代，主流的CISC架构80386、68030和VAX处理器使用微码顺序工作（通过RISC进行流水作业比较容易，因为指令都是简单的寄存器到寄存器操作，与x86、68k或VAX不同）。导致的结果，以20 MHz运行的SPARC比以33 MHz运行的386快得多。从那以后，每个处理器都至少在某种程度上进行了流水线处理。</p>
<p><img src="/images/951413iMgBlog/e6d5e70e0cbdc4ba662d79f2306758b6.png" alt="img"></p>
<ul>
<li>超标量流水线：所谓超标量（Superscalar）流 水计算机，是指它<strong>具有两条以上的指令流水线</strong>, 超标流水线数量也就是ALU执行单元的并行度</li>
</ul>
<p><img src="/images/951413iMgBlog/image-20210511155708234.png" alt="image-20210511155708234"></p>
<p>一般而言流水线的超标量不能超过单条流水线的深度</p>
<p>每个功能单元都有独立的管道，甚至可以具有不同的长度。 这使更简单的指令可以更快地完成，从而减少了等待时间。 在各个管道内部之间也有许多旁路，但是为简单起见，这些旁路已被省略。</p>
<p>下图中，处理器可能每个周期执行3条不同的指令，例如，一个整数，一个浮点和一个存储器操作。 甚至可以添加更多的功能单元，以便处理器能够在每个周期执行两个整数指令，或两个浮点指令，或使用任何其他方式。</p>
<p>鲲鹏的流水线结构：</p>
<p><img src="/images/951413iMgBlog/zh-cn_image_0000001237942853.png" alt="zh-cn_image_0000001237942853.png"></p>
<p>三级流水线的执行容易被打断，导致指令执行效率低，后面发展起来的五级指令流水线技术被认为是经典的处理器设置方式，已经在多种RISC处理器中广泛使用，它在三级流水线（取指、译码、执行）的基础上，增加了两级处理，将“执行”动作进一步分解为执行、访存、回写，解决了三级流水线中存储器访问指令在指令执行阶段的延迟问题，但是容易出现寄存器互锁等问题导致流水线中断。鲲鹏920处理器采用八级流水线结构，首先是提取指令，然后通过解码、寄存器重命名和调度阶段。一旦完成调度，指令将无序发射到八个执行管道中的一个，每个执行管道每个周期都可以接受并完成一条指令，最后就是访存和回写操作。</p>
<p><img src="/images/951413iMgBlog/b0f6c495a6794d0a1e9a8ea93d87795b.png" alt="img"></p>
<p>流水线的设计可以实现不间断取指、解码、执行、写回，也可以同时做几条流水线一起取指、解码、执行、写回，也就引出了超标量设计。</p>
<p> 超标量处理器可以在一个时钟周期内执行多个指令。需要注意的是，每个执行单元不是单独的处理器，而是单个CPU内(也可以理解成单core)的执行资源，在上面图中也由体现。</p>
<p>三路超标量四工位流水线的指令/周期，将CPI从1变成0.33，即每周期执行3.33条指令，这样的改进幅度实在是令人着迷的，因此在初期的时候超标量甚至被人们赞美为标量程序的向量式处理。</p>
<p>理想是丰满的，现实却是骨感的，现实中的CPI是不可能都这样的，因为现在的处理器执行不同指令时候的“执行”段的工位并不完全一样，例如整数可能短一些，浮点或者向量和 Load/Store 指令需要长一些(这也是为什么AVX512指令下，CPU会降频的原因，因为一个工位太费时间了，不得不降速,频率快了也没啥用)，加上一些别的因素，实际大部分程序的实际 CPI 都是 1.x 甚至更高。</p>
<p>多发射分发逻辑的复杂性随着发射数量呈现平方和指数的变化。也就是说，5发射处理器的调度逻辑几乎是4发射设计的两倍，其中6发射是4倍，而7发射是8倍，依此类推。</p>
<h3 id="流水线案例"><a href="#流水线案例" class="headerlink" title="流水线案例"></a>流水线案例</h3><p>在Linux Kernel中有大量的 likely/unlikely</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ip 层收到消息后，如果是tcp就调用tcp_v4_rcv作为tcp协议的入口</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">tcp_v4_rcv</span><span class="params">(struct sk_buff *skb)</span></span></div><div class="line">&#123;</div><div class="line">  ...</div><div class="line">	<span class="keyword">if</span> (unlikely(th-&gt;doff &lt; <span class="keyword">sizeof</span>(struct tcphdr) / <span class="number">4</span>))</div><div class="line">		<span class="keyword">goto</span> bad_packet; <span class="comment">//概率很小</span></div><div class="line">	<span class="keyword">if</span> (!pskb_may_pull(skb, th-&gt;doff * <span class="number">4</span>))</div><div class="line">		<span class="keyword">goto</span> discard_it;</div><div class="line">  </div><div class="line"><span class="comment">//file: net/ipv4/tcp_input.c</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">tcp_rcv_established</span><span class="params">(struct sock *sk, ...)</span></span></div><div class="line">&#123;</div><div class="line"> <span class="keyword">if</span> (unlikely(sk-&gt;sk_rx_dst == <span class="literal">NULL</span>))</div><div class="line">  ......</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//file: include/linux/compiler.h</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> likely(x)   __builtin_expect(!!(x),1)</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> unlikely(x) __builtin_expect(!!(x),0)</span></div></pre></td></tr></table></figure>
<p>__builtin_expect 这个指令是 gcc 引入的。该函数作用是允许程序员将最有可能执行的分支告诉编译器，来辅助系统进行分支预测。(参见 <a href="https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html" target="_blank" rel="external">https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html</a>)</p>
<p>它的用法为：__builtin_expect(EXP, N)。意思是：EXP == N的概率很大。那么上面 likely 和 unlikely 这两句的具体含义就是：</p>
<ul>
<li><strong>builtin_expect(!!(x),1) x 为真的可能性更大  //0两次取反还是0，非0两次取反都是1，这样可以适配</strong>builtin_expect(EXP, N)的N，要不N的参数没法传</li>
<li>__builtin_expect(!!(x),0) x 为假的可能性更大</li>
</ul>
<p>当正确地使用了__builtin_expect后，编译器在编译过程中会根据程序员的指令，将可能性更大的代码紧跟着前面的代码，从而减少指令跳转带来的性能上的下降。让L1i中加载的代码尽量有效紧凑</p>
<p>这样可以让 CPU流水线分支预测的时候默认走可能性更大的分支。如果分支预测错误所有流水线都要取消重新计算。</p>
<h3 id="流水线的实际效果"><a href="#流水线的实际效果" class="headerlink" title="流水线的实际效果"></a>流水线的实际效果</h3><p>假如一个15级的流水线，如果处理器要将做无用功的时间限制在 10%，那么它必须在正确预测每个分支的准确率达到 99.3%（因为错误一次，15级流水线都要重置，所以错误会放大15倍，0.7*15=10） 。很少有通用程序能够如此准确地预测分支。</p>
<p>下图是不同场景在英特尔酷睿 i7 基准测试，可以看到有19% 的指令都被浪费了，但能耗的浪费情况更加严重，因为处理器必须利用额外的能量才能在推测失误时恢复原来的状态。这样的度量导致许多人得出结论，架构师需要一种不同的方法来实现性能改进。于是多级流水线不能疯狂增加，这样只能往多核发展。</p>
<p><img src="/images/951413iMgBlog/f4.jpg" alt="f4.jpg"></p>
<h3 id="Deeper-Pipelines深度流水线"><a href="#Deeper-Pipelines深度流水线" class="headerlink" title="Deeper Pipelines深度流水线"></a>Deeper Pipelines深度流水线</h3><p>由于时钟速度受流水线中最长阶段的长度的限制，因此每个级的逻辑门可以再细分，尤其是较长的逻辑门，从而将流水线转换为更深的深度流水线,各阶段的数量长度变小而阶段总数量变多，如下图。</p>
<p><img src="/images/951413iMgBlog/ffdf76ae7c34c3445594657466b1a8fe.png" alt="img"></p>
<p>​    这样整个处理器可以更高的时钟速度运行。当然，每个指令将需要更多的周期来完成（等待时间），但是处理器仍将在每个周期中完成1条指令，这样每秒将有更多的周期，处理器每秒将完成更多的指令。</p>
<p>​    Alpha架构师尤其喜欢这个深度流水线，这也是为什么早期的Alpha拥有非常深的流水线，并且在那个时代以很高的时钟速度运行。 当然还有Intel的NetBurst架构，唯主频论。。</p>
<p>​    如今，现代处理器努力将每个流水线阶段的门延迟数量降低到很少（大约12-25个）。</p>
<p>​    在PowerPC G4e中为7-12，在ARM11和Cortex-A9中为8+，在Athlon中为10-15，在Pentium-Pro/II/III/M中为12+，在Athlon64/Phenom/Fusion-A中为12-17，在Cortex-A8中为13+，在UltraSPARC-III/IV中为14，在Core 2中为14+，在Core i*2中为14-18+，在Core i中为16+，在PowerPC G5中为16-25，在Pentium-4中为20+， 在奔腾4E中为31+。 与RISC相比，x86处理器通常具有更深的流水线，因为它们需要做更多的工作来解码复杂的x86指令。UltraSPARC-T1/T2/T3是深度流水线趋势的例外（UltraSPARC-T1仅6个，T2/T3是8-12，因为其倾向让单核简化的方式来堆叠核数量）。</p>
<p>​    不同架构的CPU流水线的级数（长度）存在很大差异，从几级到几十级不等，流水线级数越多，CPU结构就越复杂，功能也就越强大，同时功耗也会越大。相反地，流水线级数少，CPU结构简单，功耗就会降低很多。下表是一些典型的ARM流水线级别。</p>
<p>例如 Cortex-A15、Sandy Bridge 都分别具备 15 级、14 级流水线，而 Intel NetBurst（Pentium 4）、AMD Bulldozer 都是 20 级流水线，它们的工位数都远超出基本的四（或者五）工位流水线设计。更长的流水线虽然能提高频率，但是代价是耗电更高而且可能会有各种性能惩罚。</p>
<p>ARM指令集以及对应的流水线</p>
<table>
<thead>
<tr>
<th>型号</th>
<th>指令集</th>
<th>流水线</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARM7</td>
<td>ARMv4</td>
<td>3级</td>
</tr>
<tr>
<td>ARM9</td>
<td>ARMv5</td>
<td>5级</td>
</tr>
<tr>
<td>ARM11</td>
<td>ARMv6</td>
<td>8级</td>
</tr>
<tr>
<td>Cortex-A8</td>
<td>ARMv7-A</td>
<td>13级</td>
</tr>
<tr>
<td>鲲鹏920/Cortex-A55</td>
<td>ARMv8</td>
<td>8级</td>
</tr>
</tbody>
</table>
<h3 id="指令延时"><a href="#指令延时" class="headerlink" title="指令延时"></a>指令延时</h3><p>​    考虑一个非流水线机器，具有6个执行阶段，长度分别为50 ns，50 ns，60 ns，60 ns，50 ns和50 ns。</p>
<p>​    -这台机器上的指令等待时间是多少？</p>
<p>​    -执行100条指令需要多少时间？</p>
<p>​    指令等待时间 = 50+50+60+60+50+50= 320 ns<br>​    执行100条指令需 = 100*320 = 32000 ns</p>
<h3 id="对比流水线延时"><a href="#对比流水线延时" class="headerlink" title="对比流水线延时"></a>对比流水线延时</h3><p>​    假设在上面这台机器上引入了流水线技术，但引入流水线技术时，时钟偏移会给每个执行阶段增加5ns的开销。</p>
<p>​    -流水线机器上的指令等待时间是多少？</p>
<p>​    -执行100条指令需要多少时间？</p>
<p>​    这里需要注意的是，在流水线实现中，流水线级的长度必须全部相同，即最慢级的速度加上开销，开销为5ns。</p>
<p>​    流水线级的长度= MAX（非流水线级的长度）+开销= 60 + 5 = 65 ns</p>
<p>​    指令等待时间= 65 ns</p>
<p>​    执行100条指令的时间= 65 <em> 6 </em> 1 + 65 <em> 1 </em> 99 = 390 + 6435 = 6825 ns</p>
<h3 id="从流水线获得加速"><a href="#从流水线获得加速" class="headerlink" title="从流水线获得加速"></a>从流水线获得加速</h3><p>​    加速是没有流水线的平均指令时间与有流水线的平均指令时间之比。（这里不考虑由不同类型的危害引起的任何失速）</p>
<p>​    假设：</p>
<p>​    未流水线的平均指令时间= 320 ns</p>
<p>​    流水线的平均指令时间= 65 ns</p>
<p>​    那么，100条指令的加速= 32000/6825 = 4.69，这种理想情况下效率提升了4.69倍。</p>
<p>每一个功能单元的流水线的长度是不同的。事实上，不同的功能单元的流水线长度本来就不一样。我们平时所说的 14 级流水线，指的通常是进行整数计算指令的流水线长度。如果是浮点数运算，实际的流水线长度则会更长一些。</p>
<p><img src="/images/951413iMgBlog/85f15ec667d09fd2d368822904029b32.jpeg" alt="img"></p>
<h3 id="指令缓存（Instruction-Cache）和数据缓存（Data-Cache）"><a href="#指令缓存（Instruction-Cache）和数据缓存（Data-Cache）" class="headerlink" title="指令缓存（Instruction Cache）和数据缓存（Data Cache）"></a>指令缓存（Instruction Cache）和数据缓存（Data Cache）</h3><p>在第 1 条指令执行到访存（MEM）阶段的时候，流水线里的第 4 条指令，在执行取指令（Fetch）的操作。访存和取指令，都要进行内存数据的读取。我们的内存，只有一个地址译码器的作为地址输入，那就只能在一个时钟周期里面读取一条数据，没办法同时执行第 1 条指令的读取内存数据和第 4 条指令的读取指令代码。</p>
<p><img src="/images/951413iMgBlog/c2a4c0340cb835350ea954cdc520704e.jpeg" alt="img"></p>
<p>把内存拆成两部分的解决方案，在计算机体系结构里叫作哈佛架构（Harvard Architecture），来自哈佛大学设计Mark I 型计算机时候的设计。我们今天使用的 CPU，仍然是冯·诺依曼体系结构的，并没有把内存拆成程序内存和数据内存这两部分。因为如果那样拆的话，对程序指令和数据需要的内存空间，我们就没有办法根据实际的应用去动态分配了。虽然解决了资源冲突的问题，但是也失去了灵活性。</p>
<p><img src="/images/951413iMgBlog/e7508cb409d398380753b292b6df8391.jpeg" alt="img"></p>
<p>在流水线产生依赖的时候必须pipeline stall，也就是让依赖的指令执行NOP。</p>
<h3 id="Intel-X86每个指令需要的cycle"><a href="#Intel-X86每个指令需要的cycle" class="headerlink" title="Intel X86每个指令需要的cycle"></a>Intel X86每个指令需要的cycle</h3><p>Intel xeon</p>
<p><img src="/images/951413iMgBlog/v2-73a5cce599828b6c28f6f29bb310687a_1440w.jpg" alt="img"></p>
<p>不同架构带来IPC变化：</p>
<p><img src="/images/951413iMgBlog/intel-ice-lake-ipc-over-time.jpg" alt="img"></p>
<p>Intel 最新的CPU Ice Lake和其上一代的性能对比数据：</p>
<p><img src="/images/951413iMgBlog/intel-ice-lake-sunny-cove-core-table.jpg" alt="img"></p>
<p>上图最终结果导致了IPC提升了20%，以及整体效率的提升：</p>
<p><img src="/images/951413iMgBlog/Intel-Ice-Lake-improved-perf-per-core-April-2021.png" alt="img"></p>
<h2 id="perf-使用"><a href="#perf-使用" class="headerlink" title="perf 使用"></a>perf 使用</h2><p>主要是通过采集 PMU（Performance Monitoring Unit – CPU内部提供）数据来做性能监控</p>
<p><img src="/images/951413iMgBlog/5edebc74-f8ac-483c-8bcd-24e09abfd06b.png" alt="img"></p>
<p>Perf 是一个包含 22 种子工具的工具集，每个工具分别作为一个子命令。</p>
<p>annotate 命令读取 perf.data 并显示注释过的代码;diff 命令读取两个 perf.data 文件并显示两份剖析信息之间的差异; </p>
<p>evlist 命令列出一个 perf.data 文件的事件名称;</p>
<p>inject 命令过滤以加强事件流，在其中加入额外的信 息;</p>
<p>kmem 命令为跟踪和测量内核中 slab 子系统属性的工具;</p>
<p>kvm 命令为跟踪和测量 kvm 客户机操 作系统的工具;</p>
<p>list 命令列出所有符号事件类型;</p>
<p>lock 命令分析锁事件;</p>
<p>probe 命令定义新的动态跟 踪点;</p>
<p>record 命令运行一个程序，并把剖析信息记录在 perf.data 中;</p>
<p>report 命令读取 perf.data 并显 示剖析信息;</p>
<p>sched 命令为跟踪和测量内核调度器属性的工具;</p>
<p>script 命令读取 perf.data 并显示跟踪 输出;</p>
<p>stat 命令运行一个程序并收集性能计数器统计信息;</p>
<p>timechart 命令为可视化某个负载在某时 间段的系统总体性能的工具;</p>
<p>top 命令为系统剖析工具。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">sudo perf record -g -a -e skb:kfree_skb //perf 记录丢包调用栈 然后sudo perf script 查看 （网络报文被丢弃时会调用该函数kfree_skb）</div><div class="line">perf record -e &apos;skb:consume_skb&apos; -ag  //记录网络消耗</div><div class="line">perf probe --add tcp_sendmsg //增加监听probe  perf record -e probe:tcp_sendmsg -aR sleep 1</div><div class="line">sudo perf sched record -- sleep 1 //记录cpu调度的延时</div><div class="line">sudo perf sched latency //查看</div><div class="line"></div><div class="line">perf sched latency --sort max //查看上一步记录的结果，以调度延迟排序。</div><div class="line"></div><div class="line">perf record --call-graph dwarf</div><div class="line">perf report </div><div class="line">perf report --call-graph -G //反转调用关系</div><div class="line"></div><div class="line"></div><div class="line">展开汇编结果</div><div class="line"></div><div class="line">  占比   行号  指令</div><div class="line">       │      mov    %r13,%rax</div><div class="line">       │      mov    %r8,%rbx</div><div class="line">  0.56 │      mov    %r9,%rcx</div><div class="line">  0.19 │      lock   cmpxchg16b 0x10(%rsi) //加锁占89.53，下一行</div><div class="line"> 89.53 │      sete   %al</div><div class="line">  1.50 │      mov    %al,%r13b</div><div class="line">  0.19 │      mov    $0x1,%al</div><div class="line">       │      test   %r13b,%r13b</div><div class="line">       │    ↓ je     eb</div><div class="line">       │    ↓ jmpq   ef</div><div class="line">       │47:   mov    %r9,(%rsp)</div><div class="line">       </div><div class="line"></div><div class="line">//如下代码的汇编</div><div class="line">void main() &#123;</div><div class="line"></div><div class="line">	while(1) &#123;</div><div class="line">		 __asm__ (&quot;pause\n\t&quot;</div><div class="line">				 &quot;pause\n\t&quot;</div><div class="line">				 &quot;pause\n\t&quot;</div><div class="line">				 &quot;pause\n\t&quot;</div><div class="line">				 &quot;pause&quot;);</div><div class="line">	&#125;</div><div class="line">&#125;       </div><div class="line"></div><div class="line">//每行pause占20%</div><div class="line">       │</div><div class="line">       │    Disassembly of section .text:</div><div class="line">       │</div><div class="line">       │    00000000004004ed &lt;main&gt;:</div><div class="line">       │    main():</div><div class="line">       │      push   %rbp</div><div class="line">       │      mov    %rsp,%rbp</div><div class="line">  0.71 │ 4:   pause</div><div class="line"> 19.35 │      pause</div><div class="line"> 20.20 │      pause</div><div class="line"> 19.81 │      pause</div><div class="line"> 19.88 │      pause</div><div class="line"> 20.04 │    ↑ jmp    4</div></pre></td></tr></table></figure>
<p>网络收包软中断</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">_raw_spin_lock_irqsave  /proc/kcore</div><div class="line">       │    Disassembly of section load2:</div><div class="line">       │</div><div class="line">       │    ffffffff81662b00 &lt;load2+0x662b00&gt;:</div><div class="line">  0.30 │      nop</div><div class="line">       │      push   %rbp</div><div class="line">  0.21 │      mov    %rsp,%rbp</div><div class="line">  0.15 │      push   %rbx</div><div class="line">  0.12 │      pushfq</div><div class="line">  0.57 │      pop    %rax</div><div class="line">  0.45 │      nop</div><div class="line">  0.15 │      mov    %rax,%rbx</div><div class="line">  0.21 │      cli</div><div class="line">  1.20 │      nop</div><div class="line">       │      mov    $0x20000,%edx</div><div class="line">       │      lock   xadd   %edx,(%rdi) //加锁耗时83%</div><div class="line"> 83.42 │      mov    %edx,%ecx</div><div class="line">       │      shr    $0x10,%ecx</div><div class="line">  0.66 │      cmp    %dx,%cx</div><div class="line">       │    ↓ jne    34</div><div class="line">  0.06 │2e:   mov    %rbx,%rax</div><div class="line">       │      pop    %rbx</div><div class="line">       │      pop    %rbp</div><div class="line">  0.57 │    ← retq</div><div class="line">  0.12 │34:   mov    %ecx,%r8d</div><div class="line">  0.03 │      movzwl %cx,%esi</div><div class="line">       │3a:   mov    $0x8000,%eax</div><div class="line">       │    ↓ jmp    4f</div><div class="line">       │      nop</div><div class="line">  0.06 │48:   pause</div><div class="line">  4.67 │      sub    $0x1,%eax</div><div class="line">       │    ↓ je     69</div><div class="line">  0.12 │4f:   movzwl (%rdi),%edx  //慢操作</div><div class="line">  6.73 │      mov    %edx,%ecx</div><div class="line">       │      xor    %r8d,%ecx</div><div class="line">       │      and    $0xfffe,%ecx</div><div class="line">       │    ↑ jne    48</div><div class="line">  0.12 │      movzwl %dx,%esi</div><div class="line">  0.09 │      callq  0xffffffff8165501c</div><div class="line">       │    ↑ jmp    2e</div><div class="line">       │69:   nop</div><div class="line">       │    ↑ jmp    3a</div></pre></td></tr></table></figure>
<p>可以通过perf看到cpu的使用情况：</p>
<pre><code>$sudo perf stat -a -- sleep 10

Performance counter stats for &apos;system wide&apos;:

 239866.330098      task-clock (msec)         #   23.985 CPUs utilized    /10*1000        (100.00%)
        45,709      context-switches          #    0.191 K/sec                    (100.00%)
         1,715      cpu-migrations            #    0.007 K/sec                    (100.00%)
        79,586      page-faults               #    0.332 K/sec
 3,488,525,170      cycles                    #    0.015 GHz                      (83.34%)
 9,708,140,897      stalled-cycles-frontend   #  278.29% /cycles frontend cycles idle     (83.34%)
 9,314,891,615      stalled-cycles-backend    #  267.02% /cycles backend  cycles idle     (66.68%)
 2,292,955,367      instructions              #    0.66  insns per cycle  insn/cycles
                                             #    4.23  stalled cycles per insn stalled-cycles-frontend/insn (83.34%)
   447,584,805      branches                  #    1.866 M/sec                    (83.33%)
     8,470,791      branch-misses             #    1.89% of all branches          (83.33%)
</code></pre><p><img src="/images/oss/f96e50b5f3d0825b68be5b654624f839.png" alt="image.png"></p>
<h2 id="IPC测试"><a href="#IPC测试" class="headerlink" title="IPC测试"></a>IPC测试</h2><p>实际运行的时候增加如下nop到100个以上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">void main() &#123;</div><div class="line">    while(1) &#123;</div><div class="line">         __asm__ (&quot;nop\n\t&quot;</div><div class="line">                 &quot;nop\n\t&quot;</div><div class="line">                 &quot;nop&quot;);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果同时运行两个如上测试程序，鲲鹏920运行，每个程序的IPC都是3.99</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">#perf stat -- ./nop.out</div><div class="line">failed to read counter branches</div><div class="line"></div><div class="line"> Performance counter stats for &apos;./nop.out&apos;:</div><div class="line"></div><div class="line">       8826.948260      task-clock (msec)         #    1.000 CPUs utilized</div><div class="line">                 8      context-switches          #    0.001 K/sec</div><div class="line">                 0      cpu-migrations            #    0.000 K/sec</div><div class="line">                37      page-faults               #    0.004 K/sec</div><div class="line">    22,949,862,030      cycles                    #    2.600 GHz</div><div class="line">         2,099,719      stalled-cycles-frontend   #    0.01% frontend cycles idle</div><div class="line">        18,859,839      stalled-cycles-backend    #    0.08% backend  cycles idle</div><div class="line">    91,465,043,922      instructions              #    3.99  insns per cycle</div><div class="line">                                                  #    0.00  stalled cycles per insn</div><div class="line">   &lt;not supported&gt;      branches</div><div class="line">            33,262      branch-misses             #    0.00% of all branches</div><div class="line"></div><div class="line">       8.827886000 seconds time elapsed</div></pre></td></tr></table></figure>
<p>intel X86 8260</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#perf stat -- ./nop.out</div><div class="line"></div><div class="line"> Performance counter stats for &apos;./nop.out&apos;:</div><div class="line"></div><div class="line">      65061.160345      task-clock (msec)         #    1.001 CPUs utilized</div><div class="line">                46      context-switches          #    0.001 K/sec</div><div class="line">                92      cpu-migrations            #    0.001 K/sec</div><div class="line">               108      page-faults               #    0.002 K/sec</div><div class="line">   155,659,827,263      cycles                    #    2.393 GHz</div><div class="line">   &lt;not supported&gt;      stalled-cycles-frontend</div><div class="line">   &lt;not supported&gt;      stalled-cycles-backend</div><div class="line">   603,247,401,995      instructions              #    3.88  insns per cycle</div><div class="line">     4,742,051,659      branches                  #   72.886 M/sec</div><div class="line">         1,799,428      branch-misses             #    0.04% of all branches</div><div class="line"></div><div class="line">      65.012821629 seconds time elapsed</div></pre></td></tr></table></figure>
<p>这两块CPU理论IPC最大值都是4，实际x86离理论值更远一些. 增加while循环中的nop数量（从132增加到432个）IPC能提升到3.92</p>
<h2 id="IPC和超线程"><a href="#IPC和超线程" class="headerlink" title="IPC和超线程"></a>IPC和超线程</h2><p>ipc是指每个core的IPC</p>
<h3 id="超线程-Hyper-Threading-原理"><a href="#超线程-Hyper-Threading-原理" class="headerlink" title="超线程(Hyper-Threading)原理"></a><a href="https://www.intel.com/content/www/us/en/gaming/resources/hyper-threading.html" target="_blank" rel="external">超线程(Hyper-Threading)原理</a></h3><p><strong>概念</strong>：一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程，intel体系下也叫做 simultaneous multi-threading（<a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading" target="_blank" rel="external">SMT–wiki用的是simultaneous</a>，<a href="https://akkadia.org/drepper/cpumemory.pdf" target="_blank" rel="external">也有人用 symmetric</a>（29页），我觉得symmetric也比较能表达超线程的意思）。</p>
<blockquote>
<p>Two logical cores can work through tasks more efficiently than a traditional single-threaded core. By taking advantage of idle time when the core would formerly be waiting for other tasks to complete, Intel® Hyper-Threading Technology improves CPU throughput (by up to 30% in server applications).</p>
</blockquote>
<p>超线程技术主要的出发点是，当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器。<strong>pipeline进入stalled状态就可以切到其它超线程上</strong></p>
<p>举个例子，如果一个线程运行过程中，必须要等到一些数据加载到缓存中以后才能继续执行，此时 CPU 就可以切换到另一个线程，去执行其他指令，而不用去处于空闲状态，等待当前线程的数据加载完毕。<strong>通常，一个传统的处理器在线程之间切换，可能需要几万个时钟周期。而一个具有 HT 超线程技术的处理器只需要 1 个时钟周期。因此就大大减小了线程之间切换的成本，从而最大限度地让处理器满负荷运转。</strong></p>
<blockquote>
<p>ARM芯片基本不做超线程，另外请思考为什么有了应用层的多线程切换还需要CPU层面的超线程？</p>
</blockquote>
<p><strong>超线程(Hyper-Threading)物理实现</strong>: 在CPU内部增加寄存器等硬件设施，但是ALU、译码器等关键单元还是共享。在一个物理 CPU 核心内部，会有双份的 PC 寄存器、指令寄存器乃至条件码寄存器。超线程的目的，是在一个线程 A 的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU 的译码器和 ALU 就空出来了，那么另外一个线程 B，就可以拿来干自己需要的事情。这个线程 B 可没有对于线程 A 里面指令的关联和依赖。</p>
<p>CPU超线程设计过程中会引入5%的硬件，但是有30%的提升（经验值，场景不一样效果不一样，阿里的OB/MySQL/ODPS业务经验是提升35%），这是引入超线程的理论基础。如果是一个core 4个HT的话提升会是 50%</p>
<h3 id="超线程如何查看"><a href="#超线程如何查看" class="headerlink" title="超线程如何查看"></a>超线程如何查看</h3><p>如果physical id和core id都一样的话，说明这两个core实际是一个物理core，其中一个是HT。</p>
<p><img src="/images/951413iMgBlog/191276e2a1a1731969da748f1690bc9b.png" alt="image.png"></p>
<p>physical id对应socket，也就是物理上购买到的一块CPU； core id对应着每个物理CPU里面的一个物理core，同一个phyiscal id下core id一样说明开了HT</p>
<h3 id="IPC和超线程的关系"><a href="#IPC和超线程的关系" class="headerlink" title="IPC和超线程的关系"></a>IPC和超线程的关系</h3><p>IPC 和一个core上运行多少个进程没有关系。实际测试将两个运行nop指令的进程绑定到一个core上，IPC不变, 因为IPC就是从core里面取到的，不针对具体进程。但是如果是这两个进程绑定到一个物理core以及对应的超线程core上那么IPC就会减半。如果程序是IO bound（比如需要频繁读写内存）首先IPC远远低于理论值4的，这个时候超线程同时工作的话IPC基本能翻倍</p>
<p><img src="/images/951413iMgBlog/image-20210513123233344.png" alt="image-20210513123233344"></p>
<p>对应的CPU使用率, 两个进程的CPU使用率是200%，实际产出IPC是2.1+1.64=3.75，比单个进程的IPC为3.92小多了。而单个进程CPU使用率才100%</p>
<p><img src="/images/951413iMgBlog/image-20210513130252565.png" alt="image-20210513130252565"></p>
<p>以上测试CPU为Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz (Thread(s) per core:    2)</p>
<h3 id="Intel和AMD单核以及HT性能比较"><a href="#Intel和AMD单核以及HT性能比较" class="headerlink" title="Intel和AMD单核以及HT性能比较"></a>Intel和AMD单核以及HT性能比较</h3><p>测试命令，这个测试命令无论在哪个CPU下，用2个物理核用时都是一个物理核的一半，所以这个计算是可以完全并行的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">taskset -c 1,53 /usr/bin/sysbench --num-threads=2 --test=cpu --cpu-max-prime=50000 run //单核用一个threads，绑核， HT用2个threads，绑一对HT</div></pre></td></tr></table></figure>
<p>测试结果为耗时，单位秒，Hygon 7280 就是Zen2架构</p>
<table>
<thead>
<tr>
<th style="text-align:left">Family Name</th>
<th style="text-align:left">Intel 8269CY CPU @ 2.50GHz</th>
<th style="text-align:left">Intel E5-2682 v4 @ 2.50GHz</th>
<th style="text-align:left">Hygon 7280 2.1G</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">单核  prime 50000</td>
<td style="text-align:left">83</td>
<td style="text-align:left">109</td>
<td style="text-align:left">89</td>
</tr>
<tr>
<td style="text-align:left">HT  prime 50000</td>
<td style="text-align:left">48</td>
<td style="text-align:left">74</td>
<td style="text-align:left">87</td>
</tr>
</tbody>
</table>
<h2 id="主频和性价比"><a href="#主频和性价比" class="headerlink" title="主频和性价比"></a>主频和性价比</h2><p>拿Intel 在数据中心计算的大核CPU IvyBridge与当时用于 存储系列的小核CPU Avoton（ATOM）, 分别测试阿里巴巴(Oceanbase ，MySQL, ODPS)的workload，得到性能吞吐如下：</p>
<p>Intel 大小CPU 核心                   阿里 Workload Output(QPS)</p>
<p>Avoton(8 cores) 2.4GHZ                 10K on single core</p>
<p>Ivy Bridge(2650 v2 disable HT) 2.6GHZ      20K on single core</p>
<p>Ivy Bridge(2650 v2 enable HT) 2.4GHZ       25K on single core</p>
<p>Ivy Bridge(2650 v2 enable HT) 2.6GHZ       27K on single core</p>
<ol>
<li>超线程等于将一个大核CPU 分拆成两个小核，Ivy Bridge的数据显示超线程给 Ivy Bridge <strong>1.35倍</strong>(27K/20K) 的提升</li>
<li>现在我们分别评判 两种CPU对应的性能密度 (performance/core die size) ，该数据越大越好，根据我们的计算和测量发现：Avoton(包含L1D, L1I, and L2 per core)大约是 3~4平方毫米，Ivy Bridge (包含L1D, L1I, L2 )大约是12~13平方毫米, L3/core是 6~7平方毫米, 所以 Ivy Bridge 单核心的芯片面积需要18 ~ 20平方毫米。基于上面的数据我们得到的 Avoton core的性能密度为 2.5 (10K/4sqmm)，而Ivy Bridge的性能密度是1.35 (27K/20sqmm)，因此相同的芯片面积下 Avoton 的性能是 Ivy Bridge的 <strong>1.85倍</strong>(2.5/1.35).</li>
<li>从功耗的角度看性能的提升的对比数据，E5-2650v2(Ivy Bridge) 8core TDP 90w， Avoton 8 core TDP 20瓦， 性能/功耗 Avoton 是 10K QPS/20瓦， Ivy Bridge是 27KQPS/90瓦， 因此 相同的功耗下 Avoton是 Ivy Bridge的 <strong>1.75倍</strong>（10K QPS/20）/ （27KQPS/95）</li>
<li>从价格方面再进行比较，E5-2650v2(Ivy Bridge) 8core 官方价格是1107美元， Avoton 8 core官方价格是171美元。性能/价格 Avoton是 10KQPS/171美元，Ivy Bridge 是 27KQPS/1107美元， 因此相同的美元 Avoton的性能是 Ivy Bridge 的<strong>2.3倍（</strong>1 10KQPS/171美元）/ （27KQPS/1107美元）</li>
</ol>
<p>从以上结论可以看到在数据中心的场景下，由于指令数据相关性较高，同时由于内存访问的延迟更多，因此复杂的CPU体系结构并不能获得相应性能提升，该原因导致我们需要的是更多的小核CPU，以此达到高吞吐量的能力，因此2014年我们向Intel提出需要将物理CPU的超线程由 2个升级到4个/8个， 或者直接将用更多的小核CPU增加服务器的吞吐能力，最新数据表明Intel 会在大核CPU中引入4个超线程，和在相同的芯片面积下引入更多的小核CPU。</p>
<p>预测：为了减少数据中心的功耗，我们需要提升单位面积下的计算密度，因此将来会引入Rack Computing的计算模式，每台服务器将会有4～5百个CPU core，如果使用4个CPU socket，每台机器将会达到～1000个CPU core，结合Compute Express Link (CXL), 一个机架内允许16台服务器情况下，可以引入共享内存，那么一个进程可以运行在上万个CPU core中，这样复杂环境下，我们需要对于这样的软件环境做出更多的布局和优化。</p>
<h2 id="perf-top-和-pause-的案例"><a href="#perf-top-和-pause-的案例" class="headerlink" title="perf top 和 pause 的案例"></a><a href="https://topic.atatech.org/articles/85549" target="_blank" rel="external">perf top 和 pause 的案例</a></h2><p>在Skylake的架构中，将pause由10个时钟周期增加到了140个时钟周期。主要用在spin lock当中因为spin loop 多线程竞争差生的内存乱序而引起的性能下降。pause的时钟周期高过了绝大多数的指令cpu cycles，那么当我们利用perf top统计cpu 性能的时候，pause会有什么影响呢？我们可以利用一段小程序来测试一下.</p>
<p>测试机器：<br>CPU: Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz * 2, 共96个超线程</p>
<p>案例：</p>
<p><img src="/images/oss/864427c491497acb02d37c02cb35eeb2.png" alt="image.png"></p>
<p>对如上两个pause指令以及一个 count++（addq），进行perf top：</p>
<p><img src="/images/oss/40945b005eb9f716e429fd30be55b6d1.png" alt="image.png"></p>
<p>可以看到第一个pasue在perf top中cycles为0，第二个为46.85%，另外一个addq也有48.83%，基本可以猜测perf top在这里数据都往后挪了一个。</p>
<p><strong>问题总结：</strong><br> 我们知道perf top是通过读取PMU的PC寄存器来获取当前执行的指令进而根据汇编的symbol信息获得是执行的哪条指令。所以看起来CPU在执行pause指令的时候，从PMU中看到的PC值指向到了下一条指令，进而导致我们看到的这个现象。通过查阅《Intel® 64 and IA-32 Architectures Optimization Reference Manual》目前还无法得知这是CPU的一个设计缺陷还是PMU的一个bug(需要对pause指令做特殊处理)。<strong>不管怎样，这个实验证明了我们统计spin lock的CPU占比还是准确的，不会因为pause指令导致PMU采样出错导致统计信息的整体失真。只是对于指令级的CPU统计，我们能确定的就是它把pause的执行cycles 数统计到了下一条指令。</strong></p>
<p><strong>补充说明：</strong> <strong>经过测试，非skylake CPU也同样存在perf top会把pause(执行数cycles是10)的执行cycles数统计到下一条指令的问题，看来这是X86架构都存在的问题。</strong></p>
<h2 id="perf-和火焰图"><a href="#perf-和火焰图" class="headerlink" title="perf 和火焰图"></a>perf 和火焰图</h2><p>调用 perf record 采样几秒钟，一般需要加 -g 参数，也就是 call-graph，还需要抓取函数的调用关系。在多核的机器上，还要记得加上 -a 参数，保证获取所有 CPU Core 上的函数运行情况。至于采样数据的多少，在讲解 perf 概念的时候说过，我们可以用 -c 或者 -F 参数来控制。</p>
<pre><code>   83  07/08/19 13:56:26 sudo perf record -ag -p 4759
   84  07/08/19 13:56:50 ls /tmp/
   85  07/08/19 13:57:06 history |tail -16
   86  07/08/19 13:57:20 sudo chmod 777 perf.data
   87  07/08/19 13:57:33 perf script &gt;out.perf
   88  07/08/19 13:59:24 ~/tools/FlameGraph-master/./stackcollapse-perf.pl ~/out.perf &gt;out.folded
   89  07/08/19 14:01:01 ~/tools/FlameGraph-master/flamegraph.pl out.folded &gt; kernel-perf.svg
   90  07/08/19 14:01:07 ls -lh
   91  07/08/19 14:03:33 history


$ sudo perf record -F 99 -a -g -- sleep 60 //-F 99 指采样每秒钟做 99 次
</code></pre><p>　　执行这个命令将生成一个 perf.data 文件：</p>
<p>执行sudo perf report -n可以生成报告的预览。<br>执行sudo perf report -n –stdio可以生成一个详细的报告。<br>执行sudo perf script可以 dump 出 perf.data 的内容。</p>
<pre><code># 折叠调用栈
$ FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded
# 生成火焰图
$ FlameGraph/flamegraph.pl out.folded &gt; out.svg
</code></pre><h2 id="ECS和perf"><a href="#ECS和perf" class="headerlink" title="ECS和perf"></a>ECS和perf</h2><p>在ECS会采集不到 cycles等，cpu-clock、page-faults都是内核中的软事件，cycles/instructions得采集cpu的PMU数据，ECS采集不到这些PMU数据。</p>
<p><img src="/images/oss/a120388ff72d712a4fd176e7cea005cf.png" alt="image.png"></p>
<h2 id="Perf-和-false-share-cache-line"><a href="#Perf-和-false-share-cache-line" class="headerlink" title="Perf 和 false share cache_line"></a>Perf 和 false share cache_line</h2><p><a href="https://joemario.github.io/blog/2016/09/01/c2c-blog/" target="_blank" rel="external">从4.2kernel开始，perf支持perf c2c (cache 2 cahce) 来监控cache_line的伪共享</a></p>
<h2 id="系列文章-1"><a href="#系列文章-1" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU的制造和概念/">CPU的制造和概念</a></p>
<p><a href="/2021/05/16/CPU Cache Line 和性能/">CPU 性能和Cache Line</a></p>
<p><a href="/2021/05/16/Perf IPC以及CPU利用率/">Perf IPC以及CPU性能</a></p>
<p><a href="/2021/06/18/几款CPU性能对比/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/05/15/飞腾ARM芯片(FT2500">飞腾ARM芯片(FT2500)的性能测试</a>的性能测试/)</p>
<p><a href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2021/03/07/一次海光物理机资源竞争压测的记录/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhengheng.me/2015/11/12/perf-stat/" target="_blank" rel="external">perf详解</a></p>
<p><a href="https://www.atatech.org/articles/109158" target="_blank" rel="external">CPU体系结构</a></p>
<p><a href="https://mp.weixin.qq.com/s/KaDJ1EF5Y-ndjRv2iUO3cA" target="_blank" rel="external">震惊，用了这么多年的 CPU 利用率，其实是错的</a>cpu占用不代表在做事情，可能是stalled，也就是流水线卡顿，但是cpu占用了，实际没事情做。</p>
<p><a href="http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html" target="_blank" rel="external">CPU Utilization is Wrong</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;mid=2247483755&amp;idx=1&amp;sn=5324f7e46c91739b566dfc1d0847fc4a&amp;chksm=f9aa33b2ceddbaa478729383cac89967cc515bafa472001adc4ad42fb37e3ce473eddc3b591a&amp;mpshare=1&amp;scene=1&amp;srcid=0127mp3WJ6Kd1UOQISFg3SIC#rd" target="_blank" rel="external">震惊，用了这么多年的 CPU 利用率，其实是错的</a></p>
<p><a href="https://kernel.taobao.org/2019/03/Top-down-Microarchitecture-Analysis-Method/" target="_blank" rel="external">https://kernel.taobao.org/2019/03/Top-down-Microarchitecture-Analysis-Method/</a></p>
<p> <a href="http://www.akkadia.org/drepper/cpumemory.pdf" target="_blank" rel="external">What Every Programmer Should Know About Main Memory</a> by Ulrich Drepper </p>
<p><a href="https://mazzo.li/posts/fast-pipes.html" target="_blank" rel="external">How fast are Linux pipes anyway?</a> 优化 pipes 的读写带宽，perf、hugepage、splice使用</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/05/15/飞腾ARM芯片(FT2500)的性能测试/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/15/飞腾ARM芯片(FT2500)的性能测试/" itemprop="url">飞腾ARM芯片-FT2500的性能测试</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-15T17:30:03+08:00">
                2021-05-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="飞腾ARM芯片-FT2500的性能测试"><a href="#飞腾ARM芯片-FT2500的性能测试" class="headerlink" title="飞腾ARM芯片-FT2500的性能测试"></a>飞腾ARM芯片-FT2500的性能测试</h1><h2 id="ARM"><a href="#ARM" class="headerlink" title="ARM"></a>ARM</h2><p> ARM公司最早是由赫尔曼·豪泽（Hermann Hauser）和工程师Chris Curry在1978年创立（早期全称是 Acorn RISC Machine），后来改名为现在的ARM公司（Advanced RISC Machine）</p>
<p><img src="/images/951413iMgBlog/ac0bac75ae745316e0c011ffdc5a78a5.png" alt="img"></p>
<h3 id="ARM-芯片厂家"><a href="#ARM-芯片厂家" class="headerlink" title="ARM 芯片厂家"></a>ARM 芯片厂家</h3><p>查看厂家</p>
<blockquote>
<p>#cat /proc/cpuinfo |grep implementer</p>
<p>CPU implementer    : 0x70</p>
<p>#cat /sys/devices/system/cpu/cpu0/regs/identification/midr_el1<br>0x00000000701f6633  // 70 表示厂家</p>
</blockquote>
<p>vendor id对应厂家</p>
<table>
<thead>
<tr>
<th style="text-align:left">Vendor Name</th>
<th style="text-align:left">Vendor ID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ARM</td>
<td style="text-align:left">0x41</td>
</tr>
<tr>
<td style="text-align:left">Broadcom</td>
<td style="text-align:left">0x42</td>
</tr>
<tr>
<td style="text-align:left">Cavium</td>
<td style="text-align:left">0x43</td>
</tr>
<tr>
<td style="text-align:left">DigitalEquipment</td>
<td style="text-align:left">0x44</td>
</tr>
<tr>
<td style="text-align:left">HiSilicon</td>
<td style="text-align:left">0x48</td>
</tr>
<tr>
<td style="text-align:left">Infineon</td>
<td style="text-align:left">0x49</td>
</tr>
<tr>
<td style="text-align:left">Freescale</td>
<td style="text-align:left">0x4D</td>
</tr>
<tr>
<td style="text-align:left">NVIDIA</td>
<td style="text-align:left">0x4E</td>
</tr>
<tr>
<td style="text-align:left">APM</td>
<td style="text-align:left">0x50</td>
</tr>
<tr>
<td style="text-align:left">Qualcomm</td>
<td style="text-align:left">0x51</td>
</tr>
<tr>
<td style="text-align:left">Marvell</td>
<td style="text-align:left">0x56</td>
</tr>
<tr>
<td style="text-align:left">Intel</td>
<td style="text-align:left">0x69</td>
</tr>
<tr>
<td style="text-align:left">飞腾</td>
<td style="text-align:left">0x70</td>
</tr>
</tbody>
</table>
<h2 id="飞腾ARM芯片介绍"><a href="#飞腾ARM芯片介绍" class="headerlink" title="飞腾ARM芯片介绍"></a>飞腾ARM芯片介绍</h2><p><strong>飞腾处理器</strong>，又称<strong>银河飞腾处理器</strong>，是由<a href="https://zh.wikipedia.org/wiki/中國人民解放軍國防科學技術大學" target="_blank" rel="external">中国人民解放军国防科学技术大学</a>研制的一系列嵌入式<a href="https://zh.wikipedia.org/wiki/数字信号处理器" target="_blank" rel="external">数字信号处理器</a>（DSP）和<a href="https://zh.wikipedia.org/wiki/中央处理器" target="_blank" rel="external">中央处理器</a>（CPU）芯片。<a href="https://zh.wikipedia.org/wiki/飞腾处理器#cite_note-cw-1" target="_blank" rel="external">[1]</a>这个处理器系列的研发，是由国防科技大的<a href="https://zh.wikipedia.org/w/index.php?title=邢座程&amp;action=edit&amp;redlink=1" target="_blank" rel="external">邢座程</a>教授<a href="https://zh.wikipedia.org/wiki/飞腾处理器#cite_note-2" target="_blank" rel="external">[2]</a>带领的团队负责研发。<a href="https://zh.wikipedia.org/wiki/飞腾处理器#cite_note-Xing_671-3" target="_blank" rel="external">[3]</a>其<a href="https://zh.wikipedia.org/w/index.php?title=商業化&amp;action=edit&amp;redlink=1" target="_blank" rel="external">商业化</a><a href="https://zh.wikipedia.org/wiki/推廣" target="_blank" rel="external">推广</a>则是由<a href="https://zh.wikipedia.org/wiki/中国电子信息产业集团有限公司" target="_blank" rel="external">中国电子信息产业集团有限公司</a>旗下的天津飞腾信息技术有限公司负责。</p>
<p>飞腾公司在早期，考察了SPARC、MIPS、ALPHA架构，这三种指令集架构都可以以极其低廉的价格（据说SPARC的授权价只有99美元，ALPHA不要钱）获得授权，飞腾选择了SPARC架构进行了CPU的研发。</p>
<p>2012年ARM正式推出了自己的第一个64位指令集处理器架构ARMv8，进入服务器等新的领域。此后飞腾放弃了SPARC，拿了ARMv8指令集架构的授权，全面转向了ARM阵营，芯片roadmap如下：</p>
<p><img src="/images/951413iMgBlog/3407604faa7ca9a87fa26610606081ab.png" alt="img"></p>
<h3 id="测试芯片详细信息"><a href="#测试芯片详细信息" class="headerlink" title="测试芯片详细信息"></a><a href="https://pdf.dfcfw.com/pdf/H3_AP202010201422468889_1.pdf?1603181661000.pdf" target="_blank" rel="external">测试芯片详细信息</a></h3><p>2020 年 7 月 23 日，飞腾发布新一代高可扩展多路服务器芯片腾云 S2500，采用 16nm 工艺， 主频 2.0~2.2Ghz，拥有 64 个 FTC663 内核，片内集成 64MB 三级 Cache，支持 8 个 DDR4-3200 存 储通道，功耗 150W。 </p>
<p>基于 ARM 架构，兼具高可拓展性和低功耗，扩展支持 2 路到 8 路直连。与主流架构 X86 相比， ARM 架构具备低功耗、低发热和低成本的优势，ARM 单核的面积仅为 X86 核的 1/7，同样芯片尺寸下可以继承更多核心数，可以通过增加核心数提高性能，在性能快速提升下，也能保持较低的功耗，符合云计算场景下并行计算上高并发和高效率的要求，也能有效控制服务器的能耗和成本支出。腾云 S2500 增加了 4 个直连接口，总带宽 800Gbps，支持 2 路、4 路和 8 路直连，具备高可 拓展性，可以形成 128 核到 512 核的计算机系统，带动算力提升。</p>
<p>飞腾(FT2500), ARMv8架构，主频2.1G，服务器两路，每路64个物理core，没有超线程，总共16个numa，每个numa 8个core</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div></pre></td><td class="code"><pre><div class="line">#dmidecode -t processor</div><div class="line"># dmidecode 3.0</div><div class="line">Getting SMBIOS data from sysfs.</div><div class="line">SMBIOS 3.2.0 present.</div><div class="line"># SMBIOS implementations newer than version 3.0 are not</div><div class="line"># fully supported by this version of dmidecode.</div><div class="line"></div><div class="line">Handle 0x0004, DMI type 4, 48 bytes</div><div class="line">Processor Information</div><div class="line">    Socket Designation: BGA3576</div><div class="line">    Type: Central Processor</div><div class="line">    Family: &lt;OUT OF SPEC&gt;</div><div class="line">    Manufacturer: PHYTIUM</div><div class="line">    ID: 00 00 00 00 70 1F 66 22</div><div class="line">    Version: FT2500</div><div class="line">    Voltage: 0.8 V</div><div class="line">    External Clock: 50 MHz</div><div class="line">    Max Speed: 2100 MHz</div><div class="line">    Current Speed: 2100 MHz</div><div class="line">    Status: Populated, Enabled</div><div class="line">    Upgrade: Other</div><div class="line">    L1 Cache Handle: 0x0005</div><div class="line">    L2 Cache Handle: 0x0007</div><div class="line">    L3 Cache Handle: 0x0008</div><div class="line">    Serial Number: 1234567</div><div class="line">    Asset Tag: No Asset Tag</div><div class="line">    Part Number: NULL</div><div class="line">    Core Count: 64</div><div class="line">    Core Enabled: 64</div><div class="line">    Thread Count: 64</div><div class="line">    Characteristics:</div><div class="line">        64-bit capable</div><div class="line">        Multi-Core</div><div class="line">        Hardware Thread</div><div class="line">        Execute Protection</div><div class="line">        Enhanced Virtualization</div><div class="line">        Power/Performance Control</div><div class="line"></div><div class="line">#lscpu</div><div class="line">Architecture:          aarch64</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                128</div><div class="line">On-line CPU(s) list:   0-127</div><div class="line">Thread(s) per core:    1</div><div class="line">Core(s) per socket:    64</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          16</div><div class="line">Model:                 3</div><div class="line">BogoMIPS:              100.00</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              2048K</div><div class="line">L3 cache:              65536K</div><div class="line">NUMA node0 CPU(s):     0-7</div><div class="line">NUMA node1 CPU(s):     8-15</div><div class="line">NUMA node2 CPU(s):     16-23</div><div class="line">NUMA node3 CPU(s):     24-31</div><div class="line">NUMA node4 CPU(s):     32-39</div><div class="line">NUMA node5 CPU(s):     40-47</div><div class="line">NUMA node6 CPU(s):     48-55</div><div class="line">NUMA node7 CPU(s):     56-63</div><div class="line">NUMA node8 CPU(s):     64-71</div><div class="line">NUMA node9 CPU(s):     72-79</div><div class="line">NUMA node10 CPU(s):    80-87</div><div class="line">NUMA node11 CPU(s):    88-95</div><div class="line">NUMA node12 CPU(s):    96-103</div><div class="line">NUMA node13 CPU(s):    104-111</div><div class="line">NUMA node14 CPU(s):    112-119</div><div class="line">NUMA node15 CPU(s):    120-127</div><div class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</div><div class="line"></div><div class="line">node distances:</div><div class="line">node   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15</div><div class="line">  0:  10  20  40  30  20  30  50  40  100  100  100  100  100  100  100  100</div><div class="line">  1:  20  10  30  40  50  20  40  50  100  100  100  100  100  100  100  100</div><div class="line">  2:  40  30  10  20  40  50  20  30  100  100  100  100  100  100  100  100</div><div class="line">  3:  30  40  20  10  30  20  40  50  100  100  100  100  100  100  100  100</div><div class="line">  4:  20  50  40  30  10  50  30  20  100  100  100  100  100  100  100  100</div><div class="line">  5:  30  20  50  20  50  10  50  40  100  100  100  100  100  100  100  100</div><div class="line">  6:  50  40  20  40  30  50  10  30  100  100  100  100  100  100  100  100</div><div class="line">  7:  40  50  30  50  20  40  30  10  100  100  100  100  100  100  100  100</div><div class="line">  8:  100  100  100  100  100  100  100  100  10  20  40  30  20  30  50  40</div><div class="line">  9:  100  100  100  100  100  100  100  100  20  10  30  40  50  20  40  50</div><div class="line"> 10:  100  100  100  100  100  100  100  100  40  30  10  20  40  50  20  30</div><div class="line"> 11:  100  100  100  100  100  100  100  100  30  40  20  10  30  20  40  50</div><div class="line"> 12:  100  100  100  100  100  100  100  100  20  50  40  30  10  50  30  20</div><div class="line"> 13:  100  100  100  100  100  100  100  100  30  20  50  20  50  10  50  40</div><div class="line"> 14:  100  100  100  100  100  100  100  100  50  40  20  40  30  50  10  30</div><div class="line"> 15:  100  100  100  100  100  100  100  100  40  50  30  50  20  40  30  10</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20210422121346490.png" alt="image-20210422121346490"></p>
<p>cpu详细信息：</p>
<p><img src="/images/oss/e177902c-73b2-4535-9c1f-2726451820db.png" alt="img"></p>
<p>飞腾芯片，按如下distance绑核基本没区别！展示出来的distance是假的一样</p>
<p><img src="/images/oss/5a19ff61-68db-4c65-be4c-6b6c155a8a29.png" alt="img"></p>
<p>FT2500芯片集成的 64 个处理器核心，划分为 8 个 Panel，每个 Panel 中有两个 Cluster (每个 Cluster 包含 4 个处理器核心及共享的 2M 二级 cache)、两个本地目录控 制部件(DCU)、一个片上网络路由器节点(Cell)和一个紧密耦合的访存控制 器(MCU)。Panel 之间通过片上网络接口连接，一致性维护报文、数据报文、 调测试报文、中断报文等统一从同一套网络接口进行路由和通信</p>
<p>一个Panel的实现是FTC663版本，采用四发射乱序超标量流水线结构，兼容 ARMv8 指令集，支持 EL0~EL3 多个特权级。流水线分为取指、译码、分派、执 行和写回五个阶段，采用顺序取指、乱序执行、顺序提交的多发射执行机制，取 值宽度、译码宽度、分派宽度均是 4 条指令，共有 9 个执行部件(或者称为 9 条功能流水线)，分别是 4 个整数部件、2 个浮点部件、1 个 load 部件、1 个 load/store 部件和 1 个系统管理指令执行部件。浮点流水线能够合并执行双路浮点 SIMD 指 令，实现每拍可以执行 4 条双精度浮点操作的峰值性能。</p>
<p><img src="/images/951413iMgBlog/image-20210910120438276.png" alt="image-20210910120438276"></p>
<p>猜测FT2500 64core用的是一个Die, 但是core之间的连接是Ring Bus，而Ring Bus下core太多后延迟会快速增加，所以一个Die 内部做了8个小的Ring Bus，每个Ring Bus下8个core。</p>
<h3 id="飞腾官方提供的测试结果"><a href="#飞腾官方提供的测试结果" class="headerlink" title="飞腾官方提供的测试结果"></a>飞腾官方提供的测试结果</h3><p><img src="/images/951413iMgBlog/image-20210909175954574.png" alt="image-20210909175954574"></p>
<h3 id="飞腾2500-和-鲲鹏9200-参数对比"><a href="#飞腾2500-和-鲲鹏9200-参数对比" class="headerlink" title="飞腾2500 和 鲲鹏9200 参数对比"></a>飞腾2500 和 鲲鹏9200 参数对比</h3><p><img src="/images/951413iMgBlog/image-20210422095217195.png" alt="image-20210422095217195"></p>
<h3 id="FT2000与FT2500差异"><a href="#FT2000与FT2500差异" class="headerlink" title="FT2000与FT2500差异"></a>FT2000与FT2500差异</h3><p>下表是FT2000和FT2500产品规格对比表，和芯片的单核内部结构变化较少，多了L3，主频提高了，其他基本没有变化。</p>
<table>
<thead>
<tr>
<th><strong>特征</strong></th>
<th><strong>FT-2000+/64</strong></th>
<th><strong>FT-2500</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>指令</td>
<td>兼容 ARM V8 指令集 FTC662 内核</td>
<td>兼容 ARM V8 指令集FTC663 内核</td>
</tr>
<tr>
<td>Core数</td>
<td>64个</td>
<td>64个</td>
</tr>
<tr>
<td>频率</td>
<td>2.2GHZ/2.0GHZ/1.8GHZ</td>
<td><strong>2.0~2.3GHz</strong></td>
</tr>
<tr>
<td>体系架构</td>
<td>NUMA</td>
<td>NUMA</td>
</tr>
<tr>
<td>RAS</td>
<td>无</td>
<td>支持</td>
</tr>
<tr>
<td>加解密</td>
<td>无</td>
<td><strong>ASE128、SHA1、SHA2-256、PMULL</strong></td>
</tr>
<tr>
<td>L1 Cache</td>
<td>每个核独占32KB指令Cache与32KB数据Cache</td>
<td>每个核独占32K指令Cache与32K数据Cache</td>
</tr>
<tr>
<td>L2 Cache</td>
<td>共32MB，每4个核共享2MB</td>
<td>共32MB，每4个核共享2MB</td>
</tr>
<tr>
<td>L3 Cache</td>
<td>无</td>
<td><strong>64MB</strong></td>
</tr>
<tr>
<td>LMU数量</td>
<td>8个</td>
<td>8个</td>
</tr>
<tr>
<td>支持最大容量</td>
<td>1TB</td>
<td>1TB*socket数量</td>
</tr>
<tr>
<td>支持最大频率</td>
<td>3200MHZ</td>
<td>支持3200MHZ</td>
</tr>
<tr>
<td>外接设备</td>
<td>支持带 ECC 的 DDR4 DIMM，支持 RDIMM、UDIMM、SODIMM、 LR-DIMM，电压 1.2V</td>
<td>支持带 ECC 的 DDR4 DIMM，支持 RDIMM、UDIMM、SODIMM、LR-DIMM，电压 1.2V</td>
</tr>
<tr>
<td>镜像存储</td>
<td>无</td>
<td>每两个MCU互为备份</td>
</tr>
<tr>
<td>PCIe</td>
<td>PCIE3.02 个 x16 和 1 个 x1每个 x16 可拆分成 2 个 x8，支持翻转</td>
<td>PCIE3.01 个 x16 和 1 个 x1x16 可拆分成 2 个 x8，支持翻转</td>
</tr>
<tr>
<td>SPI</td>
<td>支持 4 个片选，单片最大支持容量为 512MB，电压 1.8V</td>
<td>支持 4 个片选，单片最大支持容量为 512MB，电压 1.8V</td>
</tr>
<tr>
<td>UART</td>
<td>4个 UART，其中 1 个为 9 线全功能串口，3 个为 3 线调试串口</td>
<td>4个 UART，其中 1 个为 9 线全功能串口，3 个为 3 线调试串口</td>
</tr>
<tr>
<td>GPIO</td>
<td>4 个 8 位 GPIO 接口，GPIOA[0:7]，GPIOB[0:7]，GPIOC[0:7]， GPIOD[0:7]</td>
<td>4 个 8 位 GPIO 接口，GPIOA[0:7]，GPIOB[0:7]，GPIOC[0:7]， GPIOD[0:7]</td>
</tr>
<tr>
<td>LPC</td>
<td>1 个 LPC 接口，兼容 Intel Low Pin Count 协议, 电压 1.8V</td>
<td>1 个 LPC 接口，兼容 Intel Low Pin Count 协议, 电压 1.8V</td>
</tr>
<tr>
<td>I2C</td>
<td>2 个 I2C master 控制器</td>
<td>2 个 I2C master /Slave控制器,2个slave控制器</td>
</tr>
<tr>
<td>直连</td>
<td>无</td>
<td>四个直连通路，每路X4个lane，每条lane速率为25Gbps，支持2路、4路、8路</td>
</tr>
</tbody>
</table>
<h2 id="飞腾ARM芯片性能测试数据"><a href="#飞腾ARM芯片性能测试数据" class="headerlink" title="飞腾ARM芯片性能测试数据"></a>飞腾ARM芯片性能测试数据</h2><p>以下测试场景基本都是运行CPU和网络瓶颈的业务逻辑，绑核前IPC只有0.08</p>
<p><img src="/images/oss/16b271c8-5132-4273-a26a-4b35e8f92882.png" alt="img"></p>
<p>绑核后对性能提升非常明显：</p>
<p><img src="/images/oss/4d4fdebb-6146-407e-881d-19170fbfd82b.png" alt="img"></p>
<p>点查场景：</p>
<p><img src="/images/951413iMgBlog/image-20210425092158127.png" alt="image-20210425092158127"></p>
<p>如上是绑48-63号核</p>
<p><img src="/images/951413iMgBlog/image-20210425091727122.png" alt="image-20210425091727122"></p>
<p><img src="/images/951413iMgBlog/image-20210425091557750.png" alt="image-20210425091557750"></p>
<p><img src="/images/951413iMgBlog/image-20210425093630438.png" alt="image-20210425093630438"></p>
<p>绑不同的核性能差异比较大，比如同样绑第一个socket最后16core和绑第二个socket最后16core，第二个socket的最后16core性能要好25-30%—<strong>这是因为网卡软中断，如果将软中断绑定到0-4号cpu后差异基本消失</strong>,因为网卡队列设置的是60，基本跑在前60core上，也就是第一个socket上。</p>
<p>点查场景绑核和不绑核性能能差1倍, 将table分表后，物理rt稳定了(<strong>截图中物理rt下降是因为压力小了</strong>–待证)</p>
<h3 id="点查场景压测16个core的节点"><a href="#点查场景压测16个core的节点" class="headerlink" title="点查场景压测16个core的节点"></a>点查场景压测16个core的节点</h3><p>一个节点16core，16个core绑定到14、15号NUMA上，然后压测</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">#perl numa-maps-summary.pl &lt;/proc/79694/numa_maps //16core</div><div class="line">N0        :         1103 (  0.00 GB)</div><div class="line">N1        :       107368 (  0.41 GB)</div><div class="line">N10       :       144736 (  0.55 GB)</div><div class="line">N11       :        16919 (  0.06 GB)</div><div class="line">N12       :       551987 (  2.11 GB)</div><div class="line">N13       :        59499 (  0.23 GB)</div><div class="line">N14       :      5621573 ( 21.44 GB)  //内存就近分配</div><div class="line">N15       :      6200398 ( 23.65 GB)</div><div class="line">N2        :          700 (  0.00 GB)</div><div class="line">N3        :           89 (  0.00 GB)</div><div class="line">N4        :         5784 (  0.02 GB)</div><div class="line">N5        :           77 (  0.00 GB)</div><div class="line">N6        :          426 (  0.00 GB)</div><div class="line">N7        :          472 (  0.00 GB)</div><div class="line">N8        :          107 (  0.00 GB)</div><div class="line">N9        :         6137 (  0.02 GB)</div><div class="line">active    :           85 (  0.00 GB)</div><div class="line">anon      :     12712675 ( 48.50 GB)</div><div class="line">dirty     :     12712679 ( 48.50 GB)</div><div class="line">kernelpagesize_kB:        17444 (  0.07 GB)</div><div class="line">mapmax    :         1598 (  0.01 GB)</div><div class="line">mapped    :         4742 (  0.02 GB)</div><div class="line"></div><div class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses -a -p 79694</div><div class="line">^C</div><div class="line"> Performance counter stats for process id &apos;79694&apos;:</div><div class="line"></div><div class="line">        1719788217      branch-misses                                                 (39.70%)</div><div class="line">      311069393237      bus-cycles                                                    (38.07%)</div><div class="line">        2021349865      cache-misses              #    6.669 % of all cache refs      (38.32%)</div><div class="line">       30308501243      cache-references                                              (39.67%)</div><div class="line">      310980728138      cpu-cycles                                                    (46.46%)</div><div class="line">       67298903097      instructions              #    0.22  insns per cycle          (47.63%)</div><div class="line">        1983728595      L1-dcache-load-misses     #    6.62% of all L1-dcache hits    (48.76%)</div><div class="line">       29943167305      L1-dcache-loads                                               (47.89%)</div><div class="line">        1957152091      L1-dcache-store-misses                                        (46.14%)</div><div class="line">       29572767575      L1-dcache-stores                                              (44.91%)</div><div class="line">        4223808613      L1-icache-load-misses                                         (43.08%)</div><div class="line">       49122358099      L1-icache-loads                                               (38.15%)</div><div class="line">        1724605628      branch-load-misses                                            (37.63%)</div><div class="line">       15225535577      branch-loads                                                  (36.61%)</div><div class="line">         997458038      dTLB-load-misses                                              (35.81%)</div><div class="line">         542426693      iTLB-load-misses                                              (34.98%)</div><div class="line"></div><div class="line">      10.489297296 seconds time elapsed</div><div class="line"></div><div class="line">[  29s] threads: 160, tps: 0.00, reads/s: 15292.01, writes/s: 0.00, response time: 25.82ms (95%)</div><div class="line">[  30s] threads: 160, tps: 0.00, reads/s: 16399.99, writes/s: 0.00, response time: 23.58ms (95%)</div><div class="line">[  31s] threads: 160, tps: 0.00, reads/s: 17025.00, writes/s: 0.00, response time: 20.73ms (95%)</div><div class="line">[  32s] threads: 160, tps: 0.00, reads/s: 16991.01, writes/s: 0.00, response time: 22.83ms (95%)</div><div class="line">[  33s] threads: 160, tps: 0.00, reads/s: 18400.94, writes/s: 0.00, response time: 21.29ms (95%)</div><div class="line">[  34s] threads: 160, tps: 0.00, reads/s: 17760.05, writes/s: 0.00, response time: 20.69ms (95%)</div><div class="line">[  35s] threads: 160, tps: 0.00, reads/s: 17935.00, writes/s: 0.00, response time: 20.23ms (95%)</div><div class="line">[  36s] threads: 160, tps: 0.00, reads/s: 18296.98, writes/s: 0.00, response time: 20.10ms (95%)</div><div class="line">[  37s] threads: 160, tps: 0.00, reads/s: 18111.02, writes/s: 0.00, response time: 20.56ms (95%)</div><div class="line">[  38s] threads: 160, tps: 0.00, reads/s: 17782.99, writes/s: 0.00, response time: 20.54ms (95%)</div><div class="line">[  38s] threads: 160, tps: 0.00, reads/s: 21412.13, writes/s: 0.00, response time: 11.96ms (95%)</div><div class="line">[  40s] threads: 160, tps: 0.00, reads/s: 18027.85, writes/s: 0.00, response time: 20.18ms (95%)</div><div class="line">[  41s] threads: 160, tps: 0.00, reads/s: 17907.04, writes/s: 0.00, response time: 20.02ms (95%)</div><div class="line">[  42s] threads: 160, tps: 0.00, reads/s: 13860.96, writes/s: 0.00, response time: 23.58ms (95%)</div><div class="line">[  43s] threads: 160, tps: 0.00, reads/s: 18491.02, writes/s: 0.00, response time: 20.18ms (95%)</div><div class="line">[  44s] threads: 160, tps: 0.00, reads/s: 17673.02, writes/s: 0.00, response time: 20.85ms (95%)</div><div class="line">[  45s] threads: 160, tps: 0.00, reads/s: 18048.96, writes/s: 0.00, response time: 21.47ms (95%)</div><div class="line">[  46s] threads: 160, tps: 0.00, reads/s: 18130.03, writes/s: 0.00, response time: 22.13ms (95%)</div></pre></td></tr></table></figure>
<h3 id="点查场景压测8个core的节点"><a href="#点查场景压测8个core的节点" class="headerlink" title="点查场景压测8个core的节点"></a>点查场景压测8个core的节点</h3><p>因为每个NUMA才8个core，所以测试一下8core的节点绑核前后性能对比。实际结果看起来和16core节点绑核性能提升差不多。</p>
<p>绑核前后对比：绑核后QPS翻倍，绑核后的服务rt从7.5降低到了2.2，rt下降非常明显，可以看出主要是绑核前跨numa访问慢。<strong>实际这个测试是先跑的不绑核，内存分布在所有NUMA上，没有重启再绑核就直接测试了，所以性能提升不明显，因为内存已经跨NUMA分配完毕了</strong>。</p>
<p><img src="/images/951413iMgBlog/image-20210427093424116.png" alt="image-20210427093424116"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">perl numa-maps-summary.pl &lt;/proc/33727/numa_maps //绑定8core后，在如下内存分配下QPS能到11000，但是抖动略大，应该是一个numa内存不够了</span></div><div class="line">N0        :          551 (  0.00 GB)</div><div class="line">N1        :      1023418 (  3.90 GB)</div><div class="line">N10       :        52065 (  0.20 GB)</div><div class="line">N11       :       190737 (  0.73 GB)</div><div class="line">N12       :       516115 (  1.97 GB)</div><div class="line">N13       :       186556 (  0.71 GB)</div><div class="line">N14       :      1677489 (  6.40 GB)</div><div class="line">N15       :       324531 (  1.24 GB)</div><div class="line">N2        :          397 (  0.00 GB)</div><div class="line">N3        :            8 (  0.00 GB)</div><div class="line">N4        :          398 (  0.00 GB)</div><div class="line">N6        :          349 (  0.00 GB)</div><div class="line">N7        :          437 (  0.00 GB)</div><div class="line">N8        :       108508 (  0.41 GB)</div><div class="line">N9        :        69162 (  0.26 GB)</div><div class="line">active    :         2296 (  0.01 GB)</div><div class="line">anon      :      4144997 ( 15.81 GB)</div><div class="line">dirty     :      4145002 ( 15.81 GB)</div><div class="line">kernelpagesize_kB:         7508 (  0.03 GB)</div><div class="line">mapmax    :         1548 (  0.01 GB)</div><div class="line">mapped    :         5724 (  0.02 GB)</div><div class="line"></div><div class="line">[ 349s] threads: 100, tps: 0.00, reads/s: 11088.99, writes/s: 0.00, response time: 20.18ms (95%)</div><div class="line">[ 350s] threads: 100, tps: 0.00, reads/s: 8778.98, writes/s: 0.00, response time: 26.20ms (95%)</div><div class="line">[ 351s] threads: 100, tps: 0.00, reads/s: 7995.01, writes/s: 0.00, response time: 31.79ms (95%)</div><div class="line">[ 352s] threads: 100, tps: 0.00, reads/s: 9549.01, writes/s: 0.00, response time: 23.90ms (95%)</div><div class="line">[ 353s] threads: 100, tps: 0.00, reads/s: 8757.99, writes/s: 0.00, response time: 24.60ms (95%)</div><div class="line">[ 354s] threads: 100, tps: 0.00, reads/s: 10288.02, writes/s: 0.00, response time: 21.85ms (95%)</div><div class="line">[ 355s] threads: 100, tps: 0.00, reads/s: 11003.97, writes/s: 0.00, response time: 18.90ms (95%)</div><div class="line">[ 356s] threads: 100, tps: 0.00, reads/s: 11111.01, writes/s: 0.00, response time: 20.51ms (95%)</div><div class="line">[ 357s] threads: 100, tps: 0.00, reads/s: 11426.00, writes/s: 0.00, response time: 17.98ms (95%)</div><div class="line">[ 358s] threads: 100, tps: 0.00, reads/s: 11007.01, writes/s: 0.00, response time: 19.35ms (95%)</div><div class="line">[ 359s] threads: 100, tps: 0.00, reads/s: 10425.00, writes/s: 0.00, response time: 20.92ms (95%)</div><div class="line">[ 360s] threads: 100, tps: 0.00, reads/s: 10024.00, writes/s: 0.00, response time: 23.17ms (95%)</div><div class="line">[ 361s] threads: 100, tps: 0.00, reads/s: 10100.98, writes/s: 0.00, response time: 22.94ms (95%)</div><div class="line">[ 362s] threads: 100, tps: 0.00, reads/s: 8164.01, writes/s: 0.00, response time: 27.48ms (95%)</div><div class="line">[ 363s] threads: 100, tps: 0.00, reads/s: 6593.00, writes/s: 0.00, response time: 37.10ms (95%)</div><div class="line">[ 364s] threads: 100, tps: 0.00, reads/s: 7008.00, writes/s: 0.00, response time: 32.32ms (95%)</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">调整这个实例到内存充足的NUMA7上 QPS峰值能到14000，稳定在11000-13000之间，RT明显更稳定了</span></div><div class="line"><span class="meta">#</span><span class="bash">perl numa-maps-summary.pl &lt;/proc/78245/numa_maps</span></div><div class="line">N0        :          551 (  0.00 GB)</div><div class="line">N1        :          115 (  0.00 GB)</div><div class="line">N11       :          695 (  0.00 GB)</div><div class="line">N12       :          878 (  0.00 GB)</div><div class="line">N13       :         2019 (  0.01 GB)</div><div class="line">N14       :           25 (  0.00 GB)</div><div class="line">N15       :           60 (  0.00 GB)</div><div class="line">N2        :          394 (  0.00 GB)</div><div class="line">N3        :            8 (  0.00 GB)</div><div class="line">N4        :       197713 (  0.75 GB)</div><div class="line">N6        :          349 (  0.00 GB)</div><div class="line">N7        :      3957844 ( 15.10 GB)</div><div class="line">N8        :            1 (  0.00 GB)</div><div class="line">active    :           10 (  0.00 GB)</div><div class="line">anon      :      4154693 ( 15.85 GB)</div><div class="line">dirty     :      4154698 ( 15.85 GB)</div><div class="line">kernelpagesize_kB:         7452 (  0.03 GB)</div><div class="line">mapmax    :         1567 (  0.01 GB)</div><div class="line">mapped    :         5959 (  0.02 GB)</div><div class="line"></div><div class="line">[ 278s] threads: 100, tps: 0.00, reads/s: 13410.99, writes/s: 0.00, response time: 15.36ms (95%)</div><div class="line">[ 279s] threads: 100, tps: 0.00, reads/s: 14049.99, writes/s: 0.00, response time: 15.54ms (95%)</div><div class="line">[ 280s] threads: 100, tps: 0.00, reads/s: 13107.02, writes/s: 0.00, response time: 16.72ms (95%)</div><div class="line">[ 281s] threads: 100, tps: 0.00, reads/s: 12431.99, writes/s: 0.00, response time: 17.79ms (95%)</div><div class="line">[ 282s] threads: 100, tps: 0.00, reads/s: 13164.01, writes/s: 0.00, response time: 16.33ms (95%)</div><div class="line">[ 283s] threads: 100, tps: 0.00, reads/s: 13455.01, writes/s: 0.00, response time: 16.19ms (95%)</div><div class="line">[ 284s] threads: 100, tps: 0.00, reads/s: 12932.01, writes/s: 0.00, response time: 16.22ms (95%)</div><div class="line">[ 285s] threads: 100, tps: 0.00, reads/s: 12790.99, writes/s: 0.00, response time: 17.00ms (95%)</div><div class="line">[ 286s] threads: 100, tps: 0.00, reads/s: 12706.00, writes/s: 0.00, response time: 17.88ms (95%)</div><div class="line">[ 287s] threads: 100, tps: 0.00, reads/s: 11886.00, writes/s: 0.00, response time: 19.43ms (95%)</div><div class="line">[ 288s] threads: 100, tps: 0.00, reads/s: 12700.00, writes/s: 0.00, response time: 16.97ms (95%)</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">perl numa-maps-summary.pl &lt;/proc/54723/numa_maps  //54723绑定在NUMA6上</span></div><div class="line">N0        :          551 (  0.00 GB)</div><div class="line">N1        :          115 (  0.00 GB)</div><div class="line">N11       :          682 (  0.00 GB)</div><div class="line">N12       :          856 (  0.00 GB)</div><div class="line">N13       :         2018 (  0.01 GB)</div><div class="line">N14       :           25 (  0.00 GB)</div><div class="line">N15       :           60 (  0.00 GB)</div><div class="line">N2        :      1270166 (  4.85 GB) //不应该分配这里的内存，实际是因为N6内存被PageCache使用掉了</div><div class="line"></div><div class="line">N3        :            8 (  0.00 GB)</div><div class="line">N4        :          398 (  0.00 GB)</div><div class="line">N6        :      3662400 ( 13.97 GB)</div><div class="line">N7        :          460 (  0.00 GB)</div><div class="line">N8        :            1 (  0.00 GB)</div><div class="line">active    :            9 (  0.00 GB)</div><div class="line">anon      :      4931796 ( 18.81 GB)</div><div class="line">dirty     :      4931801 ( 18.81 GB)</div><div class="line">kernelpagesize_kB:         7920 (  0.03 GB)</div><div class="line">mapmax    :         1580 (  0.01 GB)</div><div class="line">mapped    :         5944 (  0.02 GB)</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">cat /proc/meminfo | grep -i active</span></div><div class="line">Active:         22352360 kB</div><div class="line">Inactive:       275173756 kB</div><div class="line">Active(anon):      16984 kB</div><div class="line">Inactive(anon): 240344208 kB</div><div class="line">Active(file):   22335376 kB</div><div class="line">Inactive(file): 34829548 kB</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"><span class="built_in">echo</span> 3 &gt; /proc/sys/vm/drop_caches</span></div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">cat /proc/meminfo | grep -i active</span></div><div class="line">Active:          1865724 kB</div><div class="line">Inactive:       242335632 kB</div><div class="line">Active(anon):       7108 kB</div><div class="line">Inactive(anon): 240199020 kB</div><div class="line">Active(file):    1858616 kB  //回收了大量PageCache内存</div><div class="line">Inactive(file):  2136612 kB</div><div class="line"><span class="meta">#</span><span class="bash">perl numa-maps-summary.pl &lt;/proc/54723/numa_maps</span></div><div class="line">N0        :          552 (  0.00 GB)</div><div class="line">N1        :          115 (  0.00 GB)</div><div class="line">N11       :          682 (  0.00 GB)</div><div class="line">N12       :          856 (  0.00 GB)</div><div class="line">N13       :         2018 (  0.01 GB)</div><div class="line">N14       :           25 (  0.00 GB)</div><div class="line">N15       :           60 (  0.00 GB)</div><div class="line">N2        :         1740 (  0.01 GB)</div><div class="line">N3        :            8 (  0.00 GB)</div><div class="line">N4        :          398 (  0.00 GB)</div><div class="line">N6        :      4972492 ( 18.97 GB)</div><div class="line">N7        :          459 (  0.00 GB)</div><div class="line">N8        :            1 (  0.00 GB)</div><div class="line">active    :           16 (  0.00 GB)</div><div class="line">anon      :      4973486 ( 18.97 GB)</div><div class="line">dirty     :      4973491 ( 18.97 GB)</div><div class="line">kernelpagesize_kB:         8456 (  0.03 GB)</div><div class="line">mapmax    :         1564 (  0.01 GB)</div><div class="line">mapped    :         5920 (  0.02 GB)</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20210427164953340.png" alt="image-20210427164953340"></p>
<p>绑核前的IPC：</p>
<p><img src="/images/951413iMgBlog/image-20210427093625575.png" alt="image-20210427093625575"></p>
<p>绑核后的IPC：</p>
<p><img src="/images/951413iMgBlog/image-20210427095130343.png" alt="image-20210427095130343"></p>
<p><strong>如果是两个8core对一个16core在都最优绑核场景下从上面的数据来看能有40-50%的性能提升，并且RT抖动更小</strong>，这两个8core绑定在同一个Socket下，验证是否争抢，同时可以看到<strong>绑核后性能可以随着加节点线性增加</strong></p>
<p><img src="/images/951413iMgBlog/image-20210427172612685.png" alt="image-20210427172612685"></p>
<p><img src="/images/951413iMgBlog/image-20210427173047815.png" alt="image-20210427173047815"></p>
<p><img src="/images/951413iMgBlog/image-20210427173417673.png" alt="image-20210427173417673"></p>
<p>结论：不绑核一个FT2500的core点查只有500 QPS，绑核后能到1500QPS, 在Intel 8263下一个core能到6000以上(开日志、没开协程)</p>
<h3 id="MySQL-数据库场景绑核"><a href="#MySQL-数据库场景绑核" class="headerlink" title="MySQL 数据库场景绑核"></a>MySQL 数据库场景绑核</h3><p>通过同一台物理上6个Tomcat节点，总共96个core，压6台MySQL，MySQL基本快打挂了。sysbench 点查，32个分表，增加Tomcat节点进来物理rt就增加，从最初的的1.2ms加到6个Tomcat节点后变成8ms。</p>
<p><img src="/images/951413iMgBlog/image-20210425180535225.png" alt="image-20210425180535225"></p>
<p>MySQL没绑好核，BIOS默认关闭了NUMA，外加12个MySQL分布在物理机上不均匀，3个节点3个MySQL，剩下的物理机上只有一个MySQL实例。</p>
<p>MySQL每个实例32core，管控默认已经做了绑核，但是如果两个MySQL绑在了一个socket上竞争会很激烈，ipc比单独的降一半。</p>
<p>比如这三个MySQL，qps基本均匀，上面两个cpu高，但是没效率，每个MySQL绑了32core，上面两个绑在一个socket上，下面的MySQL绑在另一个socket上，第一个socket还有网络软中断在争抢cpu，飞腾环境下性能真要冲高还有很大空间。</p>
<p><img src="/images/951413iMgBlog/image-20210425180518926.png" alt="image-20210425180518926"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">第二个MySQL IPC只有第三个的30%多点，这就是为什么CPU高这么多，但是QPS差不多</span></div><div class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses  -a -p 61238</div><div class="line">^C</div><div class="line"> Performance counter stats for process id '61238':</div><div class="line"></div><div class="line">        86,491,052      branch-misses                                                 (58.55%)</div><div class="line">    98,481,418,793      bus-cycles                                                    (55.64%)</div><div class="line">       113,095,618      cache-misses              #    6.169 % of all cache refs      (53.20%)</div><div class="line">     1,833,344,484      cache-references                                              (52.00%)</div><div class="line">   101,516,165,898      cpu-cycles                                                    (57.09%)</div><div class="line">     4,229,190,014      instructions              #    0.04  insns per cycle          (55.91%)</div><div class="line">       111,780,025      L1-dcache-load-misses     #    6.34% of all L1-dcache hits    (55.40%)</div><div class="line">     1,764,421,570      L1-dcache-loads                                               (52.62%)</div><div class="line">       112,261,128      L1-dcache-store-misses                                        (49.34%)</div><div class="line">     1,814,998,338      L1-dcache-stores                                              (48.51%)</div><div class="line">       219,372,119      L1-icache-load-misses                                         (49.56%)</div><div class="line">     2,816,279,627      L1-icache-loads                                               (49.15%)</div><div class="line">        85,321,093      branch-load-misses                                            (50.38%)</div><div class="line">     1,038,572,653      branch-loads                                                  (50.65%)</div><div class="line">        45,166,831      dTLB-load-misses                                              (51.98%)</div><div class="line">        29,892,473      iTLB-load-misses                                              (52.56%)</div><div class="line"></div><div class="line">       1.163750756 seconds time elapsed</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">第三个MySQL</span></div><div class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses  -a -p 53400</div><div class="line">^C</div><div class="line"> Performance counter stats for process id '53400':</div><div class="line"></div><div class="line">       295,575,513      branch-misses                                                 (40.51%)</div><div class="line">   110,934,600,206      bus-cycles                                                    (39.30%)</div><div class="line">       537,938,496      cache-misses              #    8.310 % of all cache refs      (38.99%)</div><div class="line">     6,473,688,885      cache-references                                              (39.80%)</div><div class="line">   110,540,950,757      cpu-cycles                                                    (46.10%)</div><div class="line">    14,766,013,708      instructions              #    0.14  insns per cycle          (46.85%)</div><div class="line">       538,521,226      L1-dcache-load-misses     #    8.36% of all L1-dcache hits    (48.00%)</div><div class="line">     6,440,728,959      L1-dcache-loads                                               (46.69%)</div><div class="line">       533,693,357      L1-dcache-store-misses                                        (45.91%)</div><div class="line">     6,413,111,024      L1-dcache-stores                                              (44.92%)</div><div class="line">       673,725,952      L1-icache-load-misses                                         (42.76%)</div><div class="line">     9,216,663,639      L1-icache-loads                                               (38.27%)</div><div class="line">       299,202,001      branch-load-misses                                            (37.62%)</div><div class="line">     3,285,957,082      branch-loads                                                  (36.10%)</div><div class="line">       149,348,740      dTLB-load-misses                                              (35.20%)</div><div class="line">       102,444,469      iTLB-load-misses                                              (34.78%)</div><div class="line"></div><div class="line">       8.080841166 seconds time elapsed</div></pre></td></tr></table></figure>
<p>12个MySQL流量基本均匀：</p>
<p><img src="/images/951413iMgBlog/image-20210426083033989.png" alt="image-20210426083033989"></p>
<h3 id="numa太多，每个numa下core比较少"><a href="#numa太多，每个numa下core比较少" class="headerlink" title="numa太多，每个numa下core比较少"></a>numa太多，每个numa下core比较少</h3><p>导致跨numa高概率发生，如下是在正常部署下的测试perf 数据，可以看到IPC极低，才0.08，同样的场景在其他家芯片都能打到0.6</p>
<p><img src="/images/oss/16b271c8-5132-4273-a26a-4b35e8f92882.png" alt="img"></p>
<p>执行绑核，将一个进程限制在2个numa内，因为进程需要16core，理论上用8core的进程性能会更好</p>
<p><img src="/images/oss/4d4fdebb-6146-407e-881d-19170fbfd82b.png" alt="img"></p>
<p>可以看到IPC从0.08提升到了0.22，实际能到0.27，对应的业务测试QPS也是原来的4倍。 </p>
<p>用numactl 在启动的时候绑定cpu在 node0、1上，优先使用node0、1的内存，不够再用其它node的内存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numactl --cpunodebind 0,1 --preferred 0,1 /u01/xcluster80/bin/mysqld_safe  --defaults-file=/polarx/xcluster3308/my.cnf  --basedir=/u01/xcluster80_current  --datadir=/polarx/xcluster3308/data  --plugin-dir=/u01/xcluster80/lib/plugin  --user=mysql  --log-error=/polarx/xcluster3308/log/alert.log  --open-files-limit=615350  --pid-file=/polarx/xcluster3308/run/mysql.pid  --socket=/polarx/xcluster3308/run/mysql.sock  --cluster-info=11.158.239.200:11308@1  --mysqlx-port=13308  --port=3308</div></pre></td></tr></table></figure>
<h3 id="网卡队列调整"><a href="#网卡队列调整" class="headerlink" title="网卡队列调整"></a>网卡队列调整</h3><p>这批机器默认都是双网卡做bond，但是两块网卡是HA，默认网卡队列是60，基本都跑在前面60个core上</p>
<p>将MySQL网卡队列从60个改成6个后MySQL性能提升大概10%</p>
<p><img src="/images/951413iMgBlog/image-20210426085534983.png" alt="image-20210426085534983"></p>
<p>默认第一个MySQL都绑在0-31号核上,其实改少队列加大了0-5号core的压力，但是实际数据表现要好。</p>
<h2 id="比较其它"><a href="#比较其它" class="headerlink" title="比较其它"></a>比较其它</h2><p>绑核的时候还要考虑磁盘、网卡在哪个socket上，相对来说node和磁盘、网卡在同一个socket下性能要好一些。</p>
<p>左边的mysqld绑定在socket1的64core上，磁盘、网卡都在socket1上；右边的mysqld绑定在0-31core上，网卡在socket0上，但是磁盘在socket1上</p>
<p>右边这个刚好是跨socket访问磁盘，不知道是不是巧合log_flush排位比较高</p>
<p><img src="/images/951413iMgBlog/image-20210910180305752.png" alt="image-20210910180305752"></p>
<p>此时对应的IPC：</p>
<p><img src="/images/951413iMgBlog/image-20210910181820803.png" alt="image-20210910181820803"></p>
<p>如果上面两个进程在没有刷日志的场景下时候对应的IPC两者基本一样：</p>
<p><img src="/images/951413iMgBlog/image-20210910181909962.png" alt="image-20210910181909962"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>FT2500比同主频Intel x86芯片差了快一个数量级的性能，在对FT2500上的业务按node绑核后性能提升了几倍，但是离Intel x86还有很大的距离</p>
<p>用循环跑多个nop指令在飞腾2500下IPC只能跑到1，据说这是因为nop指令被扔掉了，所以一直在跑跳转循环判断；</p>
<p>对寄存器变量进行++运算，IPC是0.5； </p>
<p>用如下代码能将IPC跑到2.49，也是我能跑出来的最高IPC了，去掉nop那行，IPC是1.99</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">        register unsigned i=0;</div><div class="line">for (i=0;i&lt;(1u&lt;&lt;31);i++) &#123;</div><div class="line">        __asm__ (&quot;nop&quot;); </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU的制造和概念/">CPU的制造和概念</a></p>
<p><a href="/2021/05/16/CPU Cache Line 和性能/">CPU 性能和Cache Line</a></p>
<p><a href="/2021/05/16/Perf IPC以及CPU利用率/">Perf IPC以及CPU性能</a></p>
<p><a href="/2021/06/18/几款CPU性能对比/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/05/15/飞腾ARM芯片(FT2500">飞腾ARM芯片(FT2500)的性能测试</a>的性能测试/)</p>
<p><a href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2021/03/07/一次海光物理机资源竞争压测的记录/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html" target="_blank" rel="external">CPU Utilization is Wrong</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/05/14/十年后数据库还是不敢拥抱NUMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/" itemprop="url">十年后数据库还是不敢拥抱NUMA？</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-14T17:30:03+08:00">
                2021-05-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="十年后数据库还是不敢拥抱NUMA？"><a href="#十年后数据库还是不敢拥抱NUMA？" class="headerlink" title="十年后数据库还是不敢拥抱NUMA？"></a>十年后数据库还是不敢拥抱NUMA？</h1><p>在2010年前后MySQL、PG、Oracle数据库在使用NUMA的时候碰到了性能问题，流传最广的这篇  <a href="http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="external">MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture</a> 描述了性能问题的原因(文章中把原因找错了)以及解决方案：关闭NUMA。 实际这个原因是kernel实现的一个低级bug，这个Bug在<a href="https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82" target="_blank" rel="external">2014年修复了</a>，但是修复这么多年后仍然以讹传讹，这篇文章希望正本清源、扭转错误的认识。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近在做一次性能测试的时候发现MySQL实例有一个奇怪现象，在128core的物理机上运行三个MySQL实例，每个实例分别绑定32个物理core，绑定顺序就是第一个0-31、第二个32-63、第三个64-95，实际运行结果让人大跌眼镜，如下图</p>
<p><img src="/images/951413iMgBlog/1620953504602-30988926-85d8-4af1-996d-f35aa5fede00.png" alt="undefined"> </p>
<p>从CPU消耗来看差异巨大，高的实例CPU用到了2500%，低的才488%，差了5倍。但是神奇的是他们的QPS一样，执行的SQL也是一样</p>
<p><img src="/images/951413iMgBlog/1620953709047-cbe4b59c-aa2b-4845-8b59-9ed6d07e3916.png" alt="undefined"><br>所有MySQL实例流量一样</p>
<p>那么问题来了为什么在同样的机器上、同样的流量下CPU使用率差了这么多？ 换句话来问就是CPU使用率高就有效率吗？</p>
<p>这台物理机CPU 信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#lscpu</div><div class="line">Architecture:          aarch64</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                128</div><div class="line">On-line CPU(s) list:   0-127</div><div class="line">Thread(s) per core:    1</div><div class="line">Core(s) per socket:    64</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          1</div><div class="line">Model:                 3</div><div class="line">BogoMIPS:              100.00</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              2048K</div><div class="line">L3 cache:              65536K</div><div class="line">NUMA node0 CPU(s):     0-127</div><div class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</div></pre></td></tr></table></figure></p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>先来看这两个MySQL 进程的Perf数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">#第二个 MySQL IPC只有第三个的30%多点，这就是为什么CPU高这么多，但是QPS差不多</div><div class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses  -a -p 61238</div><div class="line">^C</div><div class="line"> Performance counter stats for process id &apos;61238&apos;:</div><div class="line"></div><div class="line">        86,491,052      branch-misses                                                 (58.55%)</div><div class="line">    98,481,418,793      bus-cycles                                                    (55.64%)</div><div class="line">       113,095,618      cache-misses              #    6.169 % of all cache refs      (53.20%)</div><div class="line">     1,833,344,484      cache-references                                              (52.00%)</div><div class="line">   101,516,165,898      cpu-cycles                                                    (57.09%)</div><div class="line">     4,229,190,014      instructions              #    0.04  insns per cycle          (55.91%)</div><div class="line">       111,780,025      L1-dcache-load-misses     #    6.34% of all L1-dcache hits    (55.40%)</div><div class="line">     1,764,421,570      L1-dcache-loads                                               (52.62%)</div><div class="line">       112,261,128      L1-dcache-store-misses                                        (49.34%)</div><div class="line">     1,814,998,338      L1-dcache-stores                                              (48.51%)</div><div class="line">       219,372,119      L1-icache-load-misses                                         (49.56%)</div><div class="line">     2,816,279,627      L1-icache-loads                                               (49.15%)</div><div class="line">        85,321,093      branch-load-misses                                            (50.38%)</div><div class="line">     1,038,572,653      branch-loads                                                  (50.65%)</div><div class="line">        45,166,831      dTLB-load-misses                                              (51.98%)</div><div class="line">        29,892,473      iTLB-load-misses                                              (52.56%)</div><div class="line"></div><div class="line">       1.163750756 seconds time elapsed</div><div class="line"></div><div class="line">#第三个 MySQL</div><div class="line">perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses  -a -p 53400</div><div class="line">^C</div><div class="line"> Performance counter stats for process id &apos;53400&apos;:</div><div class="line"></div><div class="line">       295,575,513      branch-misses                                                 (40.51%)</div><div class="line">   110,934,600,206      bus-cycles                                                    (39.30%)</div><div class="line">       537,938,496      cache-misses              #    8.310 % of all cache refs      (38.99%)</div><div class="line">     6,473,688,885      cache-references                                              (39.80%)</div><div class="line">   110,540,950,757      cpu-cycles                                                    (46.10%)</div><div class="line">    14,766,013,708      instructions              #    0.14  insns per cycle          (46.85%)</div><div class="line">       538,521,226      L1-dcache-load-misses     #    8.36% of all L1-dcache hits    (48.00%)</div><div class="line">     6,440,728,959      L1-dcache-loads                                               (46.69%)</div><div class="line">       533,693,357      L1-dcache-store-misses                                        (45.91%)</div><div class="line">     6,413,111,024      L1-dcache-stores                                              (44.92%)</div><div class="line">       673,725,952      L1-icache-load-misses                                         (42.76%)</div><div class="line">     9,216,663,639      L1-icache-loads                                               (38.27%)</div><div class="line">       299,202,001      branch-load-misses                                            (37.62%)</div><div class="line">     3,285,957,082      branch-loads                                                  (36.10%)</div><div class="line">       149,348,740      dTLB-load-misses                                              (35.20%)</div><div class="line">       102,444,469      iTLB-load-misses                                              (34.78%)</div><div class="line"></div><div class="line">       8.080841166 seconds time elapsed</div></pre></td></tr></table></figure>
<p>从上面可以看到 IPC 差异巨大0.04 VS 0.14 ，也就是第一个MySQL的CPU效率很低，我们看到的CPU running实际是CPU在等待(stall)。</p>
<h3 id="CPU的实际信息"><a href="#CPU的实际信息" class="headerlink" title="CPU的实际信息"></a>CPU的实际信息</h3><p>找到同一个机型，但是NUMA开着的查了一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">#lscpu</div><div class="line">Architecture:          aarch64</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                128</div><div class="line">On-line CPU(s) list:   0-127</div><div class="line">Thread(s) per core:    1</div><div class="line">Core(s) per socket:    64</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          16</div><div class="line">Model:                 3</div><div class="line">BogoMIPS:              100.00</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              2048K</div><div class="line">L3 cache:              65536K</div><div class="line">NUMA node0 CPU(s):     0-7</div><div class="line">NUMA node1 CPU(s):     8-15</div><div class="line">NUMA node2 CPU(s):     16-23</div><div class="line">NUMA node3 CPU(s):     24-31</div><div class="line">NUMA node4 CPU(s):     32-39</div><div class="line">NUMA node5 CPU(s):     40-47</div><div class="line">NUMA node6 CPU(s):     48-55</div><div class="line">NUMA node7 CPU(s):     56-63</div><div class="line">NUMA node8 CPU(s):     64-71</div><div class="line">NUMA node9 CPU(s):     72-79</div><div class="line">NUMA node10 CPU(s):    80-87</div><div class="line">NUMA node11 CPU(s):    88-95</div><div class="line">NUMA node12 CPU(s):    96-103</div><div class="line">NUMA node13 CPU(s):    104-111</div><div class="line">NUMA node14 CPU(s):    112-119</div><div class="line">NUMA node15 CPU(s):    120-127</div><div class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</div></pre></td></tr></table></figure>
<p>这告诉我们实际上这个机器有16个NUMA，跨NUMA访问内存肯定比访问本NUMA内的要慢几倍。</p>
<h2 id="关于NUMA"><a href="#关于NUMA" class="headerlink" title="关于NUMA"></a>关于NUMA</h2><p>如下图，是一个Intel Xeon E5 CPU的架构信息，左右两边的大红框分别是两个NUMA，每个NUMA的core访问直接插在自己红环上的内存必然很快，如果访问插在其它NUMA上的内存还要走两个红环之间上下的黑色箭头线路，所以要慢很多。</p>
<p><img src="/images/951413iMgBlog/1623830161880-c4c74f4d-785e-4274-a579-5d1aa8b5e990.png" alt="img"></p>
<p>实际测试Intel的E5-2682（对应V42机型）和8269（对应V62机型） 的CPU跨Socket（这两块CPU内部不再是上图的红环Bus,而是改用了Mesh Bus一个Die就是一个NUMA，服务器有两路，也就是一个Socket就是一个NUMA），也就是跨NUMA访问内存的延迟是本Node延迟的将近2倍。<a href="https://software.intel.com/content/www/us/en/develop/articles/intelr-memory-latency-checker.html" target="_blank" rel="external">测试工具从这里下载</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">//E5-2682</div><div class="line">Intel(R) Memory Latency Checker - v3.9</div><div class="line">Measuring idle latencies (in ns)...</div><div class="line">		Numa node</div><div class="line">Numa node	     0	     1</div><div class="line">       0	  85.0	 136.3</div><div class="line">       1	 137.2	  84.2</div><div class="line"></div><div class="line">//8269</div><div class="line">Intel(R) Memory Latency Checker - v3.9  </div><div class="line">Measuring idle latencies (in ns)...</div><div class="line">    Numa node</div><div class="line">Numa node      0       1</div><div class="line">       0    78.6   144.1</div><div class="line">       1   144.7    78.5</div></pre></td></tr></table></figure>
<p>开启NUMA会优先就近使用内存，在本NUMA上的内存不够的时候可以选择回收本地的PageCache还是到其它NUMA 上分配内存，这是可以通过Linux参数 zone_reclaim_mode 来配置的，默认是到其它NUMA上分配内存，也就是跟关闭NUMA是一样的。</p>
<p><strong>这个架构距离是物理上就存在的不是你在BIOS里关闭了NUMA差异就消除了，我更愿意认为在BIOS里关掉NUMA只是掩耳盗铃。</strong></p>
<p>以上理论告诉我们：<strong>也就是在开启NUMA和 zone_reclaim_mode 默认在内存不够的如果去其它NUMA上分配内存，比关闭NUMA要快很多而没有任何害处。</strong></p>
<h4 id="UMA和NUMA对比"><a href="#UMA和NUMA对比" class="headerlink" title="UMA和NUMA对比"></a>UMA和NUMA对比</h4><p>The SMP/UMA architecture</p>
<p><img src="/images/951413iMgBlog/uma-architecture.png" alt="img"></p>
<p>The NUMA architecture</p>
<p><img src="/images/951413iMgBlog/numa-architecture.png" alt="img"></p>
<p>Modern multiprocessor systems mix these basic architectures as seen in the following diagram:</p>
<p><img src="/images/951413iMgBlog/39354-figure-3-184398.jpg" alt="img"></p>
<p>In this complex hierarchical scheme, processors are grouped by their physical location on one or the other multi-core CPU package or “node.” Processors within a node share access to memory modules as per the UMA shared memory architecture. At the same time, they may also access memory from the remote node using a shared interconnect, but with slower performance as per the NUMA shared memory architecture.</p>
<p><img src="/images/951413iMgBlog/03-05-Broadwell_HCC_Architecture.svg" alt="03-05-Broadwell_HCC_Architecture"></p>
<h2 id="对比测试Intel-NUMA-性能"><a href="#对比测试Intel-NUMA-性能" class="headerlink" title="对比测试Intel NUMA 性能"></a>对比测试Intel NUMA 性能</h2><p>对如下Intel CPU进行一些测试，在开启NUMA的情况下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">#lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                64</div><div class="line">On-line CPU(s) list:   0-63</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    16</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 79</div><div class="line">Model name:            Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz</div><div class="line">Stepping:              1</div><div class="line">CPU MHz:               2500.000</div><div class="line">CPU max MHz:           3000.0000</div><div class="line">CPU min MHz:           1200.0000</div><div class="line">BogoMIPS:              5000.06</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              256K</div><div class="line">L3 cache:              40960K</div><div class="line">NUMA node0 CPU(s):     0-15,32-47</div><div class="line">NUMA node1 CPU(s):     16-31,48-63</div><div class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb invpcid_single pln pts dtherm spec_ctrl ibpb_support tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local cat_l3</div><div class="line"></div><div class="line">#numastat</div><div class="line">                           node0           node1</div><div class="line">numa_hit               129600200        60501102</div><div class="line">numa_miss                      0               0</div><div class="line">numa_foreign                   0               0</div><div class="line">interleave_hit            108648          108429</div><div class="line">local_node             129576548        60395061</div><div class="line">other_node                 23652          106041</div></pre></td></tr></table></figure>
<p>我在这个64core的物理机上运行一个MySQL 实例，先将MySQL进程绑定在0-63core，0-31core，以及0-15,32-47上</p>
<p>用sysbench对一亿条记录跑点查，数据都加载到内存中了：</p>
<ul>
<li>绑0-63core qps 不到8万，总cpu跑到5000%，降低并发的话qps能到11万；</li>
<li>如果绑0-31core qps 12万，总cpu跑到3200%，IPC 0.29；</li>
<li>如果绑同一个numa下的32core，qps飙到27万，总CPU跑到3200%  IPC: 0.42；</li>
<li>绑0-15个物理core，qps能到17万，绑32-47也是一样的效果；</li>
</ul>
<p><img src="/images/951413iMgBlog/1620954918277-c669bd74-df58-4d69-8185-a93f37046972.png" alt="undefined"> </p>
<p>从这个数据看起来<strong>即使Intel在只有两个NUMA的情况下跨性能差异也有2倍，可见正确的绑核方法收益巨大，尤其是在刷榜的情况下</strong>， NUMA更多性能差异应该会更大。</p>
<p>说明前面的理论是正确的。</p>
<p>来看看不通绑核情况下node之间的带宽利用情况：</p>
<p><img src="/images/951413iMgBlog/image-20210525151537507.png" alt="image-20210525151537507"></p>
<p><img src="/images/951413iMgBlog/image-20210525151622425.png" alt="image-20210525151622425"></p>
<p>实际在不开NUMA的同样CPU上，进行以上各种绑核测试，测试结果也完全一样。</p>
<p>如果比较读写混合场景的话肯定会因为写锁导致CPU跑起来，最终的性能差异也不会这么大，但是绑在同一个NUMA下的性能肯定要好，IPC也会高一些。具体好多少取决于锁的竞争程度。</p>
<h2 id="为什么集团内外所有物理机都把NUMA关掉了呢？"><a href="#为什么集团内外所有物理机都把NUMA关掉了呢？" class="headerlink" title="为什么集团内外所有物理机都把NUMA关掉了呢？"></a>为什么集团内外所有物理机都把NUMA关掉了呢？</h2><p>10年前几乎所有的运维都会多多少少被NUMA坑害过，让我们看看究竟有多少种在NUMA上栽的方式：</p>
<ul>
<li><a href="http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="external">MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture</a></li>
<li><a href="http://frosty-postgres.blogspot.com/2012/08/postgresql-numa-and-zone-reclaim-mode.html" target="_blank" rel="external">PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux</a></li>
<li><a href="http://blog.yannickjaquier.com/hpux/non-uniform-memory-access-numa-architecture-with-oracle-database-by-examples.html" target="_blank" rel="external">Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples</a></li>
<li><a href="http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases" target="_blank" rel="external">Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases</a></li>
</ul>
<p>最有名的是这篇  <a href="http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="external">MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture</a></p>
<p>我总结下这篇2010年的文章说的是啥：</p>
<ul>
<li>如果本NUMA内存不够的时候，Linux会优先回收PageCache内存，即使其它NUMA还有内存</li>
<li>回收PageCache经常会造成系统卡顿，这个卡顿不能接受</li>
</ul>
<p>所以文章给出的解决方案就是（三选一）：</p>
<ul>
<li>关掉NUMA</li>
<li>或者启动MySQL的时候指定不分NUMA,比如：/usr/bin/numactl –interleave all $cmd</li>
<li>或者启动MySQL的时候先回收所有PageCache</li>
</ul>
<p>我想这就是这么多人在上面栽了跟头，所以干脆一不做二不休干脆关了NUMA 一了百了。</p>
<p>但真的NUMA有这么糟糕？或者说Linux Kernel有这么笨，默认优先去回收PageCache吗？</p>
<h2 id="Linux-Kernel对NUMA内存的使用"><a href="#Linux-Kernel对NUMA内存的使用" class="headerlink" title="Linux Kernel对NUMA内存的使用"></a>Linux Kernel对NUMA内存的使用</h2><p>实际我们使用NUMA的时候期望是：优先使用本NUMA上的内存，如果本NUMA不够了不要优先回收PageCache而是优先使用其它NUMA上的内存。</p>
<h3 id="zone-reclaim-mode"><a href="#zone-reclaim-mode" class="headerlink" title="zone_reclaim_mode"></a>zone_reclaim_mode</h3><p>事实上Linux识别到NUMA架构后，默认的内存分配方案就是：优先尝试在请求线程当前所处的CPU的Local内存上分配空间。<strong>如果local内存不足，优先淘汰local内存中无用的Page（Inactive，Unmapped）</strong>。然后才到其它NUMA上分配内存。</p>
<p>intel 芯片跨node延迟远低于其他家，所以跨node性能损耗不大</p>
<p>zone_reclaim_mode，它用来管理当一个内存区域(zone)内部的内存耗尽时，是从其内部进行内存回收还是可以从其他zone进行回收的选项：</p>
<p>zone_reclaim_mode:</p>
<blockquote>
<p>Zone_reclaim_mode allows someone to set more or less aggressive approaches to<br>reclaim memory when a zone runs out of memory. If it is set to zero then no<br>zone reclaim occurs. Allocations will be satisfied from other zones / nodes<br>in the system.</p>
</blockquote>
<p>zone_reclaim_mode的四个参数值的意义分别是：</p>
<p>0   = Allocate from all nodes before reclaiming memory<br>1   = Reclaim memory from local node vs allocating from next node<br>2   = Zone reclaim writes dirty pages out<br>4   = Zone reclaim swaps pages</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># cat /proc/sys/vm/zone_reclaim_mode</div><div class="line">0</div></pre></td></tr></table></figure>
<p>我查了2.6.32以及4.19.91内核的机器 zone_reclaim_mode 都是默认0 ，也就是kernel会：优先使用本NUMA上的内存，如果本NUMA不够了不要优先回收PageCache而是优先使用其它NUMA上的内存。这也是我们想要的</p>
<p>Kernel文档也告诉大家默认就是0，但是为什么会出现优先回收了PageCache呢？</p>
<h3 id="查看kernel提交记录"><a href="#查看kernel提交记录" class="headerlink" title="查看kernel提交记录"></a>查看kernel提交记录</h3><p><a href="https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82" target="_blank" rel="external">github kernel commit</a></p>
<p><img src="/images/951413iMgBlog/1620956491058-09a1ebc6-c248-41db-9def-67b4f489c4f4.png" alt="undefined"> </p>
<p><img src="/images/951413iMgBlog/1620956524069-85ec2c06-ff55-48e9-8c26-96e738456ed4.png" alt="undefined"> </p>
<p><img src="/images/951413iMgBlog/1620956551990-6e376a3d-de40-4180-a05b-b21a9cbf33bc.png" alt="undefined"> </p>
<p>关键是上图红框中的代码，node distance比较大（也就是开启了NUMA的话），强制将 zone_reclaim_mode设为1，这是2014年提交的代码，将这个强制设为1的逻辑去掉了。</p>
<p>这也就是为什么之前大佬们碰到NUMA问题后尝试修改 zone_reclaim_mode 没有效果，<strong>也就是2014年前只要开启了NUMA就强制线回收PageCache，即使设置zone_reclaim_mode也没有意义，真是个可怕的Bug。</strong></p>
<h3 id="验证一下zone-reclaim-mode-0是生效的"><a href="#验证一下zone-reclaim-mode-0是生效的" class="headerlink" title="验证一下zone_reclaim_mode 0是生效的"></a>验证一下zone_reclaim_mode 0是生效的</h3><p>内核版本：3.10.0-327.ali2017.alios7.x86_64</p>
<h4 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a><a href="https://github.com/torvalds/linux/commit/e02dc017c3032dcdce1b993af0db135462e1b4b7" target="_blank" rel="external">测试方法</a></h4><p>先将一个160G的文件加载到内存里，然后再用代码分配64G的内存出来使用。<br>单个NUMA node的内存为256G，本身用掉了60G，加上这次的160G的PageCache，和之前的一些其他PageCache,总的 PageCache用了179G，那么这个node总内存还剩256G-60G-179G，</p>
<p>如果这个时候再分配64G内存的话，本node肯定不够了，我们来看在 zone_reclaim_mode=0 的时候是优先回收PageCache还是分配了到另外一个NUMA node(这个NUMA node 有240G以上的内存空闲）</p>
<h4 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h4><p>分配64G内存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#taskset -c 0 ./alloc 64</div><div class="line">To allocate 64GB memory</div><div class="line">Used time: 39 seconds</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/1620966121309-a264fd7f-fe50-4fc6-940f-4cb603ec7874.png" alt="undefined"> </p>
<p>从如上截图来看，再分配64G内存的时候即使node0不够了也没有回收node0上的PageCache，而是将内存跨NUMA分配到了node1上，符合预期！</p>
<p>释放这64G内存后，如下图可以看到node0回收了25G，剩下的39G都是在node1上：<br><img src="/images/951413iMgBlog/1620967573650-b8400c2f-7b48-4502-b7d5-6c050e557126.png" alt="undefined"> </p>
<h3 id="将-proc-sys-vm-zone-reclaim-mode-改成-1-继续同样的测试"><a href="#将-proc-sys-vm-zone-reclaim-mode-改成-1-继续同样的测试" class="headerlink" title="将 /proc/sys/vm/zone_reclaim_mode 改成 1 继续同样的测试"></a>将 /proc/sys/vm/zone_reclaim_mode 改成 1 继续同样的测试</h3><p>可以看到zone_reclaim_mode 改成 1，node0内存不够了也没有分配node1上的内存，而是从PageCache回收了40G内存，整个分配64G内存的过程也比不回收PageCache慢了12秒，这12秒就是额外的卡顿</p>
<p><img src="/images/951413iMgBlog/1620977108922-a2f67827-cf00-43a0-bba1-4ba105a33201.png" alt="undefined"> </p>
<p>测试结论：<strong>从这个测试可以看到NUMA 在内存使用上不会优先回收 PageCache 了</strong></p>
<h3 id="innodb-numa-interleave"><a href="#innodb-numa-interleave" class="headerlink" title="innodb_numa_interleave"></a>innodb_numa_interleave</h3><p>从5.7开始，mysql增加了对NUMA的无感知：<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_numa_interleave" target="_blank" rel="external">innodb_numa_interleave</a>，也就是在开了NUMA的机器上，使用内错交错来分配内存，相当于使用上关掉 NUMA</p>
<blockquote>
<p>For the <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_numa_interleave" target="_blank" rel="external"><code>innodb_numa_interleave</code></a> option to be available, MySQL must be compiled on a NUMA-enabled Linux system.</p>
</blockquote>
<p>当开启了 innodb_numa_interleave 的话在为innodb buffer pool分配内存的时候将 <a href="https://linux.die.net/man/2/set_mempolicy" target="_blank" rel="external">NUMA memory policy</a> 设置为 MPOL_INTERLEAVE 分配完后再设置回 MPOL_DEFAULT（OS默认内存分配行为，也就是zone_reclaim_mode指定的行为)。</p>
<p>innodb_numa_interleave参数是为innodb更精细化地分配innodb buffer pool 而增加的。很典型地innodb_numa_interleave为on只是更好地规避了前面所说的zone_reclaim_mode的kernel bug，<strong>修复后这个参数没有意义了</strong>。</p>
<h3 id="AUTOMATIC-NUMA-BALANCING"><a href="#AUTOMATIC-NUMA-BALANCING" class="headerlink" title="AUTOMATIC NUMA BALANCING"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-auto_numa_balancing" target="_blank" rel="external">AUTOMATIC NUMA BALANCING</a></h3><p>RedHat 7默认会自动让内存或者进程就近迁移，让内存和CPU距离更近以达到最好的效果</p>
<blockquote>
<p>Automatic NUMA balancing improves the performance of applications running on NUMA hardware systems. It is enabled by default on Red Hat Enterprise Linux 7 systems.</p>
<p>An application will generally perform best when the threads of its processes are accessing memory on the same NUMA node as the threads are scheduled. Automatic NUMA balancing moves tasks (which can be threads or processes) closer to the memory they are accessing. It also moves application data to memory closer to the tasks that reference it. This is all done automatically by the kernel when automatic NUMA balancing is active.</p>
</blockquote>
<p>对应参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat /proc/sys/kernel/numa_balancing shows 1</div></pre></td></tr></table></figure>
<h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><p>查找相应的内存和调度器事件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">perf <span class="built_in">stat</span> -e <span class="built_in">sched</span>:sched_stick_numa,<span class="built_in">sched</span>:sched_move_numa,<span class="built_in">sched</span>:sched_swap_numa,migrate:mm_migrate_pages,minor-faults -p 7191</span></div><div class="line"> Performance counter stats for process id '7191':</div><div class="line"></div><div class="line">                 0      sched:sched_stick_numa                                        (100.00%)</div><div class="line">                 1      sched:sched_move_numa                                         (100.00%)</div><div class="line">                 0      sched:sched_swap_numa</div><div class="line">                 0      migrate:mm_migrate_pages</div><div class="line">               286      minor-faults</div><div class="line">               </div><div class="line"><span class="meta">#</span><span class="bash"> perf <span class="built_in">stat</span> -e <span class="built_in">sched</span>:sched_stick_numa,<span class="built_in">sched</span>:sched_move_numa,<span class="built_in">sched</span>:sched_swap_numa,migrate:mm_migrate_pages,minor-faults -p PID</span></div><div class="line">...</div><div class="line">                 1      sched:sched_stick_numa</div><div class="line">                 3      sched:sched_move_numa</div><div class="line">                41      sched:sched_swap_numa</div><div class="line">             5,239      migrate:mm_migrate_pages</div><div class="line">            50,161      minor-faults</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">perf <span class="built_in">stat</span> -e <span class="built_in">sched</span>:sched_stick_numa,<span class="built_in">sched</span>:sched_move_numa,<span class="built_in">sched</span>:sched_swap_numa,migrate:mm_migrate_pages,minor-faults -p 676322</span></div><div class="line"> Performance counter stats for process id '676322':</div><div class="line"></div><div class="line">                 0      sched:sched_stick_numa</div><div class="line">                16      sched:sched_move_numa</div><div class="line">                 0      sched:sched_swap_numa</div><div class="line">                24      migrate:mm_migrate_pages</div><div class="line">             2,079      minor-faults</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>放弃对NUMA的偏见吧，优先回收 PageCache 这个Bug早已修复了</li>
<li>按NUMA绑定core收益巨大，即使只有两个NUMA的intel芯片，也有一倍以上的性能提升，在飞腾等其他芯片上收益更大</li>
<li>没必要自欺欺人关掉NUMA了</li>
<li>RDS这样独占物理机的服务可以做到按NUMA来绑定core，收益可观</li>
<li>ECS售卖如果能够精确地按NUMA绑核的话性能，超卖比能高很多</li>
<li>在刷tpcc数据的时候更应该开NUMA和正确绑核</li>
</ul>
<p>我个人一直对集团所有机器默认关闭NUMA耿耿于怀，因为定制的物理机（BIOS也是定制的）BIOS默认就是关闭NUMA的，装机还得一台台手工打开（跪了，几十万台啊），算是理清了来龙去脉。因为一个kernel的bug让大家对NUMA一直有偏见，即使14年已经修复了，大家还是以讹传讹，没必要。</p>
<p>关于cpu为什么高但是没有产出的原因是因为CPU流水线长期stall，导致很低的IPC，所以性能自然上不去，可以看<a href="http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html" target="_blank" rel="external">这篇文章</a> </p>
<p>其他同学测试的结论：</p>
<ul>
<li>Hadoop离线作业在 Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz 24 cores/socket * 2, Turbo Off 下打开NUMA后性能提升8%</li>
</ul>
<p>一些其它不好解释的现象：</p>
<ol>
<li>增加少量跨NUMA 的core进来时能增加QPS的，但是随着跨NUMA core越来越多（总core也越来越多）QPS反而会达到一个峰值后下降—效率低的core多了，抢走任务，执行得慢</li>
<li>压12-19和8-15同样8core，不跨NUMA的8-15性能只好5%左右(87873 VS 92801) — 难以解释</li>
<li>由1、2所知在测试少量core的时候跨NUMA性能下降体现不出来</li>
<li>在压0-31core的时候，如果运行 perf这个时候QPS反而会增加（13万上升到15万）— 抢走了一些CPU资源，让某个地方竞争反而减小了</li>
<li>综上在我个人理解是core越多的时候UPI压力到了瓶颈，才会出现加core性能反而下降</li>
</ol>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU的制造和概念/">CPU的制造和概念</a></p>
<p><a href="/2021/05/16/CPU Cache Line 和性能/">CPU 性能和Cache Line</a></p>
<p><a href="/2021/05/16/Perf IPC以及CPU利用率/">Perf IPC以及CPU性能</a></p>
<p><a href="/2021/06/18/几款CPU性能对比/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/05/15/飞腾ARM芯片(FT2500">飞腾ARM芯片(FT2500)的性能测试</a>的性能测试/)</p>
<p><a href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2021/03/07/一次海光物理机资源竞争压测的记录/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.redhat.com/files/summit/session-assets/2018/Performance-analysis-and-tuning-of-Red-Hat-Enterprise-Linux-Part-1.pdf" target="_blank" rel="external">https://www.redhat.com/files/summit/session-assets/2018/Performance-analysis-and-tuning-of-Red-Hat-Enterprise-Linux-Part-1.pdf</a></p>
<p><a href="https://informixdba.wordpress.com/2015/10/16/zone-reclaim-mode/" target="_blank" rel="external">https://informixdba.wordpress.com/2015/10/16/zone-reclaim-mode/</a></p>
<p><a href="https://queue.acm.org/detail.cfm?id=2513149" target="_blank" rel="external">https://queue.acm.org/detail.cfm?id=2513149</a></p>
<p><a href="https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/" target="_blank" rel="external">NUMA DEEP DIVE PART 1: FROM UMA TO NUMA</a> 这是一个系列，都很干货，值得推荐</p>
<p><a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf" target="_blank" rel="external">https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf</a> Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age 论文给出了很多numa-aware下的bandwidth、latency数据，以及对THC-H的影响</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/04/06/为什么这么多CLOSE_WAIT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/04/06/为什么这么多CLOSE_WAIT/" itemprop="url">为什么这么多CLOSE_WAIT</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-04-06T10:30:03+08:00">
                2021-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么这么多CLOSE-WAIT"><a href="#为什么这么多CLOSE-WAIT" class="headerlink" title="为什么这么多CLOSE_WAIT"></a>为什么这么多CLOSE_WAIT</h1><h2 id="案例1：服务响应慢，经常连不上"><a href="#案例1：服务响应慢，经常连不上" class="headerlink" title="案例1：服务响应慢，经常连不上"></a>案例1：服务响应慢，经常连不上</h2><p>应用发布新版本上线后，业务同学发现业务端口上的TCP连接处于CLOSE_WAIT状态的数量有积压，多的时候能堆积到几万个，有时候应用无法响应了</p>
<blockquote>
<p>从这个案例要获取：怎么样才能获取举三反一的秘籍， 普通人为什么要案例来深化对理论知识的理解。</p>
</blockquote>
<h2 id="检查机器状态"><a href="#检查机器状态" class="headerlink" title="检查机器状态"></a>检查机器状态</h2><p><img src="/images/oss/418b94ee-18ee-4976-857b-69f3016af2b0.png" alt="img"></p>
<p><img src="/images/oss/160490c8-56e9-46f2-9c48-713944b94a5c.png" alt="img"></p>
<p>从上述两个图中可以看到磁盘 sdb压力非常大，util经常会到 100%，这个时候对应地从top中也可以看到cpu wait%很高（这个ECS cpu本来竞争很激烈），st%一直非常高，所以整体留给应用的CPU不多，碰上磁盘缓慢的话，这时如果业务写日志是同步刷盘那么就会导致程序卡顿严重。</p>
<p>实际看到FGC的时间也是正常状态下的10倍了。</p>
<p>再看看实际上应用同步写日志到磁盘比较猛，平均20-30M，高的时候能到200M每秒。如果输出的时候磁盘卡住了那么就整个卡死了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">#dstat</div><div class="line">----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--</div><div class="line">usr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw</div><div class="line">  4   1  89   5   0   0|1549M 8533M|   0     0 | 521k  830k|6065k 7134</div><div class="line">  3   1  95   0   0   0|3044k   19M|1765k   85k|   0    84k| 329k 7770</div><div class="line">  5   1  93   0   0   0|3380k   18M|4050k  142k|   0     0 | 300k 8008</div><div class="line">  7   1  91   1   0   1|2788k  227M|5094k  141k|   0    28k| 316k 8644</div><div class="line">  4   1  93   2   0   0|2788k   55M|2897k   63k|   0    68k| 274k 6453</div><div class="line">  6   1  91   1   0   0|4464k   24M|3683k   98k|   0    28k| 299k 7379</div><div class="line">  7   1  91   1   0   0|  10M   34M|3655k  130k|   0   208k| 375k 8417</div><div class="line">  3   1  87   8   0   0|6940k   33M|1335k   91k|   0   112k| 334k 7369</div><div class="line">  3   1  88   7   0   0|4932k   16M|1918k   61k|   0    44k| 268k 6542</div><div class="line">  7   1  86   6   0   0|5508k   20M|5377k  111k|   0     0 | 334k 7998</div><div class="line">  7   2  88   3   0   0|5628k  115M|4713k  104k|   0     0 | 280k 7392</div><div class="line">  4   1  95   0   0   0|   0   732k|2940k   85k|   0    76k| 189k 7682</div><div class="line">  3   1  96   0   0   0|   0   800k|1809k   68k|   0    16k| 181k 9640</div><div class="line">  7   2  76  14   0   1|6300k   38M|3834k  132k|   0     0 | 333k 7502</div><div class="line">  7   2  90   1   0   0|3896k   19M|3786k   93k|   0     0 | 357k 7578</div><div class="line">  4   1  94   0   0   0|5732k   29M|2906k  806k|   0     0 | 338k 8966</div><div class="line">  4   1  94   1   0   0|6044k   17M|2202k   95k|   0     0 | 327k 7573</div><div class="line">  4   1  95   1   0   0|3524k   17M|2277k   88k|   0     0 | 299k 6462</div><div class="line">  4   1  96   0   0   0| 456k   14M|2770k   91k|  60k    0 | 252k 6644</div><div class="line">  6   2  92   0   0   0|   0    12M|4251k  847k|   0     0 | 264k   10k</div><div class="line">  3   1  92   4   0   0| 788k  204M|1555k   43k|   0     0 | 249k 6215</div><div class="line">  6   1  86   6   0   0|7180k   20M|2073k   92k|   0     0 | 303k 7028</div><div class="line"> 11   4  84   1   0   0|6116k   29M|3079k   99k|  28k    0 | 263k 6605</div></pre></td></tr></table></figure>
<p>磁盘util 100%和CLOSE_WAIT强相关，也和理论比较符合，CLOSE_WAIT就是连接被动关闭端的应用没调socket.close</p>
<p><img src="/images/oss/3b7dedca-1c79-4317-8042-bb9ba8c957b9.png" alt="img"></p>
<p>大概的原因推断是：</p>
<p>1）新发布的代码需要消耗更多的CPU，代码增加了新的逻辑 //这只是一个微小的诱因</p>
<p>2）机器本身资源(CPU /IO）很紧张 这两个条件下导致应用响应缓慢。 目前看到的稳定重现条件就是重启一个业务节点，重启会触发业务节点之间重新同步数据，以及重新推送很多数据到客户端的新连接上，这两件事情都会让应用CPU占用飙升响应缓慢，响应慢了之后会导致更多的心跳失效进一步加剧数据同步，然后就雪崩恶化了。最后表现就是看到系统卡死了，也就是tcp buffer中的数据也不读走、连接也不close，连接大量堆积在close_wait状态</p>
<p><img src="/images/oss/227c69f1-0467-425c-a19d-26c03d50c36c.png" alt="img"></p>
<p>CLOSE_WAIT的原因分析</p>
<h2 id="先看TCP连接状态图"><a href="#先看TCP连接状态图" class="headerlink" title="先看TCP连接状态图"></a>先看TCP连接状态图</h2><p>这是网络、书本上凡是描述TCP状态一定会出现的状态图，理论上看这个图能解决任何TCP状态问题。</p>
<p><img src="/images/951413iMgBlog/b3d075782450b0c8d2615c5d2b75d923.png" alt="image.png"></p>
<p>反复看这个图的右下部分的CLOSE_WAIT ，从这个图里可以得到如下结论：</p>
<blockquote>
<p><strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong></p>
</blockquote>
<p>基本上这一结论要能帮助解决所有CLOSE_WAIT相关的问题，如果不能说明对这个知识点理解的不够。</p>
<h2 id="案例1结论"><a href="#案例1结论" class="headerlink" title="案例1结论"></a>案例1结论</h2><p>机器超卖严重、IO卡顿，导致应用线程卡顿，来不及调用socket.close()</p>
<h2 id="案例2：server端大量close-wait"><a href="#案例2：server端大量close-wait" class="headerlink" title="案例2：server端大量close_wait"></a>案例2：server端大量close_wait</h2><p>用实际案例来检查自己对CLOSE_WAIT 理论（<strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong>）的掌握 – 能不能用这个结论来解决实际问题。同时也可以看看自己从知识到问题的推理能力（跟前面的知识效率呼应一下）。</p>
<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><blockquote>
<p>服务端出现大量CLOSE_WAIT ，并且个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
</blockquote>
<p>根据这个描述先不要往下看，自己推理分析下可能的原因。</p>
<p>我的推理如下：</p>
<p>从这里看起来，client跟server成功建立了somaxconn个连接（somaxconn小于backlog，所以accept queue只有这么大），但是应用没有accept这个连接，导致这些连接一直在accept queue中。但是这些连接的状态已经是ESTABLISHED了，也就是client可以发送数据了，数据发送到server后OS ack了，并放在os的tcp buffer中，应用一直没有accept也就没法读取数据。client于是发送fin（可能是超时、也可能是简单发送数据任务完成了得结束连接），这时Server上这个连接变成了CLOSE_WAIT .</p>
<p>也就是从开始到结束这些连接都在accept queue中，没有被应用accept，很快他们又因为client 发送 fin 包变成了CLOSE_WAIT ，所以始终看到的是服务端出现大量CLOSE_WAIT 并且个数正好等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）。</p>
<p>如下图所示，在连接进入accept queue后状态就是ESTABLISED了，也就是可以正常收发数据和fin了。client是感知不到server是否accept()了，只是发了数据后server的os代为保存在OS的TCP buffer中，因为应用没来取自然在CLOSE_WAIT 后应用也没有close()，所以一直维持CLOSE_WAIT 。</p>
<p>得检查server 应用为什么没有accept。</p>
<p><img src="/images/951413iMgBlog/20190706093602331.png" alt="Recv-Q和Send-Q"></p>
<p>如上是老司机的思路靠经验缺省了一些理论推理，缺省还是对理论理解不够， 这个分析抓住了 大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）但是没有抓住 CLOSE_WAIT 背后的核心原因</p>
<h3 id="更简单的推理"><a href="#更简单的推理" class="headerlink" title="更简单的推理"></a>更简单的推理</h3><p>如果没有任何实战经验，只看上面的状态图的学霸应该是这样推理的：</p>
<p>看到server上有大量的CLOSE_WAIT说明client主动断开了连接，server的OS收到client 发的fin，并回复了ack，这个过程不需要应用感知，进而连接从ESTABLISHED进入CLOSE_WAIT，此时在等待server上的应用调用close连关闭连接（处理完所有收发数据后才会调close()） —- 结论：server上的应用一直卡着没有调close().</p>
<h2 id="CLOSE-WAIT-状态拆解"><a href="#CLOSE-WAIT-状态拆解" class="headerlink" title="CLOSE_WAIT 状态拆解"></a>CLOSE_WAIT 状态拆解</h2><p>通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：</p>
<ul>
<li><strong>程序问题</strong>：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。</li>
<li>响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。</li>
<li>BACKLOG 太大：此处的 backlog 不是 syn backlog，而是 accept 的 backlog，如果 backlog 太大的话，设想突然遭遇大访问量的话，即便响应速度不慢，也可能出现来不及消费的情况，导致多余的请求还在<a href="http://jaseywang.me/2014/07/20/tcp-queue-的一些问题/" target="_blank" rel="external">队列</a>里就被对方关闭了。</li>
</ul>
<p>如果你通过「netstat -ant」或者「ss -ant」命令发现了很多 CLOSE_WAIT 连接，请注意结果中的「Recv-Q」和「Local Address」字段，通常「Recv-Q」会不为空，它表示应用还没来得及接收数据，而「Local Address」表示哪个地址和端口有问题，我们可以通过「lsof -i:<port>」来确认端口对应运行的是什么程序以及它的进程号是多少。</port></p>
<p>如果是我们自己写的一些程序，比如用 HttpClient 自定义的蜘蛛，那么八九不离十是程序问题，如果是一些使用广泛的程序，比如 Tomcat 之类的，那么更可能是响应速度太慢或者 timeout 设置太小或者 BACKLOG 设置过大导致的故障。</p>
<p>看完这段 CLOSE_WAIT 更具体深入点的分析后再来分析上面的案例看看，能否推导得到正确的结论。</p>
<h2 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h2><h3 id="连接都没有被accept-client端就能发送数据了？"><a href="#连接都没有被accept-client端就能发送数据了？" class="headerlink" title="连接都没有被accept(), client端就能发送数据了？"></a>连接都没有被accept(), client端就能发送数据了？</h3><p>答：是的。只要这个连接在OS看来是ESTABLISHED的了就可以，因为握手、接收数据都是由内核完成的，内核收到数据后会先将数据放在内核的tcp buffer中，然后os回复ack。另外三次握手之后client端是没法知道server端是否accept()了。</p>
<h3 id="CLOSE-WAIT与accept-queue有关系吗？"><a href="#CLOSE-WAIT与accept-queue有关系吗？" class="headerlink" title="CLOSE_WAIT与accept queue有关系吗？"></a>CLOSE_WAIT与accept queue有关系吗？</h3><p>答：没有关系。只是本案例中因为open files不够了，影响了应用accept(), 导致accept queue满了，同时因为即使应用不accept（三次握手后，server端是否accept client端无法感知），client也能发送数据和发 fin断连接，这些响应都是os来负责，跟上层应用没关系，连接从握手到ESTABLISHED再到CLOSE_WAIT都不需要fd，也不需要应用参与。CLOSE_WAIT只跟应用不调 close() 有关系。 </p>
<h3 id="CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？"><a href="#CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？" class="headerlink" title="CLOSE_WAIT与accept queue为什么刚好一致并且联动了？"></a>CLOSE_WAIT与accept queue为什么刚好一致并且联动了？</h3><p>答：这里他们的数量刚好一致是因为所有新建连接都没有accept，堵在queue中。同时client发现问题后把所有连接都fin了，也就是所有queue中的连接从来没有被accept过，但是他们都是ESTABLISHED，过一阵子之后client端发了fin所以所有accept queue中的连接又变成了 CLOSE_WAIT, 所以二者刚好一致并且联动了</p>
<h3 id="CLOSE-WAIT与TIME-WAIT"><a href="#CLOSE-WAIT与TIME-WAIT" class="headerlink" title="CLOSE_WAIT与TIME_WAIT"></a>CLOSE_WAIT与TIME_WAIT</h3><p>简单说就是CLOSE_WAIT出现在被动断开连接端，一般过多就不太正常；TIME_WAIT出现在主动断开连接端，是正常现象，多出现在短连接场景下</p>
<h3 id="openfiles和accept-的关系是？"><a href="#openfiles和accept-的关系是？" class="headerlink" title="openfiles和accept()的关系是？"></a>openfiles和accept()的关系是？</h3><p>答：accept()的时候才会创建文件句柄，消耗openfiles</p>
<h3 id="一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"><a href="#一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？" class="headerlink" title="一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"></a>一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？</h3><p>答：是</p>
<h3 id="如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？"><a href="#如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？" class="headerlink" title="如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？"></a>如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？</h3><p>答：对</p>
<h3 id="如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？"><a href="#如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？" class="headerlink" title="如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？"></a>如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？</h3><p>答： 能，这个时候在client和server两边看到的连接状态都是 ESTABLISHED，只是Server上的全连接队列占用加1。连接握手并切换到ESTABLISHED状态都是由OS来负责的，应用不参与，ESTABLISHED后应用才能accept，进而收发数据。也就是能放入到全连接队列里面的连接肯定都是 ESTABLISHED 状态的了</p>
<h3 id="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"><a href="#接着上面的问题，如果新连接继续连接进而全连接队列满了呢？" class="headerlink" title="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"></a>接着上面的问题，如果新连接继续连接进而全连接队列满了呢？</h3><p>答：那就连不上了，server端的OS因为全连接队列满了直接扔掉第一个syn握手包，这个时候连接在client端是SYN_SENT，Server端没有这个连接，这是因为syn到server端就直接被OS drop 了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">//如下图，本机测试，只有一个client端发起的syn_send, 3306的server端没有任何连接</div><div class="line">$netstat -antp  |grep -i 127.0.0.1:3306</div><div class="line">tcp     0   1 127.0.0.1:61106      127.0.0.1:3306    SYN_SENT    21352/telnet</div></pre></td></tr></table></figure>
<p>能进入到accept queue中的连接都是 ESTABLISHED，不管用户态有没有accept，用户态accept后队列大小减1</p>
<h3 id="如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？"><a href="#如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？" class="headerlink" title="如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？"></a>如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？</h3><p>答： 如果此时收到对方的RESET了，那么OS会释放这个连接。但是内核认为所有 listen 到的连接, 必须要 accept 走, 因为用户有权利知道有过这么一个连接存在过。所以OS不会到全连接队列拿掉这个连接，全连接队列数量也不会减1，直到应用accept这个连接，然后read/write才发现这个连接断开了，报communication failure异常</p>
<h3 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h3><p>三次握手成功就变成 ESTABLISHED 了，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>这是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</div><div class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </div><div class="line">LISTEN      73     1024            *:terabase                 *:*</div></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;  </div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</div><div class="line"># ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </div><div class="line">LISTEN      1025   1024             *:terabase              *:*</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/03/21/Intel AMD 鲲鹏 海光 飞腾性能PK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/21/Intel AMD 鲲鹏 海光 飞腾性能PK/" itemprop="url">Intel AMD 鲲鹏 海光 飞腾性能PK</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-21T17:30:03+08:00">
                2021-03-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Intel-AMD-鲲鹏-海光-飞腾性能PK"><a href="#Intel-AMD-鲲鹏-海光-飞腾性能PK" class="headerlink" title="Intel AMD 鲲鹏 海光 飞腾性能PK"></a>Intel AMD 鲲鹏 海光 飞腾性能PK</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文在sysbench、tpcc等实践场景下对多款CPU的性能进行对比，同时分析各款CPU的硬件指标，最后分析不同场景下的实际性能和核心参数的关系。</p>
<p>本文的姊妹篇：<a href="https://zhuanlan.zhihu.com/p/387117470" target="_blank" rel="external">十年后数据库还是不敢拥抱NUMA？</a> 主要讲述的是 同一块CPU的不同NUMA结构配置带来的几倍性能差异，这些性能差异原因也可以从本文最后时延测试数据得到印证，一起阅读效果更好。</p>
<h2 id="性能定义"><a href="#性能定义" class="headerlink" title="性能定义"></a>性能定义</h2><p>同一个平台下（X86、ARM是两个平台）编译好的程序可以认为他们的 <strong>指令</strong> 数是一样的，那么执行效率就是每个时钟周期所能执行的指令数量了。</p>
<p>执行指令数量第一取决的就是CPU主频了，但是目前主流CPU都是2.5G左右，另外就是单核下的并行度（多发射）以及多核，再就是分支预测等，这些基本归结到了访问内存的延时。</p>
<p>X86和ARM这两不同平台首先指令就不一样了，然后还有上面所说的主频、内存时延的差异</p>
<p>IPC的说明：</p>
<blockquote>
<p>IPC: insns per cycle  insn/cycles  也就是每个时钟周期能执行的指令数量，越大程序跑的越快</p>
<p>程序的执行时间 = 指令数/(主频*IPC) //单核下，多核的话再除以核数</p>
</blockquote>
<h2 id="参与比较的几款CPU参数"><a href="#参与比较的几款CPU参数" class="headerlink" title="参与比较的几款CPU参数"></a>参与比较的几款CPU参数</h2><p>先来看看测试所用到的几款CPU的主要指标，大家关注下主频、各级cache大小、<a href="https://zhuanlan.zhihu.com/p/387117470" target="_blank" rel="external">numa结构</a></p>
<h3 id="Hygon-7280"><a href="#Hygon-7280" class="headerlink" title="Hygon 7280"></a>Hygon 7280</h3><p>Hygon 7280 就是AMD Zen架构，最大IPC能到5.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">架构：                           x86_64</div><div class="line">CPU 运行模式：                   32-bit, 64-bit</div><div class="line">字节序：                         Little Endian</div><div class="line">Address sizes:                   43 bits physical, 48 bits virtual</div><div class="line">CPU:                             128</div><div class="line">在线 CPU 列表：                  0-127</div><div class="line">每个核的线程数：                 2</div><div class="line">每个座的核数：                   32</div><div class="line">座：                             2</div><div class="line">NUMA 节点：                      8</div><div class="line">厂商 ID：                        HygonGenuine</div><div class="line">CPU 系列：                       24</div><div class="line">型号：                           1</div><div class="line">型号名称：                       Hygon C86 7280 32-core Processor</div><div class="line">步进：                           1</div><div class="line">CPU MHz：                        2194.586</div><div class="line">BogoMIPS：                       3999.63</div><div class="line">虚拟化：                         AMD-V</div><div class="line">L1d 缓存：                       2 MiB  </div><div class="line">L1i 缓存：                       4 MiB</div><div class="line">L2 缓存：                        32 MiB</div><div class="line">L3 缓存：                        128 MiB</div><div class="line">NUMA 节点0 CPU：                 0-7,64-71</div><div class="line">NUMA 节点1 CPU：                 8-15,72-79</div><div class="line">NUMA 节点2 CPU：                 16-23,80-87</div><div class="line">NUMA 节点3 CPU：                 24-31,88-95</div><div class="line">NUMA 节点4 CPU：                 32-39,96-103</div><div class="line">NUMA 节点5 CPU：                 40-47,104-111</div><div class="line">NUMA 节点6 CPU：                 48-55,112-119</div><div class="line">NUMA 节点7 CPU：                 56-63,120-127</div></pre></td></tr></table></figure>
<h3 id="AMD-EPYC-7H12"><a href="#AMD-EPYC-7H12" class="headerlink" title="AMD EPYC 7H12"></a>AMD EPYC 7H12</h3><p>AMD EPYC 7H12 64-Core（ECS，非物理机），最大IPC能到5.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> lscpu</span></div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                64</div><div class="line">On-line CPU(s) list:   0-63</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    16</div><div class="line">座：                 2</div><div class="line">NUMA 节点：         2</div><div class="line">厂商 ID：           AuthenticAMD</div><div class="line">CPU 系列：          23</div><div class="line">型号：              49</div><div class="line">型号名称：        AMD EPYC 7H12 64-Core Processor</div><div class="line">步进：              0</div><div class="line">CPU MHz：             2595.124</div><div class="line">BogoMIPS：            5190.24</div><div class="line">虚拟化：           AMD-V</div><div class="line">超管理器厂商：  KVM</div><div class="line">虚拟化类型：     完全</div><div class="line">L1d 缓存：          32K</div><div class="line">L1i 缓存：          32K</div><div class="line">L2 缓存：           512K</div><div class="line">L3 缓存：           16384K</div><div class="line">NUMA 节点0 CPU：    0-31</div><div class="line">NUMA 节点1 CPU：    32-63</div></pre></td></tr></table></figure>
<h3 id="Intel"><a href="#Intel" class="headerlink" title="Intel"></a>Intel</h3><p>这次对比测试用到了两块Intel CPU，分别是 8163、8269 。他们的信息如下，最大IPC 是4：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">lscpu</span></div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                96</div><div class="line">On-line CPU(s) list:   0-95</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    24</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          1</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 85</div><div class="line">Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz</div><div class="line">Stepping:              4</div><div class="line">CPU MHz:               2499.121</div><div class="line">CPU max MHz:           3100.0000</div><div class="line">CPU min MHz:           1000.0000</div><div class="line">BogoMIPS:              4998.90</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              1024K</div><div class="line">L3 cache:              33792K</div><div class="line">NUMA node0 CPU(s):     0-95   </div><div class="line"></div><div class="line">-----8269CY</div><div class="line"><span class="meta">#</span><span class="bash">lscpu</span></div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                104</div><div class="line">On-line CPU(s) list:   0-103</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    26</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 85</div><div class="line">Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</div><div class="line">Stepping:              7</div><div class="line">CPU MHz:               3200.000</div><div class="line">CPU max MHz:           3800.0000</div><div class="line">CPU min MHz:           1200.0000</div><div class="line">BogoMIPS:              4998.89</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              1024K</div><div class="line">L3 cache:              36608K</div><div class="line">NUMA node0 CPU(s):     0-25,52-77</div><div class="line">NUMA node1 CPU(s):     26-51,78-103</div></pre></td></tr></table></figure>
<h3 id="鲲鹏920"><a href="#鲲鹏920" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">[root@ARM 19:15 /root/lmbench3]</div><div class="line">#numactl -H</div><div class="line">available: 4 nodes (0-3)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</div><div class="line">node 0 size: 192832 MB</div><div class="line">node 0 free: 146830 MB</div><div class="line">node 1 cpus: 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47</div><div class="line">node 1 size: 193533 MB</div><div class="line">node 1 free: 175354 MB</div><div class="line">node 2 cpus: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71</div><div class="line">node 2 size: 193533 MB</div><div class="line">node 2 free: 175718 MB</div><div class="line">node 3 cpus: 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</div><div class="line">node 3 size: 193532 MB</div><div class="line">node 3 free: 183643 MB</div><div class="line">node distances:</div><div class="line">node   0   1   2   3</div><div class="line">  0:  10  12  20  22</div><div class="line">  1:  12  10  22  24</div><div class="line">  2:  20  22  10  12</div><div class="line">  3:  22  24  12  10</div><div class="line"></div><div class="line">  #lscpu</div><div class="line">Architecture:          aarch64</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                96</div><div class="line">On-line CPU(s) list:   0-95</div><div class="line">Thread(s) per core:    1</div><div class="line">Core(s) per socket:    48</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          4</div><div class="line">Model:                 0</div><div class="line">CPU max MHz:           2600.0000</div><div class="line">CPU min MHz:           200.0000</div><div class="line">BogoMIPS:              200.00</div><div class="line">L1d cache:             64K</div><div class="line">L1i cache:             64K</div><div class="line">L2 cache:              512K</div><div class="line">L3 cache:              24576K //一个Die下24core共享24M L3，每个core 1MB</div><div class="line">NUMA node0 CPU(s):     0-23</div><div class="line">NUMA node1 CPU(s):     24-47</div><div class="line">NUMA node2 CPU(s):     48-71</div><div class="line">NUMA node3 CPU(s):     72-95</div><div class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</div></pre></td></tr></table></figure>
<h3 id="飞腾2500"><a href="#飞腾2500" class="headerlink" title="飞腾2500"></a>飞腾2500</h3><p>飞腾2500用nop去跑IPC的话，只能到1，但是跑其它代码能到2.33，理论值据说也是4但是我没跑到过</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">lscpu</span></div><div class="line">Architecture:          aarch64</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                128</div><div class="line">On-line CPU(s) list:   0-127</div><div class="line">Thread(s) per core:    1</div><div class="line">Core(s) per socket:    64</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          16</div><div class="line">Model:                 3</div><div class="line">BogoMIPS:              100.00</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              2048K</div><div class="line">L3 cache:              65536K</div><div class="line">NUMA node0 CPU(s):     0-7</div><div class="line">NUMA node1 CPU(s):     8-15</div><div class="line">NUMA node2 CPU(s):     16-23</div><div class="line">NUMA node3 CPU(s):     24-31</div><div class="line">NUMA node4 CPU(s):     32-39</div><div class="line">NUMA node5 CPU(s):     40-47</div><div class="line">NUMA node6 CPU(s):     48-55</div><div class="line">NUMA node7 CPU(s):     56-63</div><div class="line">NUMA node8 CPU(s):     64-71</div><div class="line">NUMA node9 CPU(s):     72-79</div><div class="line">NUMA node10 CPU(s):    80-87</div><div class="line">NUMA node11 CPU(s):    88-95</div><div class="line">NUMA node12 CPU(s):    96-103</div><div class="line">NUMA node13 CPU(s):    104-111</div><div class="line">NUMA node14 CPU(s):    112-119</div><div class="line">NUMA node15 CPU(s):    120-127</div><div class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</div></pre></td></tr></table></figure>
<h2 id="单核以及超线程计算Prime性能比较"><a href="#单核以及超线程计算Prime性能比较" class="headerlink" title="单核以及超线程计算Prime性能比较"></a>单核以及超线程计算Prime性能比较</h2><p>测试命令，这个测试命令无论在哪个CPU下，用2个物理核用时都是一个物理核的一半，所以这个计算是可以完全并行的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">taskset -c 1 /usr/bin/sysbench --num-threads=1 --test=cpu --cpu-max-prime=50000 run //单核绑一个core; 2个thread就绑一对HT</div></pre></td></tr></table></figure>
<p>测试结果为耗时，单位秒</p>
<table style="width:100%;"><br><colgroup><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br></colgroup><br><tr class="header"><br><th style="text-align: left;">测试项</th><br><th>AMD EPYC 7H12 2.5G CentOS 7.9</th><br><th>Hygon 7280 2.1GHz CentOS</th><br><th style="text-align: left;">Hygon 7280 2.1GHz 麒麟</th><br><th>Intel 8269 2.50G</th><br><th style="text-align: left;">Intel 8163 CPU @ 2.50GHz</th><br><th style="text-align: left;">Intel E5-2682 v4 @ 2.50GHz</th><br></tr><br><tr class="odd"><br><td style="text-align: left;">单核 prime 50000 耗时</td><br><td>59秒 IPC 0.56</td><br><td>77秒 IPC 0.55</td><br><td style="text-align: left;">89秒 IPC 0.56;</td><br><td>83 0.41</td><br><td style="text-align: left;">105秒 IPC 0.41</td><br><td style="text-align: left;">109秒 IPC 0.39</td><br></tr><br><tr class="even"><br><td style="text-align: left;">HT prime 50000 耗时</td><br><td>57秒 IPC 0.31</td><br><td>74秒 IPC 0.29</td><br><td style="text-align: left;">87秒 IPC 0.29</td><br><td>48 0.35</td><br><td style="text-align: left;">60秒 IPC 0.36</td><br><td style="text-align: left;">74秒 IPC 0.29</td><br></tr><br></table>

<p>从上面的测试结果来看，简单纯计算场景下 AMD/海光 的单核能力还是比较强的，但是超线程完全不给力（数据库场景超线程就给力了）；而Intel的超线程非常给力，一对超线程能达到单物理core的1.8倍，并且从E5到8269更是好了不少。<br>ARM基本都没有超线程所有没有跑鲲鹏、飞腾。</p>
<p>计算Prime毕竟太简单，让我们来看看他们在数据库下的真实能力吧</p>
<h2 id="对比MySQL-sysbench和tpcc性能"><a href="#对比MySQL-sysbench和tpcc性能" class="headerlink" title="对比MySQL sysbench和tpcc性能"></a>对比MySQL sysbench和tpcc性能</h2><p>MySQL 默认用5.7.34社区版，操作系统默认是centos，测试中所有mysqld都做了绑核，一样的压力配置尽量将CPU跑到100%， HT表示将mysqld绑定到一对HT核。</p>
<h3 id="sysbench点查"><a href="#sysbench点查" class="headerlink" title="sysbench点查"></a>sysbench点查</h3><p>测试命令类似如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysbench --test=&apos;/usr/share/doc/sysbench/tests/db/select.lua&apos; --oltp_tables_count=1 --report-interval=1 --oltp-table-size=10000000  --mysql-port=3307 --mysql-db=sysbench_single --mysql-user=root --mysql-password=&apos;Bj6f9g96!@#&apos;  --max-requests=0   --oltp_skip_trx=on --oltp_auto_inc=on  --oltp_range_size=5  --mysql-table-engine=innodb --rand-init=on   --max-time=300 --mysql-host=x86.51 --num-threads=4 run</div></pre></td></tr></table></figure>
<p>测试结果分别取QPS/IPC两个数据(测试中的差异AMD、Hygon CPU跑在CentOS7.9， intel CPU、Kunpeng 920 跑在AliOS上, xdb表示用集团的xdb替换社区的MySQL Server， 麒麟是国产OS)：</p>
<table style="width:100%;"><br><colgroup><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br><col style="width: 10%"><br></colgroup><br><tr class="header"><br><th style="text-align: left;">测试核数</th><br><th>AMD EPYC 7H12 2.5G</th><br><th style="text-align: left;">Hygon 7280 2.1G</th><br><th>Hygon 7280 2.1GHz 麒麟</th><br><th>Intel 8269 2.50G</th><br><th style="text-align: left;">Intel 8163 2.50G</th><br><th style="text-align: left;">Intel 8163 2.50G XDB5.7</th><br><th>鲲鹏 920-4826 2.6G</th><br><th>鲲鹏 920-4826 2.6G XDB8.0</th><br><th>FT2500 alisql 8.0 本地–socket</th><br></tr><br><tr class="odd"><br><td style="text-align: left;">单核</td><br><td>24674 0.54</td><br><td style="text-align: left;">13441 0.46</td><br><td>10236 0.39</td><br><td>28208 0.75</td><br><td style="text-align: left;">25474 0.84</td><br><td style="text-align: left;">29376 0.89</td><br><td>9694 0.49</td><br><td>8301 0.46</td><br><td>3602 0.53</td><br></tr><br><tr class="even"><br><td style="text-align: left;">一对HT</td><br><td>36157 0.42</td><br><td style="text-align: left;">21747 0.38</td><br><td>19417 0.37</td><br><td>36754 0.49</td><br><td style="text-align: left;">35894 0.6</td><br><td style="text-align: left;">40601 0.65</td><br><td>无HT</td><br><td>无HT</td><br><td>无HT</td><br></tr><br><tr class="odd"><br><td style="text-align: left;">4物理核</td><br><td>94132 0.52</td><br><td style="text-align: left;">49822 0.46</td><br><td>38033 0.37</td><br><td>90434 0.69 350%</td><br><td style="text-align: left;">87254 0.73</td><br><td style="text-align: left;">106472 0.83</td><br><td>34686 0.42</td><br><td>28407 0.39</td><br><td>14232 0.53</td><br></tr><br><tr class="even"><br><td style="text-align: left;">16物理核</td><br><td>325409 0.48</td><br><td style="text-align: left;">171630 0.38</td><br><td>134980 0.34</td><br><td>371718 0.69 1500%</td><br><td style="text-align: left;">332967 0.72</td><br><td style="text-align: left;">446290 0.85 //16核比4核好！</td><br><td>116122 0.35</td><br><td>94697 0.33</td><br><td>59199 0.6 8core:31210 0.59</td><br></tr><br><tr class="odd"><br><td style="text-align: left;">32物理核</td><br><td>542192 0.43</td><br><td style="text-align: left;">298716 0.37</td><br><td>255586 0.33</td><br><td>642548 0.64 2700%</td><br><td style="text-align: left;">588318 0.67</td><br><td style="text-align: left;">598637 0.81 CPU 2400%</td><br><td>228601 0.36</td><br><td>177424 0.32</td><br><td>114020 0.65</td><br></tr><br></table>


<p>说明：麒麟OS下CPU很难跑满，大致能跑到90%-95%左右，麒麟上装的社区版MySQL-5.7.29；飞腾要特别注意mysqld所在socket，同时以上飞腾数据都是走–socket压测锁的，32core走网络压测QPS为：99496（15%的网络损耗）</p>
<p>从上面的结果先看单物理核能力ARM 和 X86之间的差异还是很明显的</p>
<h3 id="tpcc-1000仓"><a href="#tpcc-1000仓" class="headerlink" title="tpcc 1000仓"></a>tpcc 1000仓</h3><p>测试结果(测试中Hygon 7280分别跑在CentOS7.9和麒麟上， 鲲鹏/intel CPU 跑在AliOS、麒麟是国产OS)：</p>
<p>tpcc测试数据，结果为1000仓，tpmC (NewOrders) ，未标注CPU 则为跑满了</p>
<table style="width:100%;"><br><colgroup><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br><col style="width: 14%"><br></colgroup><br><tr class="header"><br><th>测试核数</th><br><th>Intel 8269 2.50G</th><br><th>Intel 8163 2.50G</th><br><th>Hygon 7280 2.1GHz 麒麟</th><br><th>Hygon 7280 2.1G CentOS 7.9</th><br><th>鲲鹏 920-4826 2.6G</th><br><th>鲲鹏 920-4826 2.6G XDB8.0</th><br></tr><br><tr class="odd"><br><td>1物理核</td><br><td>12392</td><br><td>9902</td><br><td>4706</td><br><td>7011</td><br><td>6619</td><br><td>4653</td><br></tr><br><tr class="even"><br><td>一对HT</td><br><td>17892</td><br><td>15324</td><br><td>8950</td><br><td>11778</td><br><td>无HT</td><br><td>无HT</td><br></tr><br><tr class="odd"><br><td>4物理核</td><br><td>51525</td><br><td>40877</td><br><td>19387 380%</td><br><td>30046</td><br><td>23959</td><br><td>20101</td><br></tr><br><tr class="even"><br><td>8物理核</td><br><td>100792</td><br><td>81799</td><br><td>39664 750%</td><br><td>60086</td><br><td>42368</td><br><td>40572</td><br></tr><br><tr class="odd"><br><td>16物理核</td><br><td>160798 抖动</td><br><td>140488 CPU抖动</td><br><td>75013 1400%</td><br><td>106419 1300-1550%</td><br><td>70581 1200%</td><br><td>79844</td><br></tr><br><tr class="even"><br><td>24物理核</td><br><td>188051</td><br><td>164757 1600-2100%</td><br><td>100841 1800-2000%</td><br><td>130815 1600-2100%</td><br><td>88204 1600%</td><br><td>115355</td><br></tr><br><tr class="odd"><br><td>32物理核</td><br><td>195292</td><br><td>185171 2000-2500%</td><br><td>116071 1900-2400%</td><br><td>142746 1800-2400%</td><br><td>102089 1900%</td><br><td>143567</td><br></tr><br><tr class="even"><br><td>48物理核</td><br><td>19969l</td><br><td>195730 2100-2600%</td><br><td>128188 2100-2800%</td><br><td>149782 2000-2700%</td><br><td>116374 2500%</td><br><td>206055 4500%</td><br></tr><br></table>

<p>测试过程CPU均跑满（未跑满的话会标注出来），IPC跑不起来性能就必然低，超线程虽然总性能好了但是会导致IPC降低(参考前面的公式)。可以看到对本来IPC比较低的场景，启用超线程后一般对性能会提升更大一些。</p>
<p>tpcc并发到一定程度后主要是锁导致性能上不去，所以超多核意义不大，可以做<a href="https://www.aliyun.com/product/drds?spm=5176.19720258.J_8058803260.33.e9392c4aRo26Yg" target="_blank" rel="external">分库分表搞多个mysqld实例</a></p>
<p>比如在Hygon 7280 2.1GHz 麒麟上起两个MySQLD实例，每个实例各绑定32物理core，性能刚好翻倍：<img src="/images/951413iMgBlog/image-20210823082702539-7857743.png" alt="image-20210823082702539"></p>
<p>32核的时候对比下MySQL 社区版在Hygon7280和Intel 8163下的表现，IPC的差异还是很明显的，基本和TPS差异一致：</p>
<p><img src="/images/951413iMgBlog/image-20210817181752243-7857743.png" alt="image-20210817181752243"></p>
<p>从sysbench和tpcc测试结果来看AMD和Intel差异不大，ARM和X86差异比较大，国产CPU还有很大的进步空间。就像前面所说抛开指令集的差异，主频差不多，内存管够为什么还有这么大的性能差别呢？</p>
<h2 id="三款CPU的性能指标"><a href="#三款CPU的性能指标" class="headerlink" title="三款CPU的性能指标"></a>三款CPU的性能指标</h2><p>下面让我们回到硬件本身的数据来看这个问题</p>
<p>先记住这个图，描述的是CPU访问寄存器、L1 cache、L2 cache等延时，关键记住他们的差异<br><img src="/images/951413iMgBlog/1647855571774-4dfa1be7-7d3b-412b-9aa7-44c68656aed6.png" alt="各级延时"></p>
<p>接下来用<a href="https://github.com/intel/lmbench" target="_blank" rel="external">lmbench</a>来测试各个机器的内存延时</p>
<p>stream主要用于测试带宽，对应的时延是在带宽跑满情况下的带宽。</p>
<p>lat_mem_rd用来测试操作不同数据大小的时延。</p>
<h3 id="飞腾2500-1"><a href="#飞腾2500-1" class="headerlink" title="飞腾2500"></a>飞腾2500</h3><p>用stream测试带宽和latency，可以看到带宽随着numa距离不断减少、对应的latency不断增加，到最近的numa node有10%的损耗，这个损耗和numactl给出的距离完全一致。跨socket访问内存latency是node内的3倍，带宽是三分之一，但是socket1性能和socket0性能完全一致。从这个延时来看如果要是跑一个32core的实例性能一定不会太好，并且抖动剧烈</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">time for i in $(seq 7 8 128); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</div><div class="line"></div><div class="line">#numactl -C 7 -m 0 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 2.84 nanoseconds</div><div class="line">STREAM copy bandwidth: 5638.21 MB/sec</div><div class="line">STREAM scale latency: 2.72 nanoseconds</div><div class="line">STREAM scale bandwidth: 5885.97 MB/sec</div><div class="line">STREAM add latency: 2.26 nanoseconds</div><div class="line">STREAM add bandwidth: 10615.13 MB/sec</div><div class="line">STREAM triad latency: 4.53 nanoseconds</div><div class="line">STREAM triad bandwidth: 5297.93 MB/sec</div><div class="line"></div><div class="line">#numactl -C 7 -m 1 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 3.16 nanoseconds</div><div class="line">STREAM copy bandwidth: 5058.71 MB/sec</div><div class="line">STREAM scale latency: 3.15 nanoseconds</div><div class="line">STREAM scale bandwidth: 5074.78 MB/sec</div><div class="line">STREAM add latency: 2.35 nanoseconds</div><div class="line">STREAM add bandwidth: 10197.36 MB/sec</div><div class="line">STREAM triad latency: 5.12 nanoseconds</div><div class="line">STREAM triad bandwidth: 4686.37 MB/sec</div><div class="line"></div><div class="line">#numactl -C 7 -m 2 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 3.85 nanoseconds</div><div class="line">STREAM copy bandwidth: 4150.98 MB/sec</div><div class="line">STREAM scale latency: 3.95 nanoseconds</div><div class="line">STREAM scale bandwidth: 4054.30 MB/sec</div><div class="line">STREAM add latency: 2.64 nanoseconds</div><div class="line">STREAM add bandwidth: 9100.12 MB/sec</div><div class="line">STREAM triad latency: 6.39 nanoseconds</div><div class="line">STREAM triad bandwidth: 3757.70 MB/sec</div><div class="line"></div><div class="line">#numactl -C 7 -m 3 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 3.69 nanoseconds</div><div class="line">STREAM copy bandwidth: 4340.24 MB/sec</div><div class="line">STREAM scale latency: 3.62 nanoseconds</div><div class="line">STREAM scale bandwidth: 4422.18 MB/sec</div><div class="line">STREAM add latency: 2.47 nanoseconds</div><div class="line">STREAM add bandwidth: 9704.82 MB/sec</div><div class="line">STREAM triad latency: 5.74 nanoseconds</div><div class="line">STREAM triad bandwidth: 4177.85 MB/sec</div><div class="line"></div><div class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</div><div class="line">#numactl -C 7 -m 7 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 3.95 nanoseconds</div><div class="line">STREAM copy bandwidth: 4051.51 MB/sec</div><div class="line">STREAM scale latency: 3.94 nanoseconds</div><div class="line">STREAM scale bandwidth: 4060.63 MB/sec</div><div class="line">STREAM add latency: 2.54 nanoseconds</div><div class="line">STREAM add bandwidth: 9434.51 MB/sec</div><div class="line">STREAM triad latency: 6.13 nanoseconds</div><div class="line">STREAM triad bandwidth: 3913.36 MB/sec</div><div class="line"></div><div class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</div><div class="line">#numactl -C 7 -m 10 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 8.80 nanoseconds</div><div class="line">STREAM copy bandwidth: 1817.78 MB/sec</div><div class="line">STREAM scale latency: 8.59 nanoseconds</div><div class="line">STREAM scale bandwidth: 1861.65 MB/sec</div><div class="line">STREAM add latency: 5.55 nanoseconds</div><div class="line">STREAM add bandwidth: 4320.68 MB/sec</div><div class="line">STREAM triad latency: 13.94 nanoseconds</div><div class="line">STREAM triad bandwidth: 1721.76 MB/sec</div><div class="line"></div><div class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</div><div class="line">#numactl -C 7 -m 11 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 9.27 nanoseconds</div><div class="line">STREAM copy bandwidth: 1726.52 MB/sec</div><div class="line">STREAM scale latency: 9.31 nanoseconds</div><div class="line">STREAM scale bandwidth: 1718.10 MB/sec</div><div class="line">STREAM add latency: 5.65 nanoseconds</div><div class="line">STREAM add bandwidth: 4250.89 MB/sec</div><div class="line">STREAM triad latency: 14.09 nanoseconds</div><div class="line">STREAM triad bandwidth: 1703.66 MB/sec</div><div class="line"></div><div class="line">//在另外一个socket上测试本numa，和node0性能完全一致</div><div class="line">[root@101a0500 /root/lmbench3]</div><div class="line">#numactl -C 88 -m 11 ./bin/stream  -W 5 -N 5 -M 64M </div><div class="line">STREAM copy latency: 2.93 nanoseconds</div><div class="line">STREAM copy bandwidth: 5454.67 MB/sec</div><div class="line">STREAM scale latency: 2.96 nanoseconds</div><div class="line">STREAM scale bandwidth: 5400.03 MB/sec</div><div class="line">STREAM add latency: 2.28 nanoseconds</div><div class="line">STREAM add bandwidth: 10543.42 MB/sec</div><div class="line">STREAM triad latency: 4.52 nanoseconds</div><div class="line">STREAM triad bandwidth: 5308.40 MB/sec</div><div class="line"></div><div class="line">[root@101a0500 /root/lmbench3]</div><div class="line">#numactl -C 7 -m 15 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 8.73 nanoseconds</div><div class="line">STREAM copy bandwidth: 1831.77 MB/sec</div><div class="line">STREAM scale latency: 8.81 nanoseconds</div><div class="line">STREAM scale bandwidth: 1815.13 MB/sec</div><div class="line">STREAM add latency: 5.63 nanoseconds</div><div class="line">STREAM add bandwidth: 4265.21 MB/sec</div><div class="line">STREAM triad latency: 13.09 nanoseconds</div><div class="line">STREAM triad bandwidth: 1833.68 MB/sec</div></pre></td></tr></table></figure>
<p>Lat_mem_rd 用cpu7访问node0和node15对比结果，随着数据的加大，延时在加大，64M时能有3倍差距，和上面测试一致</p>
<p>下图 第一列 表示读写数据的大小（单位M），第二列表示访问延时（单位纳秒），一般可以看到在L1/L2/L3 cache大小的地方延时会有跳跃，远超过L3大小后，延时就是内存延时了</p>
<p><img src="/images/951413iMgBlog/image-20210924185044090-7857743.png" alt="image-20210924185044090"></p>
<p>测试命令如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numactl -C 7 -m 0 ./bin/lat_mem_rd -W 5 -N 5 -t 64M  //-C 7 cpu 7, -m 0 node0, -W 热身 -t stride</div></pre></td></tr></table></figure>
<p>同样的机型，开关numa的测试结果，关numa 时延、带宽都差了几倍，<a href="https://zhuanlan.zhihu.com/p/387117470" target="_blank" rel="external">所以一定要开NUMA</a></p>
<p><img src="/images/951413iMgBlog/image-20210924192330025-7857743.png" alt="image-20210924192330025"></p>
<h3 id="鲲鹏920-1"><a href="#鲲鹏920-1" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">#for i in $(seq 0 15); do echo core:$i; numactl -N $i -m 7 ./bin/stream  -W 5 -N 5 -M 64M; done</div><div class="line">STREAM copy latency: 1.84 nanoseconds</div><div class="line">STREAM copy bandwidth: 8700.75 MB/sec</div><div class="line">STREAM scale latency: 1.86 nanoseconds</div><div class="line">STREAM scale bandwidth: 8623.60 MB/sec</div><div class="line">STREAM add latency: 2.18 nanoseconds</div><div class="line">STREAM add bandwidth: 10987.04 MB/sec</div><div class="line">STREAM triad latency: 3.03 nanoseconds</div><div class="line">STREAM triad bandwidth: 7926.87 MB/sec</div><div class="line"></div><div class="line">#numactl -C 7 -m 1 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 2.05 nanoseconds</div><div class="line">STREAM copy bandwidth: 7802.45 MB/sec</div><div class="line">STREAM scale latency: 2.08 nanoseconds</div><div class="line">STREAM scale bandwidth: 7681.87 MB/sec</div><div class="line">STREAM add latency: 2.19 nanoseconds</div><div class="line">STREAM add bandwidth: 10954.76 MB/sec</div><div class="line">STREAM triad latency: 3.17 nanoseconds</div><div class="line">STREAM triad bandwidth: 7559.86 MB/sec</div><div class="line"></div><div class="line">#numactl -C 7 -m 2 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 3.51 nanoseconds</div><div class="line">STREAM copy bandwidth: 4556.86 MB/sec</div><div class="line">STREAM scale latency: 3.58 nanoseconds</div><div class="line">STREAM scale bandwidth: 4463.66 MB/sec</div><div class="line">STREAM add latency: 2.71 nanoseconds</div><div class="line">STREAM add bandwidth: 8869.79 MB/sec</div><div class="line">STREAM triad latency: 5.92 nanoseconds</div><div class="line">STREAM triad bandwidth: 4057.12 MB/sec</div><div class="line"></div><div class="line">[root@ARM 19:14 /root/lmbench3]</div><div class="line">#numactl -C 7 -m 3 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 3.94 nanoseconds</div><div class="line">STREAM copy bandwidth: 4064.25 MB/sec</div><div class="line">STREAM scale latency: 3.82 nanoseconds</div><div class="line">STREAM scale bandwidth: 4188.67 MB/sec</div><div class="line">STREAM add latency: 2.86 nanoseconds</div><div class="line">STREAM add bandwidth: 8390.70 MB/sec</div><div class="line">STREAM triad latency: 4.78 nanoseconds</div><div class="line">STREAM triad bandwidth: 5024.25 MB/sec</div><div class="line"></div><div class="line">#numactl -C 24 -m 3 ./bin/stream  -W 5 -N 5 -M 64M</div><div class="line">STREAM copy latency: 4.10 nanoseconds</div><div class="line">STREAM copy bandwidth: 3904.63 MB/sec</div><div class="line">STREAM scale latency: 4.03 nanoseconds</div><div class="line">STREAM scale bandwidth: 3969.41 MB/sec</div><div class="line">STREAM add latency: 3.07 nanoseconds</div><div class="line">STREAM add bandwidth: 7816.08 MB/sec</div><div class="line">STREAM triad latency: 5.06 nanoseconds</div><div class="line">STREAM triad bandwidth: 4738.66 MB/sec</div></pre></td></tr></table></figure>
<h3 id="海光7280"><a href="#海光7280" class="headerlink" title="海光7280"></a>海光7280</h3><p>可以看到跨numa（一个numa也就是一个socket，等同于跨socket）RT从1.5上升到2.5，这个数据比鲲鹏920要好很多。<br>这里还会测试同一块CPU设置不同数量的numa node对性能的影响，所以接下来的测试会列出numa node数量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">[root@hygon8 14:32 /root/lmbench-master]</div><div class="line">#lscpu</div><div class="line">架构：                           x86_64</div><div class="line">CPU 运行模式：                   32-bit, 64-bit</div><div class="line">字节序：                         Little Endian</div><div class="line">Address sizes:                   43 bits physical, 48 bits virtual</div><div class="line">CPU:                             128</div><div class="line">在线 CPU 列表：                  0-127</div><div class="line">每个核的线程数：                 2</div><div class="line">每个座的核数：                   32</div><div class="line">座：                             2</div><div class="line">NUMA 节点：                      8</div><div class="line">厂商 ID：                        HygonGenuine</div><div class="line">CPU 系列：                       24</div><div class="line">型号：                           1</div><div class="line">型号名称：                       Hygon C86 7280 32-core Processor</div><div class="line">步进：                           1</div><div class="line">CPU MHz：                        2194.586</div><div class="line">BogoMIPS：                       3999.63</div><div class="line">虚拟化：                         AMD-V</div><div class="line">L1d 缓存：                       2 MiB</div><div class="line">L1i 缓存：                       4 MiB</div><div class="line">L2 缓存：                        32 MiB</div><div class="line">L3 缓存：                        128 MiB</div><div class="line">NUMA 节点0 CPU：                 0-7,64-71</div><div class="line">NUMA 节点1 CPU：                 8-15,72-79</div><div class="line">NUMA 节点2 CPU：                 16-23,80-87</div><div class="line">NUMA 节点3 CPU：                 24-31,88-95</div><div class="line">NUMA 节点4 CPU：                 32-39,96-103</div><div class="line">NUMA 节点5 CPU：                 40-47,104-111</div><div class="line">NUMA 节点6 CPU：                 48-55,112-119</div><div class="line">NUMA 节点7 CPU：                 56-63,120-127</div><div class="line"></div><div class="line">//可以看到7号core比15、23、31号core明显要快，就近访问node 0的内存，跨numa node（跨Die）没有内存交织分配</div><div class="line">[root@hygon8 14:32 /root/lmbench-master]</div><div class="line">#time for i in $(seq 7 8 64); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</div><div class="line">7</div><div class="line">STREAM copy latency: 1.38 nanoseconds    </div><div class="line">STREAM copy bandwidth: 11559.53 MB/sec</div><div class="line">STREAM scale latency: 1.16 nanoseconds</div><div class="line">STREAM scale bandwidth: 13815.87 MB/sec</div><div class="line">STREAM add latency: 1.40 nanoseconds</div><div class="line">STREAM add bandwidth: 17145.85 MB/sec</div><div class="line">STREAM triad latency: 1.44 nanoseconds</div><div class="line">STREAM triad bandwidth: 16637.18 MB/sec</div><div class="line">15</div><div class="line">STREAM copy latency: 1.67 nanoseconds</div><div class="line">STREAM copy bandwidth: 9591.77 MB/sec</div><div class="line">STREAM scale latency: 1.56 nanoseconds</div><div class="line">STREAM scale bandwidth: 10242.50 MB/sec</div><div class="line">STREAM add latency: 1.45 nanoseconds</div><div class="line">STREAM add bandwidth: 16581.00 MB/sec</div><div class="line">STREAM triad latency: 2.00 nanoseconds</div><div class="line">STREAM triad bandwidth: 12028.83 MB/sec</div><div class="line">23</div><div class="line">STREAM copy latency: 1.65 nanoseconds</div><div class="line">STREAM copy bandwidth: 9701.49 MB/sec</div><div class="line">STREAM scale latency: 1.53 nanoseconds</div><div class="line">STREAM scale bandwidth: 10427.98 MB/sec</div><div class="line">STREAM add latency: 1.42 nanoseconds</div><div class="line">STREAM add bandwidth: 16846.10 MB/sec</div><div class="line">STREAM triad latency: 1.97 nanoseconds</div><div class="line">STREAM triad bandwidth: 12189.72 MB/sec</div><div class="line">31</div><div class="line">STREAM copy latency: 1.64 nanoseconds</div><div class="line">STREAM copy bandwidth: 9742.86 MB/sec</div><div class="line">STREAM scale latency: 1.52 nanoseconds</div><div class="line">STREAM scale bandwidth: 10510.80 MB/sec</div><div class="line">STREAM add latency: 1.45 nanoseconds</div><div class="line">STREAM add bandwidth: 16559.86 MB/sec</div><div class="line">STREAM triad latency: 1.92 nanoseconds</div><div class="line">STREAM triad bandwidth: 12490.01 MB/sec</div><div class="line">39</div><div class="line">STREAM copy latency: 2.55 nanoseconds</div><div class="line">STREAM copy bandwidth: 6286.25 MB/sec</div><div class="line">STREAM scale latency: 2.51 nanoseconds</div><div class="line">STREAM scale bandwidth: 6383.11 MB/sec</div><div class="line">STREAM add latency: 1.76 nanoseconds</div><div class="line">STREAM add bandwidth: 13660.83 MB/sec</div><div class="line">STREAM triad latency: 3.68 nanoseconds</div><div class="line">STREAM triad bandwidth: 6523.02 MB/sec</div></pre></td></tr></table></figure>
<p>如果这种芯片在bios里设置Die interleaving，4块die当成一个numa node吐出来给OS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line">#lscpu</div><div class="line">架构：                           x86_64</div><div class="line">CPU 运行模式：                   32-bit, 64-bit</div><div class="line">字节序：                         Little Endian</div><div class="line">Address sizes:                   43 bits physical, 48 bits virtual</div><div class="line">CPU:                             128</div><div class="line">在线 CPU 列表：                  0-127</div><div class="line">每个核的线程数：                 2</div><div class="line">每个座的核数：                   32</div><div class="line">座：                             2</div><div class="line">NUMA 节点：                      2</div><div class="line">厂商 ID：                        HygonGenuine</div><div class="line">CPU 系列：                       24</div><div class="line">型号：                           1</div><div class="line">型号名称：                       Hygon C86 7280 32-core Processor</div><div class="line">步进：                           1</div><div class="line">CPU MHz：                        2108.234</div><div class="line">BogoMIPS：                       3999.45</div><div class="line">虚拟化：                         AMD-V</div><div class="line">L1d 缓存：                       2 MiB</div><div class="line">L1i 缓存：                       4 MiB</div><div class="line">L2 缓存：                        32 MiB</div><div class="line">L3 缓存：                        128 MiB</div><div class="line">//注意这里bios配置了Die Interleaving Enable</div><div class="line">//表示每路内多个Die内存交织分配，这样整个一个socket就是一个大Die</div><div class="line">NUMA 节点0 CPU：                 0-31,64-95  </div><div class="line">NUMA 节点1 CPU：                 32-63,96-127</div><div class="line"></div><div class="line"></div><div class="line">//enable die interleaving 后继续streaming测试</div><div class="line">//最终测试结果表现就是7/15/23/31 core性能一致，因为默认一个numa内内存交织分配</div><div class="line">//可以看到同一路下的四个die内存交织访问，所以4个node内存延时一样了（被平均），都不如8node就近快</div><div class="line">[root@hygon3 16:09 /root/lmbench-master]</div><div class="line">#time for i in $(seq 7 8 64); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</div><div class="line">7</div><div class="line">STREAM copy latency: 1.48 nanoseconds</div><div class="line">STREAM copy bandwidth: 10782.58 MB/sec</div><div class="line">STREAM scale latency: 1.20 nanoseconds</div><div class="line">STREAM scale bandwidth: 13364.38 MB/sec</div><div class="line">STREAM add latency: 1.46 nanoseconds</div><div class="line">STREAM add bandwidth: 16408.32 MB/sec</div><div class="line">STREAM triad latency: 1.53 nanoseconds</div><div class="line">STREAM triad bandwidth: 15696.00 MB/sec</div><div class="line">15</div><div class="line">STREAM copy latency: 1.51 nanoseconds</div><div class="line">STREAM copy bandwidth: 10601.25 MB/sec</div><div class="line">STREAM scale latency: 1.24 nanoseconds</div><div class="line">STREAM scale bandwidth: 12855.87 MB/sec</div><div class="line">STREAM add latency: 1.46 nanoseconds</div><div class="line">STREAM add bandwidth: 16382.42 MB/sec</div><div class="line">STREAM triad latency: 1.53 nanoseconds</div><div class="line">STREAM triad bandwidth: 15691.48 MB/sec</div><div class="line">23</div><div class="line">STREAM copy latency: 1.50 nanoseconds</div><div class="line">STREAM copy bandwidth: 10700.61 MB/sec</div><div class="line">STREAM scale latency: 1.27 nanoseconds</div><div class="line">STREAM scale bandwidth: 12634.63 MB/sec</div><div class="line">STREAM add latency: 1.47 nanoseconds</div><div class="line">STREAM add bandwidth: 16370.67 MB/sec</div><div class="line">STREAM triad latency: 1.55 nanoseconds</div><div class="line">STREAM triad bandwidth: 15455.75 MB/sec</div><div class="line">31</div><div class="line">STREAM copy latency: 1.50 nanoseconds</div><div class="line">STREAM copy bandwidth: 10637.39 MB/sec</div><div class="line">STREAM scale latency: 1.25 nanoseconds</div><div class="line">STREAM scale bandwidth: 12778.99 MB/sec</div><div class="line">STREAM add latency: 1.46 nanoseconds</div><div class="line">STREAM add bandwidth: 16420.65 MB/sec</div><div class="line">STREAM triad latency: 1.61 nanoseconds</div><div class="line">STREAM triad bandwidth: 14946.80 MB/sec</div><div class="line">39</div><div class="line">STREAM copy latency: 2.35 nanoseconds</div><div class="line">STREAM copy bandwidth: 6807.09 MB/sec</div><div class="line">STREAM scale latency: 2.32 nanoseconds</div><div class="line">STREAM scale bandwidth: 6906.93 MB/sec</div><div class="line">STREAM add latency: 1.63 nanoseconds</div><div class="line">STREAM add bandwidth: 14729.23 MB/sec</div><div class="line">STREAM triad latency: 3.36 nanoseconds</div><div class="line">STREAM triad bandwidth: 7151.67 MB/sec</div><div class="line">47</div><div class="line">STREAM copy latency: 2.31 nanoseconds</div><div class="line">STREAM copy bandwidth: 6938.47 MB/sec</div></pre></td></tr></table></figure>
<h3 id="intel-8269CY"><a href="#intel-8269CY" class="headerlink" title="intel 8269CY"></a>intel 8269CY</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                104</div><div class="line">On-line CPU(s) list:   0-103</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    26</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 85</div><div class="line">Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</div><div class="line">Stepping:              7</div><div class="line">CPU MHz:               3200.000</div><div class="line">CPU max MHz:           3800.0000</div><div class="line">CPU min MHz:           1200.0000</div><div class="line">BogoMIPS:              4998.89</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              1024K</div><div class="line">L3 cache:              36608K</div><div class="line">NUMA node0 CPU(s):     0-25,52-77</div><div class="line">NUMA node1 CPU(s):     26-51,78-103</div><div class="line"></div><div class="line">[root@numaopen.cloud.et93 /home/ren/lmbench3]</div><div class="line">#time for i in $(seq 0 8 51); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</div><div class="line">0</div><div class="line">STREAM copy latency: 1.15 nanoseconds</div><div class="line">STREAM copy bandwidth: 13941.80 MB/sec</div><div class="line">STREAM scale latency: 1.16 nanoseconds</div><div class="line">STREAM scale bandwidth: 13799.89 MB/sec</div><div class="line">STREAM add latency: 1.31 nanoseconds</div><div class="line">STREAM add bandwidth: 18318.23 MB/sec</div><div class="line">STREAM triad latency: 1.56 nanoseconds</div><div class="line">STREAM triad bandwidth: 15356.72 MB/sec</div><div class="line">16</div><div class="line">STREAM copy latency: 1.12 nanoseconds</div><div class="line">STREAM copy bandwidth: 14293.68 MB/sec</div><div class="line">STREAM scale latency: 1.13 nanoseconds</div><div class="line">STREAM scale bandwidth: 14162.47 MB/sec</div><div class="line">STREAM add latency: 1.31 nanoseconds</div><div class="line">STREAM add bandwidth: 18293.27 MB/sec</div><div class="line">STREAM triad latency: 1.53 nanoseconds</div><div class="line">STREAM triad bandwidth: 15692.47 MB/sec</div><div class="line">32</div><div class="line">STREAM copy latency: 1.52 nanoseconds</div><div class="line">STREAM copy bandwidth: 10551.71 MB/sec</div><div class="line">STREAM scale latency: 1.52 nanoseconds</div><div class="line">STREAM scale bandwidth: 10508.33 MB/sec</div><div class="line">STREAM add latency: 1.38 nanoseconds</div><div class="line">STREAM add bandwidth: 17363.22 MB/sec</div><div class="line">STREAM triad latency: 2.00 nanoseconds</div><div class="line">STREAM triad bandwidth: 12024.52 MB/sec</div><div class="line">40</div><div class="line">STREAM copy latency: 1.49 nanoseconds</div><div class="line">STREAM copy bandwidth: 10758.50 MB/sec</div><div class="line">STREAM scale latency: 1.50 nanoseconds</div><div class="line">STREAM scale bandwidth: 10680.17 MB/sec</div><div class="line">STREAM add latency: 1.34 nanoseconds</div><div class="line">STREAM add bandwidth: 17948.34 MB/sec</div><div class="line">STREAM triad latency: 1.98 nanoseconds</div><div class="line">STREAM triad bandwidth: 12133.22 MB/sec</div><div class="line">48</div><div class="line">STREAM copy latency: 1.49 nanoseconds</div><div class="line">STREAM copy bandwidth: 10736.56 MB/sec</div><div class="line">STREAM scale latency: 1.50 nanoseconds</div><div class="line">STREAM scale bandwidth: 10692.93 MB/sec</div><div class="line">STREAM add latency: 1.34 nanoseconds</div><div class="line">STREAM add bandwidth: 17902.85 MB/sec</div><div class="line">STREAM triad latency: 1.96 nanoseconds</div><div class="line">STREAM triad bandwidth: 12239.44 MB/sec</div></pre></td></tr></table></figure>
<h3 id="Intel-R-Xeon-R-CPU-E5-2682-v4"><a href="#Intel-R-Xeon-R-CPU-E5-2682-v4" class="headerlink" title="Intel(R) Xeon(R) CPU E5-2682 v4"></a>Intel(R) Xeon(R) CPU E5-2682 v4</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line">#time for i in $(seq 0 8 51); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</div><div class="line">0</div><div class="line">STREAM copy latency: 1.59 nanoseconds</div><div class="line">STREAM copy bandwidth: 10092.31 MB/sec</div><div class="line">STREAM scale latency: 1.57 nanoseconds</div><div class="line">STREAM scale bandwidth: 10169.16 MB/sec</div><div class="line">STREAM add latency: 1.31 nanoseconds</div><div class="line">STREAM add bandwidth: 18360.83 MB/sec</div><div class="line">STREAM triad latency: 2.28 nanoseconds</div><div class="line">STREAM triad bandwidth: 10503.81 MB/sec</div><div class="line">8</div><div class="line">STREAM copy latency: 1.55 nanoseconds</div><div class="line">STREAM copy bandwidth: 10312.14 MB/sec</div><div class="line">STREAM scale latency: 1.56 nanoseconds</div><div class="line">STREAM scale bandwidth: 10283.70 MB/sec</div><div class="line">STREAM add latency: 1.30 nanoseconds</div><div class="line">STREAM add bandwidth: 18416.26 MB/sec</div><div class="line">STREAM triad latency: 2.23 nanoseconds</div><div class="line">STREAM triad bandwidth: 10777.08 MB/sec</div><div class="line">16</div><div class="line">STREAM copy latency: 2.02 nanoseconds</div><div class="line">STREAM copy bandwidth: 7914.25 MB/sec</div><div class="line">STREAM scale latency: 2.02 nanoseconds</div><div class="line">STREAM scale bandwidth: 7919.85 MB/sec</div><div class="line">STREAM add latency: 1.39 nanoseconds</div><div class="line">STREAM add bandwidth: 17276.06 MB/sec</div><div class="line">STREAM triad latency: 2.92 nanoseconds</div><div class="line">STREAM triad bandwidth: 8231.18 MB/sec</div><div class="line">24</div><div class="line">STREAM copy latency: 1.99 nanoseconds</div><div class="line">STREAM copy bandwidth: 8032.18 MB/sec</div><div class="line">STREAM scale latency: 1.98 nanoseconds</div><div class="line">STREAM scale bandwidth: 8061.12 MB/sec</div><div class="line">STREAM add latency: 1.39 nanoseconds</div><div class="line">STREAM add bandwidth: 17313.94 MB/sec</div><div class="line">STREAM triad latency: 2.88 nanoseconds</div><div class="line">STREAM triad bandwidth: 8318.93 MB/sec</div><div class="line"></div><div class="line">#lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                64</div><div class="line">On-line CPU(s) list:   0-63</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    16</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 79</div><div class="line">Model name:            Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz</div><div class="line">Stepping:              1</div><div class="line">CPU MHz:               2500.000</div><div class="line">CPU max MHz:           3000.0000</div><div class="line">CPU min MHz:           1200.0000</div><div class="line">BogoMIPS:              5000.06</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              256K</div><div class="line">L3 cache:              40960K</div><div class="line">NUMA node0 CPU(s):     0-15,32-47</div><div class="line">NUMA node1 CPU(s):     16-31,48-63</div></pre></td></tr></table></figure>
<h3 id="stream对比数据"><a href="#stream对比数据" class="headerlink" title="stream对比数据"></a>stream对比数据</h3><p>总结下几个CPU用stream测试访问内存的RT以及抖动和带宽对比数据</p>
<table><br><tr class="header"><br><th></th><br><th>最小RT</th><br><th>最大RT</th><br><th>最大copy bandwidth</th><br><th>最小copy bandwidth</th><br></tr><br><tr class="odd"><br><td>申威3231(2numa node)</td><br><td>7.09</td><br><td>8.75</td><br><td>2256.59 MB/sec</td><br><td>1827.88 MB/sec</td><br></tr><br><tr class="even"><br><td>飞腾2500(16 numa node)</td><br><td>2.84</td><br><td>10.34</td><br><td>5638.21 MB/sec</td><br><td>1546.68 MB/sec</td><br></tr><br><tr class="odd"><br><td>鲲鹏920(4 numa node)</td><br><td>1.84</td><br><td>3.87</td><br><td>8700.75 MB/sec</td><br><td>4131.81 MB/sec</td><br></tr><br><tr class="even"><br><td>海光7280(8 numa node)</td><br><td>1.38</td><br><td>2.58</td><br><td>11591.48 MB/sec</td><br><td>6206.99 MB/sec</td><br></tr><br><tr class="odd"><br><td>海光5280(4 numa node)</td><br><td>1.22</td><br><td>2.52</td><br><td>13166.34 MB/sec</td><br><td>6357.71 MB/sec</td><br></tr><br><tr class="even"><br><td>Intel8269CY(2 numa node)</td><br><td>1.12</td><br><td>1.52</td><br><td>14293.68 MB/sec</td><br><td>10551.71 MB/sec</td><br></tr><br><tr class="odd"><br><td>Intel E5-2682(2 numa node)</td><br><td>1.58</td><br><td>2.02</td><br><td>10092.31 MB/sec</td><br><td>7914.25 MB/sec</td><br></tr><br></table>


<p>从以上数据可以看出这5款CPU性能一款比一款好，飞腾2500慢的core上延时快到intel 8269的10倍了，平均延时5倍以上了。延时数据基本和单核上测试sysbench TPS一致。</p>
<h3 id="lat-mem-rd对比数据"><a href="#lat-mem-rd对比数据" class="headerlink" title="lat_mem_rd对比数据"></a>lat_mem_rd对比数据</h3><p>用不同的node上的core 跑lat_mem_rd测试访问node0内存的RT，只取最大64M的时延，时延和node距离完全一致，这里就不再列出测试原始数据了。</p>
<table><br><colgroup><br><col style="width: 50%"><br><col style="width: 50%"><br></colgroup><br><tr class="header"><br><th></th><br><th>RT变化</th><br></tr><br><tr class="odd"><br><td>飞腾2500(16 numa node)</td><br><td>core:0 149.976<br>core:8 168.805<br>core:16 191.415<br>core:24 178.283<br>core:32 170.814<br>core:40 185.699<br>core:48 212.281<br>core:56 202.479<br>core:64 426.176<br>core:72 444.367<br>core:80 465.894<br>core:88 452.245<br>core:96 448.352<br>core:104 460.603<br>core:112 485.989<br>core:120 490.402</td><br></tr><br><tr class="even"><br><td>鲲鹏920(4 numa node)</td><br><td>core:0 117.323<br>core:24 135.337<br>core:48 197.782<br>core:72 219.416</td><br></tr><br><tr class="odd"><br><td>海光7280(8 numa node)</td><br><td>numa0 106.839<br>numa1 168.583<br>numa2 163.925<br>numa3 163.690<br>numa4 289.628<br>numa5 288.632<br>numa6 236.615<br>numa7 291.880<br>分割行<br>enabled die interleaving <br>core:0 153.005<br>core:16 152.458<br>core:32 272.057<br>core:48 269.441</td><br></tr><br><tr class="even"><br><td>海光5280(4 numa node)</td><br><td>core:0 102.574<br>core:8 160.989<br>core:16 286.850<br>core:24 231.197</td><br></tr><br><tr class="odd"><br><td>Intel 8269CY(2 numa node)</td><br><td>core:0 69.792<br>core:26 93.107</td><br></tr><br><tr class="even"><br><td>申威3231(2numa node)</td><br><td>core:0 215.146<br>core:32 282.443</td><br></tr><br></table>


<p>测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">for i in $(seq 0 8 127); do echo core:$i; numactl -C $i -m 0 ./bin/lat_mem_rd -W 5 -N 5 -t 64M; done &gt;lat.log 2&gt;&amp;1</div></pre></td></tr></table></figure>
<p>测试结果和numactl -H 看到的node distance完全一致，芯片厂家应该就是这样测试然后把这个延迟当做距离写进去了</p>
<p>最后用一张实际测试Inte E5 L1 、L2、L3的cache延时图来加深印象，可以看到在每级cache大小附近时延有个跳跃：<br><img src="/images/951413iMgBlog/1647854557128-f35192b1-c1a4-4f19-b19d-41713ced19d6.png" alt="undefined"><br>纵坐标是访问延时 纳秒，横坐标是cache大小 M，为什么上图没放内存延时，因为延时太大，放出来就把L1、L2的跳跃台阶压平了</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>X86比ARM性能要好</li>
<li>AMD和Intel单核基本差别不大，Intel适合要求核多的大实例，AMD适合云上拆分售卖</li>
<li>国产CPU还有比较大的进步空间</li>
<li>性能上的差异在数据库场景下归因下来主要在CPU访问内存的时延上</li>
<li>跨Numa Node时延差异很大，一定要开NUMA 就近访问内存</li>
<li>数据库场景下大实例因为锁导致CPU很难跑满，建议 <a href="https://www.aliyun.com/product/drds?spm=5176.19720258.J_8058803260.33.e9392c4aRo26Yg" target="_blank" rel="external">分库分表搞多个mysqld实例</a></li>
</ul>
<p><strong>如果你一定要知道一块CPU性能的话先看 内存延时 而不是 主频</strong>，各种CPU自家打榜一般都是简单计算场景，内存访问不多，但是实际业务中大部分时候又是高频访问内存的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/387117470" target="_blank" rel="external">十年后数据库还是不敢拥抱NUMA？</a><br><a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a><br><a href="https://blog.csdn.net/xuanjian_bjtu/article/details/107178226" target="_blank" rel="external">lmbench测试要考虑cache等</a><br><a href="https://zhuanlan.zhihu.com/p/385518829" target="_blank" rel="external">CPU的制造和概念</a><br><a href="https://zhuanlan.zhihu.com/p/385519863" target="_blank" rel="external">CPU 性能和Cache Line</a><br><a href="https://zhuanlan.zhihu.com/p/385519404" target="_blank" rel="external">Perf IPC以及CPU性能</a><br><a href="https://plantegg.github.io/2021/07/19/CPU%E6%80%A7%E8%83%BD%E5%92%8CCACHE/">CPU性能和CACHE</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/03/07/一次海光物理机资源竞争压测的记录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/07/一次海光物理机资源竞争压测的记录/" itemprop="url">一次海光物理机资源竞争压测的记录</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-07T17:30:03+08:00">
                2021-03-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一次海光物理机资源竞争压测的记录"><a href="#一次海光物理机资源竞争压测的记录" class="headerlink" title="一次海光物理机资源竞争压测的记录"></a>一次海光物理机资源竞争压测的记录</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>问题描述如下</p>
<blockquote>
<p>sysbench 压200个服务节点(每个4c16G，总共800core), 发现qps不能线性增加（200节点比100节点好1.2倍而已)。</p>
<p>如果压单个服务节点节点QPS 2.4万，CPU跑到390%（每个服务节点独占4个核），如果压200个服务节点（分布在16台64核的海光物理机上）平均每个服务节点节点QPS才1.2万。但是每个服务节点的CPU也跑到了390%左右。 现在的疑问就是为什么CPU跑上去了QPS打了个5折。</p>
<p>机器集群为16*64core 为1024core，也就是每个服务节点独占4core还有冗余</p>
</blockquote>
<p>因为服务节点还需要通过LVS调用后端的多个MySQL集群，所以需要排除LVS、网络等链路瓶颈，然后找到根因是什么。</p>
<h3 id="海光物理机CPU相关信息"><a href="#海光物理机CPU相关信息" class="headerlink" title="海光物理机CPU相关信息"></a>海光物理机CPU相关信息</h3><p>总共有16台如下的海光服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">#lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                64</div><div class="line">On-line CPU(s) list:   0-63</div><div class="line">Thread(s) per core:    2      //每个物理core有两个超线程</div><div class="line">Core(s) per socket:    16     //每路16个物理core</div><div class="line">Socket(s):             2      //2路</div><div class="line">NUMA node(s):          4</div><div class="line">Vendor ID:             HygonGenuine</div><div class="line">CPU family:            24</div><div class="line">Model:                 1</div><div class="line">Model name:            Hygon C86 5280 16-core Processor</div><div class="line">Stepping:              1</div><div class="line">CPU MHz:               2455.552</div><div class="line">CPU max MHz:           2500.0000</div><div class="line">CPU min MHz:           1600.0000</div><div class="line">BogoMIPS:              4999.26</div><div class="line">Virtualization:        AMD-V</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             64K</div><div class="line">L2 cache:              512K</div><div class="line">L3 cache:              8192K</div><div class="line">NUMA node0 CPU(s):     0-7,32-39</div><div class="line">NUMA node1 CPU(s):     8-15,40-47</div><div class="line">NUMA node2 CPU(s):     16-23,48-55</div><div class="line">NUMA node3 CPU(s):     24-31,56-63</div><div class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd sev ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 MySQLeed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca</div><div class="line"></div><div class="line">#numactl -H</div><div class="line">available: 4 nodes (0-3)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 32 33 34 35 36 37 38 39</div><div class="line">node 0 size: 128854 MB</div><div class="line">node 0 free: 89350 MB</div><div class="line">node 1 cpus: 8 9 10 11 12 13 14 15 40 41 42 43 44 45 46 47</div><div class="line">node 1 size: 129019 MB</div><div class="line">node 1 free: 89326 MB</div><div class="line">node 2 cpus: 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55</div><div class="line">node 2 size: 128965 MB</div><div class="line">node 2 free: 86542 MB</div><div class="line">node 3 cpus: 24 25 26 27 28 29 30 31 56 57 58 59 60 61 62 63</div><div class="line">node 3 size: 129020 MB</div><div class="line">node 3 free: 98227 MB</div><div class="line">node distances:</div><div class="line">node   0   1   2   3</div><div class="line">  0:  10  16  28  22</div><div class="line">  1:  16  10  22  28</div><div class="line">  2:  28  22  10  16</div><div class="line">  3:  22  28  16  10</div></pre></td></tr></table></figure>
<p>AMD Zen 架构的CPU是胶水核，也就是把两个die拼一块封装成一块CPU，所以一块CPU内跨die之间延迟还是很高的。</p>
<h4 id="64-个-core-的分配策略"><a href="#64-个-core-的分配策略" class="headerlink" title="64 个 core 的分配策略"></a>64 个 core 的分配策略</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">physical         core      processor</div><div class="line">0                0~15         0~15</div><div class="line">1                0~15         16~31</div><div class="line">0                0~15         32~47</div><div class="line">1                0~15         48~63</div></pre></td></tr></table></figure>
<h4 id="海光bios配置"><a href="#海光bios配置" class="headerlink" title="海光bios配置"></a>海光bios配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">在grub.conf里面加入noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off tsx=on tsx_async_abort=off mitigations=off iommu.passthrough=1；持久化ip；挂盘参数defaults,noatime,nodiratime,lazytime,delalloc,nobarrier,data=writeback（因为后面步骤要重启，把一些OS优化也先做了）</div><div class="line">2. bios设置里面</div><div class="line">	配置 Hygon 设定 --- DF选项 --- 内存交错 --- Channel</div><div class="line">								 --- NB选项 --- 关闭iommu</div><div class="line">	打开CPB</div><div class="line">	风扇模式设置为高性能模式</div></pre></td></tr></table></figure>
<h4 id="海光简介"><a href="#海光简介" class="headerlink" title="海光简介"></a>海光简介</h4><p>公司成立于2016年3月，当前送测处理器为其第一代1.0版本的7185对标处理器为Intel的E5-2680V4，其服务器样机为曙光H620-G30。</p>
<p>其后续roadmap如下图，会包含1Die和2Die的处理器产品</p>
<p><img src="/images/951413iMgBlog/8faf9b906427972cb59ac4332d41d8a4.png" alt="img"></p>
<p><img src="/images/oss/376c93772606e5e237231ede0da64c0c.png" alt="img"></p>
<p>海光其产品规格如下，产品相对密集，但是产品之间差异化很小，频率总体接近。</p>
<p><img src="/images/951413iMgBlog/bad3d840f2d5017c50b77d47d4292eef.png" alt="img"></p>
<p>AMD授权Zen IP给海光的操作是先成立合资公司，授权给合资公司基于Zen 研发新的 CPU，而且转让给中国的所有信息都符合美国出口法规<strong>。</strong>天津海光和AMD成立的合资公司可以修改AMD的CPU核，变相享有X86授权，而海光公司可以通过购买合资公司研发的CPU核，开发服务器CPU，不过仅仅局限于中国市场。</p>
<p>AMD与国内公司A成立合资公司B，合资公司B由AMD控股，负责开发CPU核（其实就是拿AMD现成的内核），然后公司A购买合资公司B开发的CPU核，以此为基础开发CPU，最终实现ARM卖IP核的翻版。</p>
<h4 id="海光与AMD-的-Ryzen-EPYC-比较"><a href="#海光与AMD-的-Ryzen-EPYC-比较" class="headerlink" title="海光与AMD 的 Ryzen/EPYC 比较"></a>海光与AMD 的 Ryzen/EPYC 比较</h4><p>由于在 Zen 1 的基础上进行了大量的修改，海光 CPU 可以不用简单地称之为换壳 AMD 处理器了。但其性能相比同代原版 CPU 略差：整数性能基本相同，浮点性能显著降低——普通指令吞吐量只有基准水平的一半。海光 CPU 的随机数生成机制也被修改，加密引擎已被替换，不再对常见的 AES 指令进行加速，但覆盖了其他面向国内安全性的指令如 SM2、SM3 和 SM4。</p>
<h5 id="相同"><a href="#相同" class="headerlink" title="相同"></a>相同</h5><p>与 AMD 的 Ryzen/EPYC 相比，海光处理器究竟有哪些不同？总体而言，核心布局是相同的，缓存大小、TLB 大小和端口分配都相同，在基础级别上两者没有差异。CPU 仍然是 64KB 四路 L1 指令缓存，32KB 八路 L1 数据缓存，512KB 八路 L2 缓存以及 8MB 十六路 L3 缓存，与 Zen 1 核心完全相同。</p>
<h5 id="不同"><a href="#不同" class="headerlink" title="不同"></a>不同</h5><p><strong>加密方式变化</strong></p>
<p>在 Linux 内核升级中有关加密变化的信息已经明示。这些更新围绕 AMD 虚拟化功能（SEV）的安全加密进行。通常对于 EPYC 处理器来说，SEV 由 AMD 定义的加密协议控制，在这种情况下为 RSA、ECDSA、ECDH、SHA 和 AES。</p>
<p>但在海光 Dhyana 处理器中，SEV 被设计为使用 SM2、SM3 和 SM4 算法。在更新中有关 SM2 的部分声明道，这种算法基于椭圆曲线加密法，且需要其他私钥/公钥交换；SM3 是一种哈希算法，类似于 SHA-256；而 SM4 是类似于 AES-128 的分组密码算法。为支持这些算法所需的额外功能，其他指令也被加入到了 Linux 内核中。在说明文件中指出，这些算法已在 Hygon Dhyana Plus 处理器上成功进行测试，也已在 AMD 的 EPYC CPU 上成功测试。</p>
<p>此外，海光与 AMD 原版芯片最大的设计区别在于<strong>吞吐</strong>量，尽管整数性能相同，但海光芯片对于某些浮点指令并未做流水线处理，这意味着吞吐量和延迟都减小了：</p>
<p><img src="/images/951413iMgBlog/640-6077478.jpeg" alt="img"></p>
<p>这些对于最基础的任务来说也会有所影响，降低吞吐量的设计会让 CPU 在并行计算时性能受限。另外一个最大的变化，以及 Dhyana 与服务器版的「Dhyana Plus」版本之间的不同在于随机数生成的能力。</p>
<h2 id="验证是否是上下游的瓶颈"><a href="#验证是否是上下游的瓶颈" class="headerlink" title="验证是否是上下游的瓶颈"></a>验证是否是上下游的瓶颈</h2><p>需要先分析问题是否在LVS调用后端的多个MySQL集群上。</p>
<p>先写一个简单的测试程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">#cat Test.java</div><div class="line">import java.sql.Connection;</div><div class="line">import java.sql.DriverManager;</div><div class="line">import java.sql.ResultSet;</div><div class="line">import java.sql.SQLException;</div><div class="line">import java.sql.Statement;</div><div class="line">/*</div><div class="line"> * 目录：/home/admin/jdbc</div><div class="line"> *</div><div class="line"> * 编译：</div><div class="line"> *  javac -cp /home/admin/lib/*:. Test.java</div><div class="line"> *</div><div class="line"> *  运行：</div><div class="line"> *   java -cp /home/admin/MySQL-server/lib/*:. Test &quot;jdbc:mysql://172.16.160.1:4261/qc_pay_0xwd_0002&quot; &quot;myhhzi0d&quot; &quot;jOXaC1Lbif-k&quot; &quot;select count(*) from pay_order where user_id=1169257092557639682 and order_no=&apos;201909292111250000102&apos;&quot; &quot;100&quot;</div><div class="line"> *   */</div><div class="line">public class Test &#123;</div><div class="line"></div><div class="line">    public static void main(String args[]) throws NumberFormatException, InterruptedException, ClassNotFoundException &#123;</div><div class="line">        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);</div><div class="line">        String url = args[0];</div><div class="line">        String user = args[1];</div><div class="line">        String pass = args[2];</div><div class="line">        String sql = args[3];</div><div class="line">        String interval = args[4];</div><div class="line">        try &#123;</div><div class="line">            Connection conn = DriverManager.getConnection(url, user, pass);</div><div class="line">            while (true) &#123;</div><div class="line">                long start = System.currentTimeMillis();</div><div class="line">                for(int i=0; i&lt;1000; ++i)&#123;</div><div class="line">                    Statement stmt = conn.createStatement();</div><div class="line">                    ResultSet rs = stmt.executeQuery(sql);</div><div class="line">                    while (rs.next()) &#123;</div><div class="line">                    &#125;</div><div class="line">                    rs.close();</div><div class="line">                    stmt.close();</div><div class="line">                    Thread.sleep(Long.valueOf(interval));</div><div class="line">                &#125;</div><div class="line">                long end = System.currentTimeMillis();</div><div class="line">                System.out.println(&quot;rt : &quot; + (end - start));</div><div class="line">            &#125;</div><div class="line">        &#125; catch (SQLException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后通过传入不同的jdbc参数跑2组测试:</p>
<ol>
<li>走服务节点执行指定id的点查； </li>
<li>直接从服务节点节点连MySQL指定id点查  </li>
</ol>
<p>上述2组测试同时跑在三组场景下：</p>
<ul>
<li>A) 服务节点和MySQL都没有压力； </li>
<li>B) 跑1、2测试的服务节点没有压力，但是sysbench 在压别的服务节点，这样后端的MySQL是有sysbench压侧压力，LVS也有流量压力的； </li>
<li>C) sysbench压所有服务节点, 包含运行 1、2测试程序节点） </li>
</ul>
<p>这样2组测试3个场景组合可以得到6组响应时间的测试数据</p>
<p>从最终得到6组数据来看可以排除链路以及MySQL的问题，瓶颈似乎还是在服务节点上</p>
<p><img src="/images/oss/595bc15fdd72860f2d1875c86384a14b.png" alt="image.png"></p>
<p>单独压一个服务节点节点并在上面跑测试，服务节点 CPU被压到 390%(每个服务节点 节点固定绑到4核), 这个时候整个宿主机压力不大，但是这四个核比较紧张了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#cat rt.log  | awk &apos;&#123; print $3 &#125;&apos;  | awk &apos;&#123;if(min==&quot;&quot;)&#123;min=max=$1&#125;; if($1&gt;max) &#123;max=$1&#125;; if($1&lt;min) &#123;min=$1&#125;; total+=$1; count+=1&#125; END &#123;print &quot;avg &quot; total/count,&quot; | max &quot;max,&quot; | min &quot; min, &quot;| count &quot;, count&#125;&apos; ; cat MySQL.log  | awk &apos;&#123; print $3 &#125;&apos;  | awk &apos;&#123;if(min==&quot;&quot;)&#123;min=max=$1&#125;; if($1&gt;max) &#123;max=$1&#125;; if($1&lt;min) &#123;min=$1&#125;; total+=$1; count+=1&#125; END &#123;print &quot;avg &quot; total/count,&quot; | max &quot;max,&quot; | min &quot; min, &quot;| count &quot;, count &#125;&apos;;</div><div class="line">avg 2589.13  | max 3385  | min 2502 | count  69</div><div class="line">avg 1271.07  | max 1405  | min 1254 | count  141</div><div class="line"></div><div class="line">[root@d42a01107.cloud.a02.am78 /root]</div><div class="line">#taskset -pc 48759</div><div class="line">pid 48759&apos;s current affinity list: 19,52-54</div></pre></td></tr></table></figure>
<p>通过这6组测试数据可以看到，只有在整个系统都有压力（服务节点所在物理机、LVS、MySQL）的时候rt飙升最明显（C组数据），如果只是LVS、MySQL有压力，服务节点没有压力的时候可以看到数据还是很好的（B组数据）</p>
<h2 id="分析宿主机资源竞争"><a href="#分析宿主机资源竞争" class="headerlink" title="分析宿主机资源竞争"></a>分析宿主机资源竞争</h2><h3 id="perf分析"><a href="#perf分析" class="headerlink" title="perf分析"></a>perf分析</h3><p>只压单个服务节点</p>
<p><img src="/images/oss/7aea9045e50794fadc0439ee806f2496.png" alt="image.png"></p>
<p><img src="/images/oss/86c3f14887345a1d5f08cae985816564.png" alt="image.png"></p>
<p><strong>从以上截图，可以看到关键的 insn per cycle 能到0.51和0.66（这个数值越大性能越好）</strong></p>
<p>如果同时压物理机上的所有服务节点</p>
<p><img src="/images/oss/02f47474c612c2bf6e612efea3ab5de3.png" alt="image.png"></p>
<p><img src="/images/oss/c3d90e077d00a7a3db54770d9eea2dbb.png" alt="image.png"></p>
<p><strong>从以上截图，可以看到关键的 insn per cycle 能降到了0.27和0.31（这个数值越大性能越好），基本相当于单压的5折</strong></p>
<p>通过 perf list 找出所有Hardware event，然后对他们进行perf:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo perf stat -e branch-instructions,branch-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-prefetches,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads -a -- `pidof java`</div></pre></td></tr></table></figure>
<h3 id="尝试不同的绑核后的一些数据"><a href="#尝试不同的绑核后的一些数据" class="headerlink" title="尝试不同的绑核后的一些数据"></a>尝试不同的绑核后的一些数据</h3><p>通过以上perf数据以及numa结构，尝试将不同服务进程绑定到指定的4个核上</p>
<p>试了以下三种绑核的办法：</p>
<p>1）docker swarm随机绑（<strong>以上测试都是用的这种默认方案</strong>）；</p>
<p>2）一个服务节点绑连续4个core,这4个core都在同一个node； </p>
<p>3）一个服务节点绑4个core，这个4个core都在在同一个node，同时尽量HT在一起，也就是0，1，32，33 ； 2，3，34，35 这种绑法 </p>
<p><strong>结果是绑法2性能略好</strong>. </p>
<p>如果是绑法2，压单个服务节点 QPS能到2.3万；绑法1和3，压单个服务节点性能差别不明显，都是2万左右。 </p>
<h2 id="尝试将Java进程开启HugePage"><a href="#尝试将Java进程开启HugePage" class="headerlink" title="尝试将Java进程开启HugePage"></a>尝试将Java进程开启HugePage</h2><p>从perf数据来看压满后tlab miss比较高，得想办法降低这个值</p>
<h3 id="修改JVM启动参数"><a href="#修改JVM启动参数" class="headerlink" title="修改JVM启动参数"></a>修改JVM启动参数</h3><p>JVM启动参数增加如下三个(-XX:LargePageSizeInBytes=2m, 这个一定要，有些资料没提这个，在我的JDK8.0环境必须要)：</p>
<blockquote>
<p>-XX:+UseLargePages -XX:LargePageSizeInBytes=2m -XX:+UseHugeTLBFS</p>
</blockquote>
<h3 id="修改机器系统配置"><a href="#修改机器系统配置" class="headerlink" title="修改机器系统配置"></a>修改机器系统配置</h3><p>设置HugePage的大小</p>
<blockquote>
<p>cat /proc/sys/vm/nr_hugepages</p>
</blockquote>
<p>nr_hugepages设置多大参考如下计算方法：</p>
<blockquote>
<p>If you are using the option <code>-XX:+UseSHM</code> or <code>-XX:+UseHugeTLBFS</code>, then specify the number of large pages. In the following example, 3 GB of a 4 GB system are reserved for large pages (assuming a large page size of 2048kB, then 3 GB = 3 <em> 1024 MB = 3072 MB = 3072 </em> 1024 kB = 3145728 kB and 3145728 kB / 2048 kB = 1536):</p>
<p>echo 1536 &gt; /proc/sys/vm/nr_hugepages </p>
</blockquote>
<p>透明大页是没有办法减少系统tlab，tlab是对应于进程的，系统分给进程的透明大页还是由物理上的4K page组成。</p>
<p>Java进程用上HugePages后iTLB-load-misses从80%下降到了14%左右, dTLB也从30%下降到了20%，但是ipc变化不明显，QPS有不到10%的增加(不能确定是不是抖动所致)</p>
<p><img src="/images/oss/f6882f4c671b4c4b46feb01aa5e272fd.png" alt="image.png"></p>
<p>在公有云ecs虚拟机上测试对性能没啥帮助，实际看到用掉的HuagPage不多，如果/proc/sys/vm/nr_hugepages 设置比较大的话JVM会因为内存不足起不来，两者内存似乎是互斥的</p>
<h2 id="用sysbench验证一下海光服务器的多core能力"><a href="#用sysbench验证一下海光服务器的多core能力" class="headerlink" title="用sysbench验证一下海光服务器的多core能力"></a>用sysbench验证一下海光服务器的多core能力</h2><p>Intel E5 2682 2.5G VS hygon 7280 2.0G（Zen1）</p>
<p><img src="/images/951413iMgBlog/image-20210813095409553.png" alt="image-20210813095409553"></p>
<p><img src="/images/951413iMgBlog/image-20210813095757299.png" alt="image-20210813095757299"></p>
<p>由以上两个测试结果可以看出单核能力hygon 7280 强于 Intel 2682，但是hygon超线程能力还是没有任何提升。Intel用超线程计算能将耗时从109秒降到74秒。但是hygon(Zen1) 只是从89秒降到了87秒，基本没有变化。</p>
<p>再补充一个Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz 对比数据 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">#taskset -c 1,53 /usr/bin/sysbench --num-threads=2 --test=cpu --cpu-max-prime=50000 run</div><div class="line">sysbench 0.5:  multi-threaded system evaluation benchmark</div><div class="line"></div><div class="line">Running the test with following options:</div><div class="line">Number of threads: 2</div><div class="line">Random number generator seed is 0 and will be ignored</div><div class="line"></div><div class="line"></div><div class="line">Primer numbers limit: 50000</div><div class="line"></div><div class="line">Threads started!</div><div class="line"></div><div class="line"></div><div class="line">General statistics:</div><div class="line">    total time:                          48.5571s</div><div class="line">    total number of events:              10000</div><div class="line">    total time taken by event execution: 97.0944s</div><div class="line">    response time:</div><div class="line">         min:                                  8.29ms</div><div class="line">         avg:                                  9.71ms</div><div class="line">         max:                                 20.88ms</div><div class="line">         approx.  95 percentile:               9.71ms</div><div class="line"></div><div class="line">Threads fairness:</div><div class="line">    events (avg/stddev):           5000.0000/2.00</div><div class="line">    execution time (avg/stddev):   48.5472/0.01</div><div class="line"></div><div class="line">#taskset -c 1 /usr/bin/sysbench --num-threads=1 --test=cpu --cpu-max-prime=50000 run</div><div class="line">sysbench 0.5:  multi-threaded system evaluation benchmark</div><div class="line"></div><div class="line">Running the test with following options:</div><div class="line">Number of threads: 1</div><div class="line">Random number generator seed is 0 and will be ignored</div><div class="line"></div><div class="line"></div><div class="line">Primer numbers limit: 50000</div><div class="line"></div><div class="line">Threads started!</div><div class="line"></div><div class="line"></div><div class="line">General statistics:</div><div class="line">    total time:                          83.2642s</div><div class="line">    total number of events:              10000</div><div class="line">    total time taken by event execution: 83.2625s</div><div class="line">    response time:</div><div class="line">         min:                                  8.27ms</div><div class="line">         avg:                                  8.33ms</div><div class="line">         max:                                 10.03ms</div><div class="line">         approx.  95 percentile:               8.36ms</div><div class="line"></div><div class="line">Threads fairness:</div><div class="line">    events (avg/stddev):           10000.0000/0.00</div><div class="line">    execution time (avg/stddev):   83.2625/0.00</div><div class="line"></div><div class="line">#lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                104</div><div class="line">On-line CPU(s) list:   0-103</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    26</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 85</div><div class="line">Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</div><div class="line">Stepping:              7</div><div class="line">CPU MHz:               3200.097</div><div class="line">CPU max MHz:           3800.0000</div><div class="line">CPU min MHz:           1200.0000</div><div class="line">BogoMIPS:              4998.89</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              1024K</div><div class="line">L3 cache:              36608K</div><div class="line">NUMA node0 CPU(s):     0-25,52-77</div><div class="line">NUMA node1 CPU(s):     26-51,78-103</div><div class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb invpcid_single pln pts dtherm spec_ctrl ibpb_support tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt avx512f avx512dq rdseed adx smap clflushopt avx512cdavx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local cat_l3 mba</div></pre></td></tr></table></figure>
<p>用sysbench 测试Hygon C86 5280 16-core Processor，分别1、8、16、24、32、40、48、56、64 个thread，32个thread前都是完美的线性增加，32core之后基本不增长了，这个应该能说明这个服务器就是32core的能力</p>
<blockquote>
<p>sysbench –threads=1 –cpu-max-prime=50000 cpu run</p>
</blockquote>
<p><img src="/images/oss/f86cd786f3a8297078579b547f78ec81.png" alt="image.png"></p>
<p>对比下intel的 Xeon 104core，也是物理52core，但是性能呈现完美线性</p>
<p><img src="/images/oss/8086f47b955d8d951e4dd4c7fe5e135e.png" alt="image.png"></p>
<h3 id="openssl场景多核能力验证"><a href="#openssl场景多核能力验证" class="headerlink" title="openssl场景多核能力验证"></a>openssl场景多核能力验证</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">openssl speed aes-256-ige -multi N</div></pre></td></tr></table></figure>
<p>intel 52 VS 26，可以看到52个线程的性能大概是26个的1.8倍</p>
<p><img src="/images/oss/4534d8e5901cc812aa54e610d1386445.png" alt="image.png"></p>
<p>intel 104 VS 52 线程，性能还能提升1.4倍</p>
<p><img src="/images/oss/6583f52e03b9753969e52d6a8b211725.png" alt="image.png"></p>
<p>海光32 VS 16, 性能能提升大概1.8倍，跟intel一致</p>
<p><img src="/images/oss/41e7f230ec27653c1b5ae5287971cd3a.png" alt="image.png"></p>
<p>海光64 VS 32, 性能能提升大概1.2倍</p>
<p><img src="/images/oss/2ad45a252392a06fa64d7475848e5601.png" alt="image.png"></p>
<p>总结下就是，在物理core数以内的线程数intel和海光性能基本增加一致；但如果超过物理core数开始使用HT后海光明显相比Intel差了很多。</p>
<p>intel超线程在openssl场景下性能能提升40%，海光就只能提升20%了。</p>
<h3 id="对比一下鲲鹏920-ARM架构的芯片"><a href="#对比一下鲲鹏920-ARM架构的芯片" class="headerlink" title="对比一下鲲鹏920 ARM架构的芯片"></a>对比一下鲲鹏920 ARM架构的芯片</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">#numactl -H</div><div class="line">available: 1 nodes (0)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</div><div class="line">node 0 size: 773421 MB</div><div class="line">node 0 free: 756092 MB</div><div class="line">node distances:</div><div class="line">node   0</div><div class="line">  0:  10</div></pre></td></tr></table></figure>
<p>96核一起跑openssl基本就是1核的96倍，完美线性，这是因为鲲鹏就没有超线程，都是物理核。如果并发增加到192个，性能和96个基本一样的。</p>
<p><img src="/images/oss/be30ab94eddc37f1d90c53204a0ed215.png" alt="image.png"></p>
<h3 id="用Sysbench直接压MySQL-oltp-read-only的场景"><a href="#用Sysbench直接压MySQL-oltp-read-only的场景" class="headerlink" title="用Sysbench直接压MySQL oltp_read_only的场景"></a>用Sysbench直接压MySQL oltp_read_only的场景</h3><p><img src="/images/oss/89c7a0228c45f79b688710206ba9d414.png" alt="image.png"></p>
<p>从1到10个thread的时候完美呈现线性，到20个thread就只比10个thread增加50%了，30thread比20增加40%，过了32个thread后增加10个core性能加不到10%了。</p>
<p>在32thread前，随着并发的增加 IPC也有所减少，这也是导致thread翻倍性能不能翻倍的一个主要原因。</p>
<p>基本也和openssl 场景一致，海光的HT基本可以忽略，做的不是很好。超过32个thread后（物理core数）性能增加及其微弱</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><a href="https://topic.atatech.org/articles/178985" target="_blank" rel="external">海光的一个core只有一个fpu</a>，所以超线程算除法完全没用.</p>
<blockquote>
<p>FP处理所有矢量操作。简单的整数向量运算（例如shift，add）都可以在一个周期内完成，是AMD以前架构延迟的一半。<strong>基本浮点数学运算具有三个周期的延迟</strong>，其中包括乘法（用于双精度需要一个额外周期）。<strong>融合乘加是五个周期。</strong></p>
<p>FP具有用于128位加载操作的单个管道。实际上，整个FP端都针对128位操作进行了优化。 Zen支持所有最新指令，例如SSE和AVX1/2。 256位AVX的设计方式是可以将它们作为两个独立的128位操作来执行。 Zen通过将这些指令作为两个操作。也就是说，<strong>Zen将256位操作分为两个µOP</strong>。同样，存储也是在128位块上完成的，从而使256位加载的有效吞吐量为每两个周期一个存储。这些管道之间的平衡相当好，因此大多数操作将安排至少两个管道，以保持每个周期至少一个这样的指令的吞吐量。暗示着，<strong>256位操作将占用两倍的资源来完成操作（即2x寄存器，调度程序和端口）。这是AMD采取的一种折衷方案，有助于节省芯片空间和功耗。</strong>相比之下，英特尔的竞争产品Skylake确实具有专用的256位电路。还应注意的是，英特尔的现代服务器级型号进一步扩展了此功能，以纳入支持AVX-512的专用512位电路，而性能最高的型号则具有二个专用的AVX-512单元。</p>
<p>此外，Zen还支持SHA和AES（并实现了2个AES单元），以提高加密性能。这些单位可以在浮点调度程序的管道0和1上找到。</p>
<p>这个也是为什么浮点比Intel X86会弱的原因。</p>
</blockquote>
<h3 id="一些其他对比结论"><a href="#一些其他对比结论" class="headerlink" title="一些其他对比结论"></a>一些其他对比结论</h3><ul>
<li>对纯CPU 运算场景，并发不超过物理core时，比如Prime运算，比如DRDS(CPU bound，IO在网络，可以加并发弥补)<ul>
<li>海光的IPC能保持稳定；</li>
<li>intel的IPC有所下降，但是QPS在IPC下降后还能完美线性</li>
</ul>
</li>
<li>在openssl和MySQL oltp_read_only场景下<ul>
<li>如果并发没超过物理core数时，海光和Intel都能随着并发的翻倍性能能增加80%</li>
<li>如果并发超过物理core数后，Intel还能随着并发的翻倍性能增加50%，海光增加就只有20%了</li>
<li>简单理解在这两个场景下Intel的HT能发挥半个物理core的作用，海光的HT就只能发挥0.2个物理core的作用了</li>
</ul>
</li>
<li>海光5280/7280 是Zen1/Zen2的AMD 架构，每个core只有一个fpu，综上在多个场景下HT基本上都可以忽略</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU的制造和概念/">CPU的制造和概念</a></p>
<p><a href="/2021/05/16/CPU Cache Line 和性能/">CPU 性能和Cache Line</a></p>
<p><a href="/2021/05/16/Perf IPC以及CPU利用率/">Perf IPC以及CPU性能</a></p>
<p><a href="/2021/06/18/几款CPU性能对比/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/05/15/飞腾ARM芯片(FT2500">飞腾ARM芯片(FT2500)的性能测试</a>的性能测试/)</p>
<p><a href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2021/03/07/一次海光物理机资源竞争压测的记录/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://dino.ciuffetti.info/2011/07/howto-java-huge-pages-linux/" target="_blank" rel="external">How to use Huge Pages with Java and Linux</a>这个资料中提到了Java使用HugePage的时候启动进程的用户权限问题，在我的docker容器中用的admin启动的进程，测试验证是不需要按资料中的设置。</p>
<p><a href="https://www.atatech.org/articles/157681" target="_blank" rel="external">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<p><a href="https://bbs.huaweicloud.com/blogs/146367" target="_blank" rel="external">华为TaiShan服务器ARMNginx应用调优案例 大量绑核、中断、Numa等相关调优信息</a></p>
<p><a href="https://topic.atatech.org/articles/178985" target="_blank" rel="external">主流处理器内部单核微架构细节1——AMD ZEN(即海光)微架构</a></p>
<p><a href="https://topic.atatech.org/articles/178986" target="_blank" rel="external">主流处理器内部单核微架构细节2——Skylake微架构</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/03/05/MacOS下如何使用iTerm2访问水木社区/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/05/MacOS下如何使用iTerm2访问水木社区/" itemprop="url">MacOS下如何使用iTerm2访问水木社区</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-05T17:30:03+08:00">
                2021-03-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MacOS下如何使用iTerm2访问水木社区"><a href="#MacOS下如何使用iTerm2访问水木社区" class="headerlink" title="MacOS下如何使用iTerm2访问水木社区"></a>MacOS下如何使用iTerm2访问水木社区</h1><p>关键字： MacOS、iTerm 、Dracula、ssh、bbs.newsmth.net</p>
<p>windows下有各种Term软件来帮助我们通过ssh访问bbs.newsmth.net, 但是工作环境切换到MacOS后发现FTerm、CTerm这样的工具都没有对应的了。但是term下访问 bbs.newsmth.net 简直是太爽了，所以本文希望解决这个问题。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>ssh 访问 bbs.newsmth.net 是没问题的，但是<strong>要解决配色和字符编码问题</strong></p>
<h3 id="解决编码"><a href="#解决编码" class="headerlink" title="解决编码"></a>解决编码</h3><p>在iTerm2的配置中增加一个profile，如下图 smth，主要是改字符编码集为 GB 18030，然后修改配色方案，我喜欢的Dracula不适合SMTH，十大完全看不了。</p>
<p><img src="/images/951413iMgBlog/image-20210602133111201.png" alt="image-20210602133111201"></p>
<p>执行脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#cat /usr/local/bin/smth</div><div class="line">echo -e &quot;\033]50;SetProfile=smth\a&quot;          //切换到smth的profile,也就是切换到了 GB18030</div><div class="line">sshpass -p&apos;密码&apos; ssh -o ServerAliveInterval=60 水木id@bbs.newsmth.net</div><div class="line">echo -e &quot;\033]50;SetProfile=Default\a&quot;       //切换回UTF8</div></pre></td></tr></table></figure>
<p>SetProfile=smth用来解决profile切换，连smth term前切换成GB 18030，断开的时候恢复成UTF-8，要不然的话正常工作的命令行就乱码了。</p>
<h3 id="解决配色问题"><a href="#解决配色问题" class="headerlink" title="解决配色问题"></a>解决配色问题</h3><p>然后还是在profile里面把smth的配色方案改成：Tango Dark, 一切简直是完美，工作灌水两不误，别人还发现不了</p>
<h2 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h2><p>目录（右边是工作窗口）：</p>
<p><img src="/images/oss/0265ed7a728bfdd6be940d838fc1feaf.png" alt="image.png"></p>
<p>十大，这个十大颜色和右边工作模式的配色方案不一样</p>
<p><img src="/images/oss/252b9295375f6e6078278a6e64e1d68c.png" alt="image.png"></p>
<p>断开后恢复成 Dracula 配色和UTF-8编码，不影响工作，别的工作tab也还是正常使用utf8</p>
<p><img src="/images/oss/cf8912c0634182b44fa92eeb9f854362.png" alt="image.png"></p>
<p>别的term网站也是类似，比如小百合、byr、ptt等</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/02/14/TCP疑难问题案例汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/14/TCP疑难问题案例汇总/" itemprop="url">TCP疑难问题案例汇总</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-14T13:30:03+08:00">
                2021-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TCP疑难问题案例汇总"><a href="#TCP疑难问题案例汇总" class="headerlink" title="TCP疑难问题案例汇总"></a>TCP疑难问题案例汇总</h1><p>碰到各种奇葩的TCP相关问题，所以汇总记录一下。分析清楚这些问题的所有来龙去脉，就能帮你在TCP知识体系里建立几个坚固的抓手，让TCP知识慢慢在抓手之间生长和互通</p>
<h2 id="服务不响应的现象或者奇怪异常的原因分析"><a href="#服务不响应的现象或者奇怪异常的原因分析" class="headerlink" title="服务不响应的现象或者奇怪异常的原因分析"></a>服务不响应的现象或者奇怪异常的原因分析</h2><p> <a href="/2021/02/10/%E4%B8%80%E4%B8%AA%E9%BB%91%E7%9B%92%E7%A8%8B%E5%BA%8F%E5%A5%87%E6%80%AA%E7%9A%84%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/">一个黑盒程序奇怪行为的分析</a> listen端口上很快就全连接队列溢出了，导致整个程序不响应了</p>
<p><a href="/2020/11/02/%E4%B8%BE%E4%B8%89%E5%8F%8D%E4%B8%80--%E4%BB%8E%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%88%B0%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%A8%E5%AF%BC/">举三反一–从理论知识到实际问题的推导</a> 服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
<p><a href="/2020/11/18/TCP%E8%BF%9E%E6%8E%A5%E4%B8%BA%E5%95%A5%E4%BA%92%E4%B8%B2%E4%BA%86/">活久见，TCP连接互串了</a>  应用每过一段时间总是会抛出几个连接异常的错误，需要查明原因。排查后发现是TCP连接互串了，这个案例实在是很珍惜，所以记录一下。</p>
<p> <a href="/2020/07/01/如何创建一个自己连自己的TCP连接/">如何创建一个自己连自己的TCP连接</a></p>
<h2 id="传输速度分析"><a href="#传输速度分析" class="headerlink" title="传输速度分析"></a>传输速度分析</h2><p>案例：<a href="/2021/01/15/TCP%E4%BC%A0%E8%BE%93%E9%80%9F%E5%BA%A6%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/">TCP传输速度案例分析</a>（长肥网络、rt升高、delay ack的影响等）</p>
<p>原理：<a href="/2019/09/28/就是要你懂TCP--性能和发送接收Buffer的关系/">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></p>
<p><a href="/2018/06/14/就是要你懂TCP--最经典的TCP性能问题/">就是要你懂TCP–最经典的TCP性能问题 Nagle和Delay ack</a></p>
<p><a href="/2019/06/21/就是要你懂TCP--性能优化大全/">就是要你懂TCP–性能优化大全</a></p>
<h2 id="TCP队列问题以及连接数"><a href="#TCP队列问题以及连接数" class="headerlink" title="TCP队列问题以及连接数"></a>TCP队列问题以及连接数</h2><p> <a href="/2020/11/30/一台机器上最多能创建多少个TCP连接/">到底一台服务器上最多能创建多少个TCP连接</a></p>
<p> <a href="/2019/08/31/就是要你懂TCP队列--通过实战案例来展示问题/">就是要你懂TCP队列–通过实战案例来展示问题</a></p>
<p> <a href="/2017/06/07/就是要你懂TCP--半连接队列和全连接队列/">就是要你懂TCP–半连接队列和全连接队列</a></p>
<p> <a href="/2017/06/02/就是要你懂TCP--连接和握手/">就是要你懂TCP–握手和挥手</a></p>
<h2 id="防火墙和reset定位分析"><a href="#防火墙和reset定位分析" class="headerlink" title="防火墙和reset定位分析"></a>防火墙和reset定位分析</h2><p>对ttl、identification等的运用</p>
<p><a href="/2018/08/26/关于TCP连接的KeepAlive和reset/">关于TCP连接的Keepalive和reset</a></p>
<p><a href="/2019/11/06/谁动了我的TCP连接/">就是要你懂网络–谁动了我的TCP连接</a></p>
<h2 id="TCP相关参数"><a href="#TCP相关参数" class="headerlink" title="TCP相关参数"></a>TCP相关参数</h2><p> <a href="/2020/01/26/TCP相关参数解释/">TCP相关参数解释</a></p>
<p><a href="/2019/05/16/网络通不通是个大问题--半夜鸡叫/">网络通不通是个大问题–半夜鸡叫</a> </p>
<p><a href="/2018/12/26/网络丢包/">网络丢包</a></p>
<h2 id="工具技巧篇"><a href="#工具技巧篇" class="headerlink" title="工具技巧篇"></a>工具技巧篇</h2><p> <a href="/2019/04/21/netstat定位性能案例/">netstat定位性能案例</a></p>
<p> <a href="/2017/08/28/netstat --timer/">netstat timer keepalive explain</a></p>
<p><a href="/2016/10/12/ss用法大全/">就是要你懂网络监控–ss用法大全</a></p>
<p><a href="/2019/06/21/就是要你懂抓包--WireShark之命令行版tshark/">就是要你懂抓包–WireShark之命令行版tshark</a></p>
<p><a href="/2018/01/01/通过tcpdump对Unix Socket 进行抓包解析/">通过tcpdump对Unix Domain Socket 进行抓包解析</a></p>
<p><a href="/2017/12/07/如何追踪网络流量/">如何追踪网络流量</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/02/10/一个黑盒程序奇怪的行为分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/10/一个黑盒程序奇怪的行为分析/" itemprop="url">一个黑盒程序奇怪行为的分析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-10T10:30:03+08:00">
                2021-02-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一个黑盒程序奇怪行为的分析"><a href="#一个黑盒程序奇怪行为的分析" class="headerlink" title="一个黑盒程序奇怪行为的分析"></a>一个黑盒程序奇怪行为的分析</h1><h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><blockquote>
<p>从银行金主baba手里拿到一个区块链程序，监听4000，在我们的环境中4000端口上很快就全连接队列溢出了，导致整个程序不响应了。这个程序是黑盒子，没有源代码，但是在金主baba自己的环境运行正常（一样的OS）</p>
</blockquote>
<p>如下图所示：</p>
<p><img src="/images/oss/623ca2f46084958efa447882cbb58e72.png" alt="image.png"></p>
<p>ss -lnt 看到全连接队列增长到了39，但是netstat -ant找不到这39个连接，本来是想看看队列堆了这么多连接，都是哪些ip连过来的，实际看不到这就奇怪了</p>
<p>同时验证过程发现我们的环境4000端口上开了slb，也就是slb会不停滴探活4000端口，关掉slb探活后一切正常了。</p>
<p>所以总结下来问题就是：</p>
<ol>
<li><p>为什么全连接队列里面的连接netstat看不到这些连接，但是ss能看到总数 </p>
</li>
<li><p>为什么关掉slb就正常了 </p>
</li>
<li><p>为什么应用不accept连接,也不close（应用是个黑盒子） </p>
</li>
</ol>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="为什么全连接队列里面的连接netstat-ss都看不到-ss能看到总数"><a href="#为什么全连接队列里面的连接netstat-ss都看不到-ss能看到总数" class="headerlink" title="为什么全连接队列里面的连接netstat/ss都看不到(ss能看到总数)"></a>为什么全连接队列里面的连接netstat/ss都看不到(ss能看到总数)</h3><p>这是因为这些连接都是探活连接，三次握手后很快被slb reset了，在OS层面这个连接已经被释放，所以肯定看不见。反过来想要是netstat能看见这个连接，那么它的状态是什么？ reset吗？tcp连接状态里肯定是没有reset状态的。</p>
<p>ss看到的总数是指只要这个连接没有被accept，那么连接队列里就还有这个连接，通过ss也能看到连接队列数量。</p>
<h4 id="为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？"><a href="#为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？" class="headerlink" title="为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？"></a>为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？</h4><p>那是因为正常情况都是能看到的，从没有考虑过握手后很快reset的情况。也没反问过如果能看到这个连接该是什么状态呢？</p>
<h4 id="这个连接被reset后，kernel会将全连接队列数量减1吗？"><a href="#这个连接被reset后，kernel会将全连接队列数量减1吗？" class="headerlink" title="这个连接被reset后，kernel会将全连接队列数量减1吗？"></a>这个连接被reset后，kernel会将全连接队列数量减1吗？</h4><p>不会，按照我们的理解连接被reset释放后，那么kernel要释放全连接队列里面的这个连接，因为这些动作都是kernel负责，上层没法处理这个reset。实际上内核认为所有 listen 到的连接, 必须要 accept 走, 用户有权利知道存在过这么一个连接。</p>
<p>也就是reset后，连接在内核层面释放了，所以netstat/ss看不到，但是全连接队列里面的应用数不会减1，只有应用accept后队列才会减1，accept这个空连接后读写会报错。基本可以认为全连接队列溢出了，主要是应用accept太慢导致的。</p>
<p>当应用从accept队列读取到已经被reset了的连接的时候应用层会得到一个报错信息。</p>
<h4 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h4><p>三次握手成功就变成 ESTABLISHED 了，三次握手成功的第一是收到第三步的ack并且全连接队列没有满，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>下面是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</div><div class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </div><div class="line">LISTEN      73     1024            *:terabase                 *:*</div></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;  </div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</div><div class="line"># ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </div><div class="line">LISTEN      1025   1024             *:terabase              *:*</div></pre></td></tr></table></figure>
<h3 id="为什么关掉slb就正常了"><a href="#为什么关掉slb就正常了" class="headerlink" title="为什么关掉slb就正常了"></a>为什么关掉slb就正常了</h3><p>slb探活逻辑是向监听端口发起三次握手，握手成功后立即发送一个reset断开连接</p>
<p>这是一个完整的探活过程：</p>
<p><img src="/images/oss/b81dcbaea26a5130383d0bc8317fd3c5.png" alt="image.png"></p>
<p>关掉就正常后要结合第三个问题来讲</p>
<h3 id="为什么应用不accept连接-也不close（应用是个黑盒子）"><a href="#为什么应用不accept连接-也不close（应用是个黑盒子）" class="headerlink" title="为什么应用不accept连接,也不close（应用是个黑盒子）"></a>为什么应用不accept连接,也不close（应用是个黑盒子）</h3><p>因为应用是个黑盒子，看不到源代码，只能从行为来分析了</p>
<p>从行为来看，这个应用在三次握手后，会主动给client发送一个12字节的数据，但是这个逻辑写在了accept主逻辑内部，一旦主动给client发12字节数据失败（比如这个连接reset了）那么一直卡在这里导致应用不再accept也不再close。</p>
<p>正确的实现逻辑是，accept在一个单独的线程里，一旦accept到一个新连接，那么就开启一个新的线程来处理这个新连接的读写。accept线程专注accept。</p>
<p>关掉slb后应用有机会发出这12个字节，然后accept就能继续了，否则就卡死了。</p>
<h2 id="一些验证"><a href="#一些验证" class="headerlink" title="一些验证"></a>一些验证</h2><h3 id="nc测试连接4000端口"><a href="#nc测试连接4000端口" class="headerlink" title="nc测试连接4000端口"></a>nc测试连接4000端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># nc -p 12346 dcep-blockchain-1 4000</div><div class="line"> //握手后4000返回的内容</div><div class="line"></div><div class="line">抓包：</div><div class="line">11:03:16.762547 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [S], seq 397659761, win 43690, options [mss 65495,sackOK,TS val 2329725964 ecr 0,nop,wscale 7], length 0</div><div class="line">04:42:24.466211 IP dcep-blockchain-1.terabase &gt; dcep-blockchain-1.12346: Flags [S.], seq 4239354556, ack 397659762, win 43690, options [mss 65495,sackOK,TS val 2329725964 ecr 2329725964,nop,wscale 7], length 0</div><div class="line">11:03:16.762571 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [.], ack 1, win 342, options [nop,nop,TS val 2329725964 ecr 2329725964], length 0</div><div class="line"></div><div class="line">----到这三次握手完毕，下面是隔了大概1.5ms，4000发了12字节给nc</div><div class="line">11:03:16.763893 IP dcep-blockchain-1.terabase &gt; dcep-blockchain-1.12346: Flags [P.], seq 1:13, ack 1, win 342, options [nop,nop,TS val 2329725966 ecr 2329725964], length 12</div><div class="line">11:03:16.763904 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [.], ack 13, win 342, options [nop,nop,TS val 2329725966 ecr 2329725966], length 0</div></pre></td></tr></table></figure>
<p>如果在上面的1.5ms之间nc  reset了这个连接，那么这12字节就发不出来了</p>
<h3 id="握手后Server主动发数据的行为非常像MySQL-Server"><a href="#握手后Server主动发数据的行为非常像MySQL-Server" class="headerlink" title="握手后Server主动发数据的行为非常像MySQL Server"></a>握手后Server主动发数据的行为非常像MySQL Server</h3><p>MySQL Server在收到mysql client连接后会主动发送 Server Greeting、版本号、认证方式等给client</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">#nc -p 12345 127.0.0.1 3306</div><div class="line">J</div><div class="line">5.6.29�CuaV9v0xo�!</div><div class="line">                  qCHRrGRIJqvzmysql_native_password  </div><div class="line">                  </div><div class="line">#tcpdump -i any port 12345 -ennt</div><div class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</div><div class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 76: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [S], seq 3186409724, win 43690, options [mss 65495,sackOK,TS val 3967896050 ecr 0,nop,wscale 7], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 76: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [S.], seq 4188709983, ack 3186409725, win 43690, options [mss 65495,sackOK,TS val 3967896051 ecr 3967896050,nop,wscale 7], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 1, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 0 // 握手完毕</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 146: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [P.], seq 1:79, ack 1, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 78 //Server 发出Greeting</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 79, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [F.], seq 79, ack 1, win 342, options [nop,nop,TS val 3967913551 ecr 3967896051], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 80, win 342, options [nop,nop,TS val 3967913591 ecr 3967913551], length 0</div></pre></td></tr></table></figure>
<p>如下是Server发出的长度为 78 的 Server Greeting信息内容</p>
<p><img src="/images/oss/203c52d94018bbf72dfd4fc64d8a237b.png" alt="image.png"></p>
<p>理论上如果slb探活连接检查MySQL Server的状态的时候也是很快reset了，如果MySQL Server程序写得烂的话也会出现同样的情况。</p>
<p>但是比如我们有实验验证MySQL  Server 是否正常的时候会用 nc 去测试，一般以能看到</p>
<blockquote>
<p>5.6.29�CuaV9v0xo�!<br>                  qCHRrGRIJqvzmysql_native_password </p>
</blockquote>
<p>就认为MySQL Server是正常的。但是真的是这样吗？我们看看 nc 的如下案例</p>
<h4 id="nc-6-4-快速fin"><a href="#nc-6-4-快速fin" class="headerlink" title="nc 6.4 快速fin"></a>nc 6.4 快速fin</h4><blockquote>
<p>#nc –version<br>Ncat: Version 6.40 ( <a href="http://nmap.org/ncat" target="_blank" rel="external">http://nmap.org/ncat</a> )</p>
</blockquote>
<p>用 nc 测试发现有一定的概率没有出现上面的Server Greeting信息，那么这是因为MySQL Server服务不正常了吗？</p>
<p><img src="/images/oss/1607660605575-1305739f-1621-4a01-89ad-0f81eef94922.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<blockquote>
<p> nc -i 3 10.97.170.11 3306 -w 4 -p 1234</p>
</blockquote>
<p>-i 3 表示握手成功后 等三秒钟nc退出（发fin）</p>
<p>nc 6.4 握手后立即发fin断开连接，导致可能收不到Greeting，换成7.5或者mysql client就OK了</p>
<p>nc 7.5的抓包，明显可以看到nc在发fin前会先等4秒钟：</p>
<p><img src="/images/oss/1607660937618-d66c4074-9aa2-44cb-8054-f7d3680d1181.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<h3 id="tcpping-模拟slb-探活"><a href="#tcpping-模拟slb-探活" class="headerlink" title="tcpping 模拟slb 探活"></a>tcpping 模拟slb 探活</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tcpping.py -R -i 0.1 -t 1 dcep-blockchain-1 4000</div></pre></td></tr></table></figure>
<p>-i 间隔0.1秒 </p>
<p>-R reset断开连接</p>
<p>-t 超时时间1秒</p>
<p>执行如上代码，跟4000端口握手，然后立即发出reset断开连接（完全模拟slb探活行为），很快重现了问题</p>
<p>增加延时</p>
<p>-D 0.01表示握手成功后10ms后再发出reset（让应用有机会成功发出那12个字节），应用工作正常</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tcpping.py -R -i 0.1 -t 1 -D 0.01 dcep-blockchain-1 4000</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最大的错误认知就是 ss 看到的全连接队列数量，netstat也能看到。实际是不一定，而这个快速reset+应用不accept就导致了看不到这个现象</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/01/28/journald和rsyslog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="weibo @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/28/journald和rsyslog/" itemprop="url">journald和rsyslogd</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-28T17:30:03+08:00">
                2021-01-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="journald和rsyslogd"><a href="#journald和rsyslogd" class="headerlink" title="journald和rsyslogd"></a>journald和rsyslogd</h1><p>碰到rsyslog-8.24.0-34.1.al7.x86_64 的 rsyslogd 占用内存过高，于是分析了一下原因并学习了一下系统日志、rsyslog、journald之间的关系，流水账记录此文。</p>
<h2 id="rsyslogd-占用内存过高的分析"><a href="#rsyslogd-占用内存过高的分析" class="headerlink" title="rsyslogd 占用内存过高的分析"></a>rsyslogd 占用内存过高的分析</h2><p>rsyslogd使用了大概1.6-2G内存，不正常（正常情况下内存占用30-50M之间）</p>
<p>现象：</p>
<p><img src="/images/oss/12d137f9416d7935dbe6540c626ca8b4.png" alt="image.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">KiB Mem :  7971268 total,   131436 free,  7712020 used,   127812 buff/cache</div><div class="line">KiB Swap:        0 total,        0 free,        0 used.    43484 avail Mem</div><div class="line"></div><div class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</div><div class="line">24850 admin     20   0 8743896   5.1g      0 S   2.0 66.9   1413:55 java</div><div class="line"> 1318 root      20   0 2380404   1.6g    536 S   0.0 21.6 199:09.36 rsyslogd</div><div class="line"> </div><div class="line"># systemctl status rsyslog</div><div class="line">● rsyslog.service - System Logging Service</div><div class="line">   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Tue 2020-10-20 16:01:01 CST; 3 months 8 days ago</div><div class="line">     Docs: man:rsyslogd(8)</div><div class="line">           http://www.rsyslog.com/doc/</div><div class="line"> Main PID: 1318 (rsyslogd)</div><div class="line">   CGroup: /system.slice/rsyslog.service</div><div class="line">           └─1318 /usr/sbin/rsyslogd -n</div><div class="line"></div><div class="line">Jan 28 09:10:07 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 09:10:07 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 10:27:48 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 10:27:49 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 11:45:23 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 11:45:24 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 13:03:00 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 13:03:01 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 14:20:42 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 14:20:42 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ] </div><div class="line"></div><div class="line"></div><div class="line"># grep HUPed /var/log/messages</div><div class="line">Jan 24 03:39:15 iZwz95gaul6x9167sqdqz4Z rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;8.24.0-34.1.al7&quot; x-pid=&quot;1318&quot; x-info=&quot;http://www.rsyslog.com&quot;] rsyslogd was HUPed</div><div class="line"></div><div class="line"># journalctl --verify</div><div class="line">PASS: /var/log/journal/20190829214900434421844640356160/system@efef6fd56e2e4c9f861d0be25c8c0781-0000000001567546-0005b9e2e02a0a4f.journal</div><div class="line">PASS: /var/log/journal/20190829214900434421844640356160/system@efef6fd56e2e4c9f861d0be25c8c0781-00000000015ae56b-0005b9ea76e922e9.journal</div><div class="line">1be1e0: Data object references invalid entry at 1d03018</div><div class="line">File corruption detected at /var/log/journal/20190829214900434421844640356160/system.journal:1d02d80 (of 33554432 bytes, 90%).</div><div class="line">FAIL: /var/log/journal/20190829214900434421844640356160/system.journal (Bad message)</div></pre></td></tr></table></figure>
<p><code>journalctl --verify</code>命令检查发现系统日志卷文件损坏</p>
<h3 id="问题根因"><a href="#问题根因" class="headerlink" title="问题根因"></a>问题根因</h3><p><a href="https://access.redhat.com/solutions/3705051" target="_blank" rel="external">来自redhat官网的描述</a></p>
<p><img src="/images/oss/e1a1cd75553b5cbe2a64e835ba9f99a7.png" alt="image.png"></p>
<p>以下是现场收集到的日志：</p>
<p><img src="/images/oss/cdfe3fb8d50ee148b816a82a432f1b88.png" alt="image.png"></p>
<p>主要是rsyslogd的sd_journal_get_cursor报错，然后导致内存泄露。</p>
<p>journald 报Bad message, 跟rsyslogd内存泄露完全没关系，实际上升级rsyslogd后也有journald bad message,但是rsyslogd的内存一直稳定在30M以内</p>
<p><a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="external">这个CSDN的文章中有完全一样的症状</a> 但是作者的结论是：这是systemd的bug，在journald需要压缩的时候就会发生这个问题。实际上我用的是 systemd-219-62.6.al7.9.x86_64 比他描述的已经修复的版本还要要新，也还是有这个问题，所以这个结论是不对的</p>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>1、重启rsyslog <code>systemctl restart rsyslog</code> 可以释放内存</p>
<p>2、升级rsyslog到rsyslog-8.24.0-38.1.al7.x86_64或更新的版本才能彻底修复这个问题</p>
<h3 id="一些配置方法"><a href="#一些配置方法" class="headerlink" title="一些配置方法"></a>一些配置方法</h3><p>修改配置/etc/rsyslog.conf，增加如下两行，然后重启<code>systemctl restart rsyslog</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$imjournalRatelimitInterval 0</div><div class="line">$imjournalRatelimitBurst 0</div><div class="line">12</div></pre></td></tr></table></figure>
<p>1、关掉journal压缩配置</p>
<p>vi /etc/systemd/journald.conf，把#Compress=yes改成Compress=no，之后systemctl restart systemd-journald即可</p>
<p>2、限制rsyslogd 内存大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">cat /etc/systemd/system/multi-user.target.wants/rsyslog.service</div><div class="line"></div><div class="line">在Service配置中添加MemoryAccounting=yes，MemoryMax=80M，MemoryHigh=8M三项如下所示。</div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line">EnvironmentFile=-/etc/sysconfig/rsyslog</div><div class="line">ExecStart=/usr/sbin/rsyslogd -n $SYSLOGD_OPTIONS</div><div class="line">Restart=on-failure</div><div class="line">UMask=0066</div><div class="line">StandardOutput=null</div><div class="line">Restart=on-failure</div><div class="line">MemoryAccounting=yes</div><div class="line">MemoryMax=80M</div><div class="line">MemoryHigh=8M</div></pre></td></tr></table></figure>
<h2 id="OOM-kill"><a href="#OOM-kill" class="headerlink" title="OOM kill"></a>OOM kill</h2><p>rsyslogd内存消耗过高后导致了OOM Kill</p>
<p><img src="/images/oss/c7332f5b48506ea1faa015cfc6ae1709.png" alt="image.png"></p>
<p><strong>RSS对应物理内存，单位是4K（page大小）</strong>，红框两个进程用了5G+2G，总内存是8G，所以触发OOM killer了</p>
<p>每次OOM Kill日志前后总带着systemd-journald的重启</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z journal: Permanent journal is using 520.0M (max allowed 500.0M, trying to leave 4.0G free of 83.7G available → current limit 520.0M).</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z journal: Journal started</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: AliYunDun invoked oom-killer: gfp_mask=0x6200ca(GFP_HIGHUSER_MOVABLE), nodemask=(null), order=0, oom_score_adj=0</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: AliYunDun cpuset=/ mems_allowed=0</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: CPU: 3 PID: 13296 Comm: AliYunDun Tainted: G           OE     4.19.57-15.1.al7.x86_64 #1</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/01/2014</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Call Trace:</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: dump_stack+0x5c/0x7b</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: dump_header+0x77/0x29f</div><div class="line">***</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: [  18118]     0 18118    28218      255   245760        0             0 sshd</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Out of memory: Kill process 18665 (java) score 617 or sacrifice child</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Killed process 18665 (java) total-vm:8446992kB, anon-rss:4905856kB, file-rss:0kB, shmem-rss:0kB</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: oom_reaper: reaped process 18665 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z systemd: systemd-journald.service watchdog timeout (limit 3min)!</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z rsyslogd: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z rsyslogd: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 20:14:38 iZwz95gaul6x9167sqdqz5Z rsyslogd: imjournal: journal reloaded... [v8.24.0-57.1.al7 try http://www.rsyslog.com/e/0 ]</div></pre></td></tr></table></figure>
<p><img src="/images/oss/45008a8323742fb7f145211a6281afbc.png" alt="image.png"></p>
<p>OOM kill前大概率伴随着systemd-journald 重启是因为watch dog timeout(limit 3min)，造成timeout的原因是journald定期要把日志刷到磁盘上，然后要么是内存不够，要么是io负载太重，导致刷磁盘这个过程非常慢，于是就timeout了。</p>
<p>当然systemd-journald 重启也不一定意味着OOM Killer，只是肯定是内存比较紧张了。</p>
<h2 id="What-is-the-difference-between-syslog-rsyslog-and-syslog-ng"><a href="#What-is-the-difference-between-syslog-rsyslog-and-syslog-ng" class="headerlink" title="What is the difference between syslog, rsyslog and syslog-ng? "></a><a href="https://serverfault.com/questions/692309/what-is-the-difference-between-syslog-rsyslog-and-syslog-ng" target="_blank" rel="external">What is the difference between syslog, rsyslog and syslog-ng? </a></h2><p>Basically, they are all the same, in the way they all permit the logging of data from different types of systems in a central repository.</p>
<p>But they are three different project, each project trying to improve the previous one with more reliability and functionalities.</p>
<p>The <code>Syslog</code> project was the very first project. It started in 1980. It is the root project to <code>Syslog</code> protocol. At this time Syslog is a very simple protocol. At the beginning it only supports UDP for transport, so that it does not guarantee the delivery of the messages.</p>
<p>Next came <code>syslog-ng</code> in 1998. It extends basic <code>syslog</code> protocol with new features like:</p>
<ul>
<li>content-based filtering</li>
<li>Logging directly into a database</li>
<li>TCP for transport</li>
<li>TLS encryption</li>
</ul>
<p>Next came <code>Rsyslog</code> in 2004. It extends <code>syslog</code> protocol with new features like:</p>
<ul>
<li>RELP Protocol support</li>
<li>Buffered operation support</li>
</ul>
<h2 id="rsyslog和journald的基础知识"><a href="#rsyslog和journald的基础知识" class="headerlink" title="rsyslog和journald的基础知识"></a>rsyslog和journald的基础知识</h2><p><code>systemd-journald</code>是用来协助<code>rsyslog</code>记录系统启动服务和服务启动失败的情况等等. <code>systemd-journald</code>使用内存保存记录, 系统重启记录会丢失. 所有还要用<code>rsyslog</code>来记录分类信息, 如上面<code>/etc/rsyslog.d/listen.conf</code>中的<code>syslog</code>分类.</p>
<p><code>systemd-journald</code>跟随systemd开机就启动，能及时记录所有日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># systemd-analyze critical-chain systemd-journald.service</div><div class="line">The time after the unit is active or started is printed after the &quot;@&quot; character.</div><div class="line">The time the unit takes to start is printed after the &quot;+&quot; character.</div><div class="line"></div><div class="line">systemd-journald.service +13ms</div><div class="line">└─system.slice</div><div class="line">  └─-.slice</div></pre></td></tr></table></figure>
<p>systemd-journald 由于是使用于内存的登录文件记录方式，因此重新开机过后，开机前的登录文件信息当然就不会被记载了。 为此，我们还是建议启动 rsyslogd 来协助分类记录！也就是说， systemd-journald 用来管理与查询这次开机后的登录信息，而 rsyslogd 可以用来记录以前及现在的所以数据到磁盘文件中，方便未来进行查询喔！</p>
<p><strong>Tips</strong> 虽然 systemd-journald 所记录的数据其实是在内存中，但是系统还是利用文件的型态将它记录到 /run/log/ 下面！ 不过我们从前面几章也知道， /run 在 CentOS 7 其实是内存内的数据，所以重新开机过后，这个 /run/log 下面的数据当然就被刷新，旧的当然就不再存在了！</p>
<blockquote>
<p>其实鸟哥是这样想的，既然我们还有 rsyslog.service 以及 logrotate 的存在，因此这个 systemd-journald.service 产生的登录文件， 个人建议最好还是放置到 /run/log 的内存当中，以加快存取的速度！而既然 rsyslog.service 可以存放我们的登录文件， 似乎也没有必要再保存一份 journal 登录文件到系统当中就是了。单纯的建议！如何处理，依照您的需求即可喔！</p>
</blockquote>
<p><strong><code>system-journal</code>服务监听 <code>/dev/log</code> socket获取日志, 保存在内存中, 并间歇性的写入<code>/var/log/journal</code>目录中.</strong></p>
<p><code>rsyslog</code>服务启动后监听<code>/run/systemd/journal/socket</code> 获取syslog类型日志, 并写入<code>/var/log/messages</code>文件中. </p>
<p>获取日志时需要记录日志条目的<code>position</code>到<code>/var/lib/rsyslog/imjournal.state</code>文件中.</p>
<p>比如haproxy日志配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># cat /etc/haproxy/haproxy.cfg</div><div class="line">global</div><div class="line"># log发给journald(journald监听 /dev/log)</div><div class="line">        log /dev/log    local1 warning</div></pre></td></tr></table></figure>
<p>以下是drds 的iptables日志配置，将tcp reset包记录下来，默认iptable日志输出到/varlog/messages中（dmesg也能看到），然后可以通过rsyslog.d 配置将这部分日志输出到单独的文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># 配置iptables 日志，增加 [drds] 标识</div><div class="line"># cat /home/admin/drds-worker/install/drds_filter.conf</div><div class="line"># Generated by iptables-save v1.4.21 on Wed Apr  1 11:39:31 2020</div><div class="line">*filter</div><div class="line">:INPUT ACCEPT [557:88127]</div><div class="line">:FORWARD ACCEPT [0:0]</div><div class="line">:OUTPUT ACCEPT [527:171711]</div><div class="line">-A INPUT -p tcp -m tcp ! --sport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</div><div class="line"># -A INPUT -p tcp -m tcp ! --dport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level7 --log-tcp-sequence --log-tcp-options --log-ip-options</div><div class="line">-A OUTPUT -p tcp -m tcp ! --sport 3406 --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</div><div class="line">COMMIT</div><div class="line"># Completed on Wed Apr  1 11:39:31 2020</div><div class="line"></div><div class="line">#通过rsyslogd将日志写出到指定位置(不配置的话默认输出到 dmesg)</div><div class="line"># cat /etc/rsyslog.d/drds_filter_log.conf</div><div class="line">:msg, startswith, &quot;[drds]&quot; -/home/admin/logs/tcp-rt/drds-tcp.log</div></pre></td></tr></table></figure>
<h3 id="journald-log持久化"><a href="#journald-log持久化" class="headerlink" title="journald log持久化"></a>journald log持久化</h3><p>创建 /var/log/journal 文件夹后默认会持久化，设置持久化后 /run/log 里面就没有日志了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"># cat /etc/systemd/journald.conf</div><div class="line">#  This file is part of systemd.</div><div class="line">#</div><div class="line">#  systemd is free software; you can redistribute it and/or modify it</div><div class="line">#  under the terms of the GNU Lesser General Public License as published by</div><div class="line">#  the Free Software Foundation; either version 2.1 of the License, or</div><div class="line">#  (at your option) any later version.</div><div class="line">#</div><div class="line"># Entries in this file show the compile time defaults.</div><div class="line"># You can change settings by editing this file.</div><div class="line"># Defaults can be restored by simply deleting this file.</div><div class="line">#</div><div class="line"># See journald.conf(5) for details.</div><div class="line"></div><div class="line">[Journal]</div><div class="line">#Storage=auto  //默认如果有 /var/log/journal 目录就会持久化到这里</div><div class="line">Compress=no</div><div class="line">#Seal=yes</div><div class="line">#SplitMode=uid</div><div class="line">#SyncIntervalSec=5m</div><div class="line">#RateLimitInterval=30s</div><div class="line">#RateLimitBurst=1000</div><div class="line">SystemMaxUse=500M   //最多保留500M日志文件，免得撑爆磁盘</div><div class="line">#SystemKeepFree=</div><div class="line">#SystemMaxFileSize=</div><div class="line">#RuntimeMaxUse=</div><div class="line">#RuntimeKeepFree=</div><div class="line">#RuntimeMaxFileSize=</div><div class="line">#MaxRetentionSec=</div><div class="line">#MaxFileSec=1month</div><div class="line">#ForwardToSyslog=yes</div><div class="line">#ForwardToKMsg=no</div><div class="line">#ForwardToConsole=no</div><div class="line">#ForwardToWall=yes</div><div class="line">#TTYPath=/dev/console</div><div class="line">#MaxLevelStore=debug</div><div class="line">#MaxLevelSyslog=debug</div><div class="line">#MaxLevelKMsg=notice</div><div class="line">#MaxLevelConsole=info</div><div class="line">#MaxLevelWall=emerg</div><div class="line">#LineMax=48K</div></pre></td></tr></table></figure>
<p>清理日志保留1M：journalctl –vacuum-size=1M </p>
<p>设置最大保留500M日志： journalctl –vacuum-size=500</p>
<h3 id="rsyslogd"><a href="#rsyslogd" class="headerlink" title="rsyslogd"></a>rsyslogd</h3><p>以下内容来自鸟哥的书：</p>
<p>CentOS 7 除了保有既有的 rsyslog.service 之外，其实最上游还使用了 systemd 自己的登录文件日志管理功能喔！他使用的是 systemd-journald.service 这个服务来支持的。基本上，系统由 systemd 所管理，那所有经由 systemd 启动的服务，如果再启动或结束的过程中发生一些问题或者是正常的讯息， 就会将该讯息由 systemd-journald.service 以二进制的方式记录下来，之后再将这个讯息发送给 rsyslog.service 作进一步的记载。</p>
<p>基本上， rsyslogd 针对各种服务与讯息记录在某些文件的配置文件就是 /etc/rsyslog.conf， 这个文件规定了“（1）什么服务 （2）的什么等级讯息 （3）需要被记录在哪里（设备或文件）” 这三个咚咚，所以设置的语法会是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$cat /etc/rsyslog.conf</div><div class="line">服务名称[.=!]讯息等级        讯息记录的文件名或设备或主机</div><div class="line"># 下面以 mail 这个服务产生的 info 等级为例：</div><div class="line">mail.info            /var/log/maillog_info</div><div class="line"># 这一行说明：mail 服务产生的大于等于 info 等级的讯息，都记录到</div><div class="line"># /var/log/maillog_info 文件中的意思。</div></pre></td></tr></table></figure>
<p><img src="/images/oss/1cce7612a84cf1a1addceeff6032cb5c.png" alt="syslog 所制订的服务名称与软件调用的方式"></p>
<p> CentOS 7.x 默认的 rsyslogd 本身就已经具有远程日志服务器的功能了， 只是默认并没有启动该功能而已。你可以通过 man rsyslogd 去查询一下相关的选项就能够知道啦！ 既然是远程日志服务器，那么我们的 Linux 主机当然会启动一个端口来监听了，那个默认的端口就是 UDP 或 TCP 的 port 514 </p>
<p><img src="/images/oss/40740cd5cfc8896c07c15b959420646f.png" alt="image.png"></p>
<p>Server配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ cat /etc/rsyslog.conf</div><div class="line"># 找到下面这几行：</div><div class="line"># Provides UDP syslog reception</div><div class="line">#$ModLoad imudp</div><div class="line">#$UDPServerRun 514</div><div class="line"></div><div class="line"># Provides TCP syslog reception</div><div class="line">#$ModLoad imtcp</div><div class="line">#$InputTCPServerRun 514</div><div class="line"># 上面的是 UDP 端口，下面的是 TCP 端口！如果你的网络状态很稳定，就用 UDP 即可。</div><div class="line"># 不过，如果你想要让数据比较稳定传输，那么建议使用 TCP 啰！所以修改下面两行即可！</div><div class="line">$ModLoad imtcp</div><div class="line">$InputTCPServerRun 514</div><div class="line"></div><div class="line"># 2\. 重新启动与观察 rsyslogd 喔！</div><div class="line">[root@study ~]# systemctl restart rsyslog.service</div><div class="line">[root@study ~]# netstat -ltnp &amp;#124; grep syslog</div><div class="line">Proto Recv-Q Send-Q Local Address  Foreign Address   State    PID/Program name</div><div class="line">tcp        0      0 0.0.0.0:514    0.0.0.0:*         LISTEN   2145/rsyslogd</div><div class="line">tcp6       0      0 :::514         :::*              LISTEN   2145/rsyslogd</div><div class="line"># 嘿嘿！你的登录文件主机已经设置妥当啰！很简单吧！</div></pre></td></tr></table></figure>
<p>client配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cat /etc/rsyslog.conf</div><div class="line">*.*       @@192.168.1.100</div><div class="line">#*.*       @192.168.1.100  # 若用 UDP 传输，设置要变这样！</div></pre></td></tr></table></figure>
<p>常见的几个系统日志有哪些呢？一般而言，有下面几个：</p>
<ul>
<li>/var/log/boot.log： 开机的时候系统核心会去侦测与启动硬件，接下来开始各种核心支持的功能启动等。这些流程都会记录在 /var/log/boot.log 里面哩！ 不过这个文件只会存在这次开机启动的信息，前次开机的信息并不会被保留下来！</li>
<li>/var/log/cron： 还记得<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html" target="_blank" rel="external">第十五章例行性工作调度</a>吧？你的 crontab 调度有没有实际被进行？ 进行过程有没有发生错误？你的 /etc/crontab 是否撰写正确？在这个登录文件内查询看看。</li>
<li>/var/log/dmesg： 记录系统在开机的时候核心侦测过程所产生的各项信息。由于 CentOS 默认将开机时核心的硬件侦测过程取消显示， 因此额外将数据记录一份在这个文件中；</li>
<li>/var/log/lastlog： 可以记录系统上面所有的帐号最近一次登陆系统时的相关信息。<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#uselinux_find" target="_blank" rel="external">第十三章讲到的 lastlog</a> 指令就是利用这个文件的记录信息来显示的。</li>
<li>/var/log/maillog 或 /var/log/mail/*： 记录邮件的往来信息，其实主要是记录 postfix （SMTP 协定提供者） 与 dovecot （POP3 协定提供者） 所产生的讯息啦。 SMTP 是发信所使用的通讯协定， POP3 则是收信使用的通讯协定。 postfix 与 dovecot 则分别是两套达成通讯协定的软件。</li>
<li>/var/log/messages： 这个文件相当的重要，几乎系统发生的错误讯息 （或者是重要的信息） 都会记录在这个文件中； 如果系统发生莫名的错误时，这个文件是一定要查阅的登录文件之一。</li>
<li>/var/log/secure： 基本上，只要牵涉到“需要输入帐号密码”的软件，那么当登陆时 （不管登陆正确或错误） 都会被记录在此文件中。 包括系统的 login 程序、图形接口登陆所使用的 gdm 程序、 su, sudo 等程序、还有网络连线的 ssh, telnet 等程序， 登陆信息都会被记载在这里；</li>
<li>/var/log/wtmp, /var/log/faillog： 这两个文件可以记录正确登陆系统者的帐号信息 （wtmp） 与错误登陆时所使用的帐号信息 （faillog） ！ 我们在<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#last" target="_blank" rel="external">第十章谈到的 last</a> 就是读取 wtmp 来显示的， 这对于追踪一般帐号者的使用行为很有帮助！</li>
<li>/var/log/httpd/<em>, /var/log/samba/</em>： 不同的网络服务会使用它们自己的登录文件来记载它们自己产生的各项讯息！上述的目录内则是个别服务所制订的登录文件。</li>
</ul>
<h2 id="journalctl-常用参数"><a href="#journalctl-常用参数" class="headerlink" title="journalctl 常用参数"></a><a href="https://linuxhint.com/journalctl-tail-and-cheatsheet/" target="_blank" rel="external">journalctl 常用参数</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">-n or –lines= Show the most recent **n** number of log lines</div><div class="line"></div><div class="line">-f or –follow Like a tail operation for viewing live updates</div><div class="line"></div><div class="line">-S, –since=, -U, –until= Search based on a date. “2019-07-04 13:19:17”, “00:00:00”, “yesterday”, “today”, “tomorrow”, “now” are valid formats. For complete time and date specification, see systemd.time(7)</div><div class="line"></div><div class="line">-u service unit</div></pre></td></tr></table></figure>
<p>清理journald日志</p>
<blockquote>
<p> journalctl –vacuum-size=1M &amp;&amp; journalctl –vacuum-size=500</p>
</blockquote>
<h2 id="logrotate"><a href="#logrotate" class="headerlink" title="logrotate"></a>logrotate</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">/var/log/cron</div><div class="line">&#123;</div><div class="line">    sharedscripts</div><div class="line">    postrotate</div><div class="line">        /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</div><div class="line">    endscript</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="kill-HUP"><a href="#kill-HUP" class="headerlink" title="kill -HUP"></a><a href="https://unix.stackexchange.com/questions/440004/why-is-kill-hup-used-in-logrotate-in-rhel-is-it-necessary-in-all-cases" target="_blank" rel="external">kill -HUP</a></h3><p>Generally services keep the log files opened while they are running. This mean that they do not care if the log files are renamed/moved or deleted they will continue to write to the open file handled.</p>
<p>When logrotate move the files, the services keep writing to the same file.</p>
<p>Example: syslogd will write to /var/log/cron.log. Then logrotate will rename the file to /var/log/cron.log.1, so syslogd will keep writing to the open file /var/log/cron.log.1.</p>
<p>Sending the HUP signal to syslogd will force him to close existing file handle and open new file handle to the original path /var/log/cron.log which will create a new file.</p>
<p>The use of the HUP signal instead of another one is at the discretion of the program. Some services like php-fpm will listen to the USR1 signal to reopen it’s file handle without terminating itself.</p>
<p>不过还得看应用是否屏蔽了 HUP 信号</p>
<h2 id="systemd"><a href="#systemd" class="headerlink" title="systemd"></a>systemd</h2><p>sudo systemctl list-unit-files –type=service | grep enabled //列出启动项</p>
<p> journalctl -b -1 //复审前一次启动， -2 复审倒数第 2 次启动. 重演你的系统启动的所有消息</p>
<p>sudo systemd-analyze blame   <strong>sudo systemd-analyze critical-chain</strong></p>
<p>systemd-analyze critical-chain –fuzz 1h</p>
<p>sudo systemd-analyze blame networkd</p>
<p>systemd-analyze critical-chain network.target local-fs.target</p>
<p><img src="/images/oss/bb21293e-9b52-40f9-9ab2-7c5aeb7beca1.png" alt="img"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>一模一样的症状，但是根因找错了：<a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="external">rsyslog占用内存高</a> </p>
<p><a href="https://access.redhat.com/solutions/3705051" target="_blank" rel="external">https://access.redhat.com/solutions/3705051</a></p>
<p><a href="https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html" target="_blank" rel="external">https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html</a></p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html" target="_blank" rel="external">鸟哥 journald 介绍</a></p>
<p><a href="https://linuxhint.com/journalctl-tail-and-cheatsheet/" target="_blank" rel="external">journalctl tail and cheatsheet</a></p>
<p><a href="https://lp007819.wordpress.com/2015/01/17/systemd-journal-介绍/" target="_blank" rel="external">Journal的由来</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="weibo @plantegg" />
          <p class="site-author-name" itemprop="name">weibo @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">142</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">235</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">weibo @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
