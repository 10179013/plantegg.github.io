<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/7/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/page/7/"/>





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/24/如何制作本地yum repository/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/24/如何制作本地yum repository/" itemprop="url">如何制作本地yum repository</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-24T17:30:03+08:00">
                2020-01-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何制作本地yum-repository"><a href="#如何制作本地yum-repository" class="headerlink" title="如何制作本地yum repository"></a>如何制作本地yum repository</h1><p>某些情况下在没有外网的环境需要安装一些软件，但是软件依赖比较多，那么可以提前将所有依赖下载到本地，然后将他们制作成一个yum repo，安装的时候就会自动将依赖包都安装好。</p>
<h2 id="收集所有rpm包"><a href="#收集所有rpm包" class="headerlink" title="收集所有rpm包"></a>收集所有rpm包</h2><p>创建一个文件夹，比如 Yum，将收集到的所有rpm包放在里面，比如安装ansible和docker需要的依赖文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-rwxr-xr-x 1 root root  73K 7月  12 14:22 audit-libs-python-2.8.4-4.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root 295K 7月  12 14:22 checkpolicy-2.5-8.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  23M 7月  12 14:22 containerd.io-1.2.2-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  26K 7月  12 14:22 container-selinux-2.9-4.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  37K 7月  12 14:22 container-selinux-2.74-1.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  14M 7月  12 14:22 docker-ce-cli-18.09.0-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  29K 7月  12 14:22 docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-2.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-1.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root 154K 7月  12 14:23 PyYAML-3.10-11.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  29K 7月  12 14:23 python-six-1.9.0-2.el7.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root 397K 7月  12 14:23 python-setuptools-0.9.8-7.el7.noarch.rpm</div></pre></td></tr></table></figure>
<p>收集方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//先安装yum工具</div><div class="line">yum install yum-utils -y</div><div class="line">//将 ansible 依赖包都下载下来</div><div class="line">repoquery --requires --resolve --recursive ansible | xargs -r yumdownloader --destdir=/tmp/ansible</div><div class="line">//将ansible rpm自己下载回来</div><div class="line">yumdownloader --destdir=/tmp/ansible --resolve ansible</div><div class="line">//验证一下依赖关系是完整的</div><div class="line">//repotrack ansible</div></pre></td></tr></table></figure>
<h2 id="创建仓库索引"><a href="#创建仓库索引" class="headerlink" title="创建仓库索引"></a>创建仓库索引</h2><p>需要安装工具 yum install createrepo -y：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># createrepo ./public_yum/</div><div class="line">Spawning worker 0 with 6 pkgs</div><div class="line">Spawning worker 1 with 6 pkgs</div><div class="line">Spawning worker 23 with 5 pkgs</div><div class="line">Workers Finished</div><div class="line">Saving Primary metadata</div><div class="line">Saving file lists metadata</div><div class="line">Saving other metadata</div><div class="line">Generating sqlite DBs</div><div class="line">Sqlite DBs complete</div></pre></td></tr></table></figure>
<p>会在yum文件夹下生成一个索引文件夹 repodata</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x 2 root root 4.0K 7月  12 14:25 repodata</div><div class="line">[root@az1-drds-79 yum]# ls repodata/</div><div class="line">5e15c62fec1fe43c6025ecf4d370d632f4b3f607500016e045ad94b70f87bac3-filelists.xml.gz</div><div class="line">7a314396d6e90532c5c534567f9bd34eee94c3f8945fc2191b225b2861ace2b6-other.xml.gz</div><div class="line">ce9dce19f6b426b8856747b01d51ceaa2e744b6bbd5fbc68733aa3195f724590-primary.xml.gz</div><div class="line">ee33b7d79e32fe6ad813af92a778a0ec8e5cc2dfdc9b16d0be8cff6a13e80d99-filelists.sqlite.bz2</div><div class="line">f7e8177e7207a4ff94bade329a0f6b572a72e21da106dd9144f8b1cdf0489cab-primary.sqlite.bz2</div><div class="line">ff52e1f1859790a7b573d2708b02404eb8b29aa4b0c337bda83af75b305bfb36-other.sqlite.bz2</div><div class="line">repomd.xml</div></pre></td></tr></table></figure>
<h2 id="生成iso镜像文件"><a href="#生成iso镜像文件" class="headerlink" title="生成iso镜像文件"></a>生成iso镜像文件</h2><p>非必要步骤，如果需要带到客户环境可以先生成iso，不过不够灵活。</p>
<p>也可以不用生成iso，直接在drds.repo中指定 createrepo 的目录也可以，记得要先执行 yum clean all和yum update </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#mkisofs -r -o docker_ansible.iso ./yum/</div><div class="line">I: -input-charset not specified, using utf-8 (detected in locale settings)</div><div class="line">Using PYTHO000.RPM;1 for  /python-httplib2-0.7.7-3.el7.noarch.rpm (python-httplib2-0.9.1-3.el7.noarch.rpm)</div><div class="line">Using MARIA006.RPM;1 for  /mariadb-5.5.56-2.el7.x86_64.rpm (mariadb-libs-5.5.56-2.el7.x86_64.rpm)</div><div class="line">Using LIBTO001.RPM;1 for  /libtomcrypt-1.17-25.el7.x86_64.rpm (libtomcrypt-1.17-26.el7.x86_64.rpm)</div><div class="line">  6.11% done, estimate finish Sun Jul 12 14:26:47 2020</div><div class="line"> 97.60% done, estimate finish Sun Jul 12 14:26:48 2020</div><div class="line">Total translation table size: 0</div><div class="line">Total rockridge attributes bytes: 14838</div><div class="line">Total directory bytes: 2048</div><div class="line">Path table size(bytes): 26</div><div class="line">Max brk space used 21000</div><div class="line">81981 extents written (160 MB)</div></pre></td></tr></table></figure>
<h2 id="将-生成的-iso挂载到目标机器上"><a href="#将-生成的-iso挂载到目标机器上" class="headerlink" title="将 生成的 iso挂载到目标机器上"></a>将 生成的 iso挂载到目标机器上</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mkdir /mnt/iso</div><div class="line"># mount ./docker_ansible.iso /mnt/iso</div><div class="line">mount: /dev/loop0 is write-protected, mounting read-only</div></pre></td></tr></table></figure>
<h2 id="配置本地-yum-源"><a href="#配置本地-yum-源" class="headerlink" title="配置本地 yum 源"></a>配置本地 yum 源</h2><p>yum repository不是必须要求iso挂载，直接指向rpm文件夹（必须要有 createrepo 建立索引了）也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># cat /etc/yum.repos.d/drds.repo </div><div class="line">[drds]</div><div class="line">name=drds Extra Packages for Enterprise Linux 7 - $basearch</div><div class="line">enabled=1</div><div class="line">failovermethod=priority</div><div class="line">baseurl=file:///mnt/repo #baseurl=http://192.168.1.91:8000/ 本地内网</div><div class="line">priority=1  #添加priority=1，数字越小优先级越高，也可以修改网络源的priority的值</div><div class="line">gpgcheck=0</div><div class="line">#gpgkey=file:///mnt/cdrom/RPM-GPG-KEY-CentOS-5    #注：这个你cd /mnt/cdrom/可以看到这个key，这里仅仅是个例子， 因为gpgcheck是0 ，所以gpgkey不需要了</div></pre></td></tr></table></figure>
<p>到此就可以在没有网络环境的机器上直接：yum install ansible docker -y 了 </p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的话可以指定repo 源： yum install ansible –enablerepo=drds （drds 优先级最高）</p>
<p>本地会cache一些rpm的版本信息，可以执行 yum clean all 得到一个干净的测试环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum clean all</div><div class="line">yum list</div><div class="line">yum deplist ansible</div></pre></td></tr></table></figure>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a href="https://access.redhat.com/solutions/641093" target="_blank" rel="external">Yum commands error “pycurl.so: undefined symbol”</a></p>
<h2 id="xargs-作用"><a href="#xargs-作用" class="headerlink" title="xargs 作用"></a>xargs 作用</h2><p><code>xargs</code>命令的作用，是将标准输入转为命令行参数。因为有些命令是不接受标准输入的，比如echo</p>
<p><code>xargs</code>的作用在于，大多数命令（比如<code>rm</code>、<code>mkdir</code>、<code>ls</code>）与管道一起使用时，都需要<code>xargs</code>将标准输入转为命令行参数。</p>
<h2 id="dnf-使用"><a href="#dnf-使用" class="headerlink" title="dnf 使用"></a>dnf 使用</h2><p><strong>DNF</strong> 是新一代的rpm软件包管理器。他首先出现在 Fedora 18 这个发行版中。而最近，它取代了yum，正式成为 Fedora 22 的包管理器。</p>
<p>DNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等多方面的内容。DNF使用 RPM, libsolv 和 hawkey 库进行包管理操作。尽管它没有预装在 CentOS 和 RHEL 7 中，但你可以在使用 YUM 的同时使用 DNF 。你可以在这里获得关于 DNF 的更多知识：《 DNF 代替 YUM ，你所不知道的缘由》</p>
<p>DNF 包管理器作为 YUM 包管理器的升级替代品，它能自动完成更多的操作。但在我看来，正因如此，所以 DNF 包管理器不会太受那些经验老道的 Linux 系统管理者的欢迎。举例如下：</p>
<ol>
<li>在 DNF 中没有 –skip-broken 命令，并且没有替代命令供选择。</li>
<li>在 DNF 中没有判断哪个包提供了指定依赖的 resolvedep 命令。</li>
<li>在 DNF 中没有用来列出某个软件依赖包的 deplist 命令。</li>
<li>当你在 DNF 中排除了某个软件库，那么该操作将会影响到你之后所有的操作，不像在 YUM 下那样，你的排除操作只会咋升级和安装软件时才起作用。</li>
</ol>
<h2 id="安装yum源"><a href="#安装yum源" class="headerlink" title="安装yum源"></a>安装yum源</h2><p>安装7.70版本curl yum源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -Uvh http://www.city-fan.org/ftp/contrib/yum-repo/city-fan.org-release-2-1.rhel7.noarch.rpm</div></pre></td></tr></table></figure>
<h2 id="其它技巧"><a href="#其它技巧" class="headerlink" title="其它技巧"></a>其它技巧</h2><h3 id="rpm依赖查询"><a href="#rpm依赖查询" class="headerlink" title="rpm依赖查询"></a>rpm依赖查询</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">rpm -q --whatprovides file-name //查询一个文件来自哪个rpm包</div><div class="line">rpm -qf /usr/lib/systemd/libsystemd-shared-239.so // 查询一个so lib来自哪个rpm包</div><div class="line">或者 yum whatprovides /usr/lib/systemd/libsystemd-shared-239.so</div><div class="line">yum provides */libmysqlclient.so.18</div></pre></td></tr></table></figure>
<h2 id="制作debian-仓库"><a href="#制作debian-仓库" class="headerlink" title="制作debian 仓库"></a><a href="https://rpmdeb.com/devops-articles/how-to-create-local-debian-repository/" target="_blank" rel="external">制作debian 仓库</a></h2><p>适合ubuntu、deepin、uos等, 参考：<a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">https://lework.github.io/2021/04/03/debian-kubeadm-install/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#添加新仓库</div><div class="line">sudo apt-add-repository &apos;deb http://ftp.us.debian.org/debian stretch main contrib non-free&apos;</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/2-60.png" alt="img"></p>
<p><a href="https://zh-cn.linuxcapable.com/%E5%9C%A8-debian-11-%E9%9D%B6%E5%BF%83%E4%B8%8A%E5%AE%89%E8%A3%85-rpm-%E5%8C%85/" target="_blank" rel="external">rpm转换成 dpkg</a></p>
<h3 id="apt-mirror"><a href="#apt-mirror" class="headerlink" title="apt-mirror"></a><a href="https://www.rickylss.site/os/linux/2020/05/12/debian-repositry/" target="_blank" rel="external">apt-mirror</a></h3><p>先要安装apt-mirror 工具，安装后会生成配置文件 /etc/apt/mirror.list 然后需要手工修改配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">#cat /etc/apt/mirror.list</div><div class="line">############# config ##################</div><div class="line"></div><div class="line">#下载下来的仓库文件放在哪里</div><div class="line">set base_path    /polarx/debian</div><div class="line"></div><div class="line">set mirror_path  $base_path/mirror</div><div class="line">set skel_path    $base_path/skel</div><div class="line">set var_path     $base_path/var</div><div class="line">set cleanscript $var_path/clean.sh</div><div class="line">set defaultarch  amd64</div><div class="line">#set postmirror_script $var_path/postmirror.sh</div><div class="line">set run_postmirror 0</div><div class="line">set nthreads     20</div><div class="line">set _tilde 0</div><div class="line">#</div><div class="line">############# end config ##############</div><div class="line"></div><div class="line">#从哪里镜像仓库</div><div class="line">deb http://yum.tbsite.net/mirrors/debian/ buster main non-free contrib</div><div class="line">#deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</div><div class="line"></div><div class="line">#deb http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-src http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line"># mirror additional architectures</div><div class="line">#deb-alpha http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-amd64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-armel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-hppa http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-i386 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-ia64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-m68k http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mips http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mipsel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-powerpc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-s390 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-sparc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line">#clean http://ftp.us.debian.org/debian</div><div class="line">clean http://yum.tbsite.net/mirrors/debian/</div></pre></td></tr></table></figure>
<h3 id="debian仓库介绍"><a href="#debian仓库介绍" class="headerlink" title="debian仓库介绍"></a><a href="https://wiki.debian.org/zh_CN/DebianRepository" target="_blank" rel="external">debian仓库介绍</a></h3><p>一个Debian仓库包含多个<strong>发行版</strong>。Debian 的发行版是以 “玩具总动员 “电影中的角色命名的 (wheezy, jessie, stretch, …)。 代号有别名，叫做<strong>套件</strong>(stable, oldstable, testing, unstable)。一个发行版会被分成几个<strong>组件</strong>。在 Debian 中，这些组件被命名为 <code>main</code>, <code>contrib</code>, 和 <code>non-free</code>，并表并表示它们所包含的软件的授权条款。一个版本也有各种<strong>架构</strong>(amd64, i386, mips, powerpc, s390x, …)的软件包，以及源码和架构独立的软件包。</p>
<p>仓库的<a href="https://mirrors.aliyun.com/debian/dists/" target="_blank" rel="external">根目录下有一个<code>dists</code> 目录</a>，而这个目录又有每个发行版和套件的目录，后者通常是前者的符号链接，但浏览器不会向您显示出这个区别。每个发行版子目录都包含一个加密签名的<code>Release</code>文件和每个组件的目录，里面是不同架构的目录，名为<code>binary</code>-<em>&lt;架构&gt;</em>和<code>sources</code>。而在这些文件中，<code>Packages</code>是文本文件，包含了软件包。嗯，那么实际的软件包在哪里？</p>
<p><img src="/images/951413iMgBlog/image-20220829163817671.png" alt="image-20220829163817671"></p>
<p>软件包本身在仓库根目录下的<code>pool</code>。在<code>pool</code>下面又有所有组件的目录，其中有<code>0</code>，…，<code>9</code>，<code>a</code>，<code>b</code>，.., <code>z</code>, <code>liba</code>, … , <code>libz</code>。 而在这些目录中，是以它们所包含的软件包命名的目录，这些目录最后包含实际的软件包，即<code>.deb</code>文件。这个名字不一定是软件包本身的名字，例如，软件包bsdutils在目录<code>pool/main/u/util-linux</code> 下，它是生成软件包的源码的名称。一个上游源可能会生成多个二进制软件包，而所有这些软件包最终都会在<code>pool</code>下面的同一个子目录中。额外的单字母目录只是一个技巧，以避免在一个目录中有太多的条目，因为这是很多系统传统上存在性能问题的原因。</p>
<p>在<code>pool</code>下面的子目录中，通常会有多个版本的软件包，而每个版本的软件包属于什么发行版的信息只存在于索引中。这样一来，同一个版本的包可软件以属于多个发行版，但只使用一次磁盘空间，而且不需要求助于硬链接或符号链接，所以镜像相当简单，甚至可以在没有这些概念的系统中进行。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">apt install kubeadm=1.20.12-00 //指定版本安装</div><div class="line"></div><div class="line">#查询可用版本</div><div class="line">apt-cache policy kubeadm</div><div class="line">apt list --all-versions kubeadm</div><div class="line"></div><div class="line">#清理</div><div class="line">apt clean --dry-run </div><div class="line">apt update</div><div class="line">apt list</div><div class="line">apt show kubeadm</div><div class="line"></div><div class="line">#查询安装包的所有文件</div><div class="line">dpkg-query -L kubeadm</div><div class="line"></div><div class="line">#列出所有依赖包</div><div class="line">apt-cache depends ansible</div><div class="line"></div><div class="line">#被依赖查询</div><div class="line">apt-cache rdepends kubelet</div><div class="line"></div><div class="line">dpkg -I kubernetes/pool/kubeadm_1.21.0-00_amd64.deb</div><div class="line"></div><div class="line">#下载依赖包</div><div class="line">apt-get download $(apt-rdepends kubeadm|grep -v &quot;^ &quot;)</div><div class="line">aptitude --download-only -y install $(apt-rdepends kubeadm|grep -v &quot;^ &quot;) //不能下载已经安装了的依赖包</div><div class="line"></div><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<h3 id="简单仓库"><a href="#简单仓库" class="headerlink" title="简单仓库"></a><a href="https://zhuanlan.zhihu.com/p/482592599" target="_blank" rel="external">简单仓库</a></h3><p>下载所有 deb 包以及他们的依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<p>生成 index</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dpkg-scanpackages -m . &gt; Packages</div></pre></td></tr></table></figure>
<p>apt source 指向这个目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">deb [trusted=yes] file:/polarx/test /</div></pre></td></tr></table></figure>
<h3 id="Kubernetes-仓库"><a href="#Kubernetes-仓库" class="headerlink" title="Kubernetes 仓库"></a>Kubernetes 仓库</h3><p><a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">debian 上通过kubeadm 安装 kubernetes 集群</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">//官方</div><div class="line">echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee -a /etc/apt/sources.list.d/kubernetes.list</div><div class="line"></div><div class="line">//阿里云仓库</div><div class="line">echo &apos;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&apos; &gt; /etc/apt/sources.list.d/kubernetes.list</div><div class="line">curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</div><div class="line">apt-get update</div><div class="line"></div><div class="line">export KUBE_VERSION=&quot;1.20.5&quot;</div><div class="line">apt-get install -y kubeadm=$KUBE_VERSION-00 kubelet=$KUBE_VERSION-00 kubectl=$KUBE_VERSION-00</div><div class="line">sudo apt-mark hold kubelet kubeadm kubectl</div><div class="line"></div><div class="line">[ -d /etc/bash_completion.d ] &amp;&amp; \</div><div class="line">    &#123; kubectl completion bash &gt; /etc/bash_completion.d/kubectl; \</div><div class="line">      kubeadm completion bash &gt; /etc/bash_completion.d/kubadm; &#125;</div><div class="line">      </div><div class="line">[ ! -d /usr/lib/systemd/system/kubelet.service.d ] &amp;&amp; mkdir -p /usr/lib/systemd/system/kubelet.service.d</div><div class="line">cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf</div><div class="line">[Service]</div><div class="line">CPUAccounting=true</div><div class="line">MemoryAccounting=true</div><div class="line">BlockIOAccounting=true</div><div class="line">ExecStartPre=/usr/bin/bash -c &apos;/usr/bin/mkdir -p /sys/fs/cgroup/&#123;cpuset,memory,systemd,pids,&quot;cpu,cpuacct&quot;&#125;/&#123;system,kube,kubepods&#125;.slice&apos;</div><div class="line">Slice=kube.slice</div><div class="line">EOF</div><div class="line">systemctl daemon-reload</div><div class="line"> </div><div class="line">systemctl enable kubelet.service</div></pre></td></tr></table></figure>
<h3 id="docker-仓库"><a href="#docker-仓库" class="headerlink" title="docker 仓库"></a>docker 仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apt-get install -y apt-transport-https ca-certificates curl gnupg2 lsb-release bash-completion</div><div class="line">    </div><div class="line">curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add -</div><div class="line">echo &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs)   stable&quot; &gt; /etc/apt/sources.list.d/docker-ce.list</div><div class="line">sudo apt-get update</div><div class="line">apt-get install -y docker-ce docker-ce-cli containerd.io</div><div class="line">apt-mark hold docker-ce docker-ce-cli containerd.io</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ruanyifeng.com/blog/2019/08/xargs-tutorial.html" target="_blank" rel="external">xargs 命令教程</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/24/如何制作本地软件仓库/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/24/如何制作本地软件仓库/" itemprop="url">如何制作本地软件仓库</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-24T17:30:03+08:00">
                2020-01-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何制作软件仓库"><a href="#如何制作软件仓库" class="headerlink" title="如何制作软件仓库"></a>如何制作软件仓库</h1><p>某些情况下在没有外网的环境需要安装一些软件，但是软件依赖比较多，那么可以提前将所有依赖下载到本地，然后将他们制作成一个yum repo，安装的时候就会自动将依赖包都安装好。</p>
<p>centos下是 yum 仓库，Debian、ubuntu下是apt仓库，我们先讲 yum 仓库的制作，Debian apt 仓库类似</p>
<h2 id="收集所有rpm包"><a href="#收集所有rpm包" class="headerlink" title="收集所有rpm包"></a>收集所有rpm包</h2><p>创建一个文件夹，比如 Yum，将收集到的所有rpm包放在里面，比如安装ansible和docker需要的依赖文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-rwxr-xr-x 1 root root  73K 7月  12 14:22 audit-libs-python-2.8.4-4.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root 295K 7月  12 14:22 checkpolicy-2.5-8.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  23M 7月  12 14:22 containerd.io-1.2.2-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  26K 7月  12 14:22 container-selinux-2.9-4.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  37K 7月  12 14:22 container-selinux-2.74-1.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  14M 7月  12 14:22 docker-ce-cli-18.09.0-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  29K 7月  12 14:22 docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-2.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-1.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root 154K 7月  12 14:23 PyYAML-3.10-11.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  29K 7月  12 14:23 python-six-1.9.0-2.el7.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root 397K 7月  12 14:23 python-setuptools-0.9.8-7.el7.noarch.rpm</div></pre></td></tr></table></figure>
<p>收集方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//先安装yum工具</div><div class="line">yum install yum-utils -y</div><div class="line">//将 ansible 依赖包都下载下来</div><div class="line">repoquery --requires --resolve --recursive ansible | xargs -r yumdownloader --destdir=/tmp/ansible</div><div class="line">//将ansible rpm自己下载回来</div><div class="line">yumdownloader --destdir=/tmp/ansible --resolve ansible</div><div class="line">//验证一下依赖关系是完整的</div><div class="line">//repotrack ansible</div></pre></td></tr></table></figure>
<h2 id="创建仓库索引"><a href="#创建仓库索引" class="headerlink" title="创建仓库索引"></a>创建仓库索引</h2><p>需要安装工具 yum install createrepo -y：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># createrepo ./public_yum/</div><div class="line">Spawning worker 0 with 6 pkgs</div><div class="line">Spawning worker 1 with 6 pkgs</div><div class="line">Spawning worker 23 with 5 pkgs</div><div class="line">Workers Finished</div><div class="line">Saving Primary metadata</div><div class="line">Saving file lists metadata</div><div class="line">Saving other metadata</div><div class="line">Generating sqlite DBs</div><div class="line">Sqlite DBs complete</div></pre></td></tr></table></figure>
<p>会在yum文件夹下生成一个索引文件夹 repodata</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x 2 root root 4.0K 7月  12 14:25 repodata</div><div class="line">[root@az1-drds-79 yum]# ls repodata/</div><div class="line">5e15c62fec1fe43c6025ecf4d370d632f4b3f607500016e045ad94b70f87bac3-filelists.xml.gz</div><div class="line">7a314396d6e90532c5c534567f9bd34eee94c3f8945fc2191b225b2861ace2b6-other.xml.gz</div><div class="line">ce9dce19f6b426b8856747b01d51ceaa2e744b6bbd5fbc68733aa3195f724590-primary.xml.gz</div><div class="line">ee33b7d79e32fe6ad813af92a778a0ec8e5cc2dfdc9b16d0be8cff6a13e80d99-filelists.sqlite.bz2</div><div class="line">f7e8177e7207a4ff94bade329a0f6b572a72e21da106dd9144f8b1cdf0489cab-primary.sqlite.bz2</div><div class="line">ff52e1f1859790a7b573d2708b02404eb8b29aa4b0c337bda83af75b305bfb36-other.sqlite.bz2</div><div class="line">repomd.xml</div></pre></td></tr></table></figure>
<h2 id="生成iso镜像文件"><a href="#生成iso镜像文件" class="headerlink" title="生成iso镜像文件"></a>生成iso镜像文件</h2><p>非必要步骤，如果需要带到客户环境可以先生成iso，不过不够灵活。</p>
<p>也可以不用生成iso，直接在drds.repo中指定 createrepo 的目录也可以，记得要先执行 yum clean all和yum update </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#mkisofs -r -o docker_ansible.iso ./yum/</div><div class="line">I: -input-charset not specified, using utf-8 (detected in locale settings)</div><div class="line">Using PYTHO000.RPM;1 for  /python-httplib2-0.7.7-3.el7.noarch.rpm (python-httplib2-0.9.1-3.el7.noarch.rpm)</div><div class="line">Using MARIA006.RPM;1 for  /mariadb-5.5.56-2.el7.x86_64.rpm (mariadb-libs-5.5.56-2.el7.x86_64.rpm)</div><div class="line">Using LIBTO001.RPM;1 for  /libtomcrypt-1.17-25.el7.x86_64.rpm (libtomcrypt-1.17-26.el7.x86_64.rpm)</div><div class="line">  6.11% done, estimate finish Sun Jul 12 14:26:47 2020</div><div class="line"> 97.60% done, estimate finish Sun Jul 12 14:26:48 2020</div><div class="line">Total translation table size: 0</div><div class="line">Total rockridge attributes bytes: 14838</div><div class="line">Total directory bytes: 2048</div><div class="line">Path table size(bytes): 26</div><div class="line">Max brk space used 21000</div><div class="line">81981 extents written (160 MB)</div></pre></td></tr></table></figure>
<h2 id="将-生成的-iso挂载到目标机器上"><a href="#将-生成的-iso挂载到目标机器上" class="headerlink" title="将 生成的 iso挂载到目标机器上"></a>将 生成的 iso挂载到目标机器上</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mkdir /mnt/iso</div><div class="line"># mount ./docker_ansible.iso /mnt/iso</div><div class="line">mount: /dev/loop0 is write-protected, mounting read-only</div></pre></td></tr></table></figure>
<h2 id="配置本地-yum-源"><a href="#配置本地-yum-源" class="headerlink" title="配置本地 yum 源"></a>配置本地 yum 源</h2><p>yum repository不是必须要求iso挂载，直接指向rpm文件夹（必须要有 createrepo 建立索引了）也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># cat /etc/yum.repos.d/drds.repo </div><div class="line">[drds]</div><div class="line">name=drds Extra Packages for Enterprise Linux 7 - $basearch</div><div class="line">enabled=1</div><div class="line">failovermethod=priority</div><div class="line">baseurl=file:///mnt/repo #baseurl=http://192.168.1.91:8000/ 本地内网</div><div class="line">priority=1  #添加priority=1，数字越小优先级越高，也可以修改网络源的priority的值</div><div class="line">gpgcheck=0</div><div class="line">#gpgkey=file:///mnt/cdrom/RPM-GPG-KEY-CentOS-5    #注：这个你cd /mnt/cdrom/可以看到这个key，这里仅仅是个例子， 因为gpgcheck是0 ，所以gpgkey不需要了</div></pre></td></tr></table></figure>
<p>到此就可以在没有网络环境的机器上直接：yum install ansible docker -y 了 </p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的话可以指定repo 源： yum install ansible –enablerepo=drds （drds 优先级最高）</p>
<p>本地会cache一些rpm的版本信息，可以执行 yum clean all 得到一个干净的测试环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum clean all</div><div class="line">yum list</div><div class="line">yum deplist ansible</div></pre></td></tr></table></figure>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a href="https://access.redhat.com/solutions/641093" target="_blank" rel="external">Yum commands error “pycurl.so: undefined symbol”</a></p>
<h2 id="xargs-作用"><a href="#xargs-作用" class="headerlink" title="xargs 作用"></a>xargs 作用</h2><p><code>xargs</code>命令的作用，是将标准输入转为命令行参数。因为有些命令是不接受标准输入的，比如echo</p>
<p><code>xargs</code>的作用在于，大多数命令（比如<code>rm</code>、<code>mkdir</code>、<code>ls</code>）与管道一起使用时，都需要<code>xargs</code>将标准输入转为命令行参数。</p>
<h2 id="dnf-使用"><a href="#dnf-使用" class="headerlink" title="dnf 使用"></a>dnf 使用</h2><p><strong>DNF</strong> 是新一代的rpm软件包管理器。他首先出现在 Fedora 18 这个发行版中。而最近，它取代了yum，正式成为 Fedora 22 的包管理器。</p>
<p>DNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等多方面的内容。DNF使用 RPM, libsolv 和 hawkey 库进行包管理操作。尽管它没有预装在 CentOS 和 RHEL 7 中，但你可以在使用 YUM 的同时使用 DNF 。你可以在这里获得关于 DNF 的更多知识：《 DNF 代替 YUM ，你所不知道的缘由》</p>
<p>DNF 包管理器作为 YUM 包管理器的升级替代品，它能自动完成更多的操作。但在我看来，正因如此，所以 DNF 包管理器不会太受那些经验老道的 Linux 系统管理者的欢迎。举例如下：</p>
<ol>
<li>在 DNF 中没有 –skip-broken 命令，并且没有替代命令供选择。</li>
<li>在 DNF 中没有判断哪个包提供了指定依赖的 resolvedep 命令。</li>
<li>在 DNF 中没有用来列出某个软件依赖包的 deplist 命令。</li>
<li>当你在 DNF 中排除了某个软件库，那么该操作将会影响到你之后所有的操作，不像在 YUM 下那样，你的排除操作只会咋升级和安装软件时才起作用。</li>
</ol>
<h2 id="安装yum源"><a href="#安装yum源" class="headerlink" title="安装yum源"></a>安装yum源</h2><p>安装7.70版本curl yum源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -Uvh http://www.city-fan.org/ftp/contrib/yum-repo/city-fan.org-release-2-1.rhel7.noarch.rpm</div></pre></td></tr></table></figure>
<h2 id="其它技巧"><a href="#其它技巧" class="headerlink" title="其它技巧"></a>其它技巧</h2><h3 id="rpm依赖查询"><a href="#rpm依赖查询" class="headerlink" title="rpm依赖查询"></a>rpm依赖查询</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">rpm -q --whatprovides file-name //查询一个文件来自哪个rpm包</div><div class="line">rpm -qf /usr/lib/systemd/libsystemd-shared-239.so // 查询一个so lib来自哪个rpm包</div><div class="line">或者 yum whatprovides /usr/lib/systemd/libsystemd-shared-239.so</div><div class="line">yum provides */libmysqlclient.so.18</div></pre></td></tr></table></figure>
<h2 id="制作debian-仓库"><a href="#制作debian-仓库" class="headerlink" title="制作debian 仓库"></a><a href="https://rpmdeb.com/devops-articles/how-to-create-local-debian-repository/" target="_blank" rel="external">制作debian 仓库</a></h2><p>适合ubuntu、deepin、uos等, 参考：<a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">https://lework.github.io/2021/04/03/debian-kubeadm-install/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#添加新仓库</div><div class="line">sudo apt-add-repository &apos;deb http://ftp.us.debian.org/debian stretch main contrib non-free&apos;</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/2-60.png" alt="img"></p>
<p><a href="https://zh-cn.linuxcapable.com/%E5%9C%A8-debian-11-%E9%9D%B6%E5%BF%83%E4%B8%8A%E5%AE%89%E8%A3%85-rpm-%E5%8C%85/" target="_blank" rel="external">rpm转换成 dpkg</a></p>
<h3 id="apt-mirror"><a href="#apt-mirror" class="headerlink" title="apt-mirror"></a><a href="https://www.rickylss.site/os/linux/2020/05/12/debian-repositry/" target="_blank" rel="external">apt-mirror</a></h3><p>先要安装apt-mirror 工具，安装后会生成配置文件 /etc/apt/mirror.list 然后需要手工修改配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">#cat /etc/apt/mirror.list</div><div class="line">############# config ##################</div><div class="line"></div><div class="line">#下载下来的仓库文件放在哪里</div><div class="line">set base_path    /polarx/debian</div><div class="line"></div><div class="line">set mirror_path  $base_path/mirror</div><div class="line">set skel_path    $base_path/skel</div><div class="line">set var_path     $base_path/var</div><div class="line">set cleanscript $var_path/clean.sh</div><div class="line">set defaultarch  amd64</div><div class="line">#set postmirror_script $var_path/postmirror.sh</div><div class="line">set run_postmirror 0</div><div class="line">set nthreads     20</div><div class="line">set _tilde 0</div><div class="line">#</div><div class="line">############# end config ##############</div><div class="line"></div><div class="line">#从哪里镜像仓库</div><div class="line">deb http://yum.tbsite.net/mirrors/debian/ buster main non-free contrib</div><div class="line">#deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</div><div class="line"></div><div class="line">#deb http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-src http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line"># mirror additional architectures</div><div class="line">#deb-alpha http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-amd64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-armel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-hppa http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-i386 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-ia64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-m68k http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mips http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mipsel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-powerpc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-s390 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-sparc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line">#clean http://ftp.us.debian.org/debian</div><div class="line">clean http://yum.tbsite.net/mirrors/debian/</div></pre></td></tr></table></figure>
<h3 id="debian仓库介绍"><a href="#debian仓库介绍" class="headerlink" title="debian仓库介绍"></a><a href="https://wiki.debian.org/zh_CN/DebianRepository" target="_blank" rel="external">debian仓库介绍</a></h3><p>一个Debian仓库包含多个<strong>发行版</strong>。Debian 的发行版是以 “玩具总动员 “电影中的角色命名的 (wheezy, jessie, stretch, …)。 代号有别名，叫做<strong>套件</strong>(stable, oldstable, testing, unstable)。一个发行版会被分成几个<strong>组件</strong>。在 Debian 中，这些组件被命名为 <code>main</code>, <code>contrib</code>, 和 <code>non-free</code>，并表并表示它们所包含的软件的授权条款。一个版本也有各种<strong>架构</strong>(amd64, i386, mips, powerpc, s390x, …)的软件包，以及源码和架构独立的软件包。</p>
<p>仓库的<a href="https://mirrors.aliyun.com/debian/dists/" target="_blank" rel="external">根目录下有一个<code>dists</code> 目录</a>，而这个目录又有每个发行版和套件的目录，后者通常是前者的符号链接，但浏览器不会向您显示出这个区别。每个发行版子目录都包含一个加密签名的<code>Release</code>文件和每个组件的目录，里面是不同架构的目录，名为<code>binary</code>-<em>&lt;架构&gt;</em>和<code>sources</code>。而在这些文件中，<code>Packages</code>是文本文件，包含了软件包。嗯，那么实际的软件包在哪里？</p>
<p><img src="/images/951413iMgBlog/image-20220829163817671.png" alt="image-20220829163817671"></p>
<p>软件包本身在仓库根目录下的<code>pool</code>。在<code>pool</code>下面又有所有组件的目录，其中有<code>0</code>，…，<code>9</code>，<code>a</code>，<code>b</code>，.., <code>z</code>, <code>liba</code>, … , <code>libz</code>。 而在这些目录中，是以它们所包含的软件包命名的目录，这些目录最后包含实际的软件包，即<code>.deb</code>文件。这个名字不一定是软件包本身的名字，例如，软件包bsdutils在目录<code>pool/main/u/util-linux</code> 下，它是生成软件包的源码的名称。一个上游源可能会生成多个二进制软件包，而所有这些软件包最终都会在<code>pool</code>下面的同一个子目录中。额外的单字母目录只是一个技巧，以避免在一个目录中有太多的条目，因为这是很多系统传统上存在性能问题的原因。</p>
<p>在<code>pool</code>下面的子目录中，通常会有多个版本的软件包，而每个版本的软件包属于什么发行版的信息只存在于索引中。这样一来，同一个版本的包可软件以属于多个发行版，但只使用一次磁盘空间，而且不需要求助于硬链接或符号链接，所以镜像相当简单，甚至可以在没有这些概念的系统中进行。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">apt install kubeadm=1.20.12-00 //指定版本安装</div><div class="line"></div><div class="line">#查询可用版本</div><div class="line">apt-cache policy kubeadm</div><div class="line">apt list --all-versions kubeadm</div><div class="line"></div><div class="line">#清理</div><div class="line">apt clean --dry-run </div><div class="line">apt update</div><div class="line">apt list</div><div class="line">apt show kubeadm</div><div class="line"></div><div class="line">#查询安装包的所有文件</div><div class="line">dpkg-query -L kubeadm</div><div class="line"></div><div class="line">#列出所有依赖包</div><div class="line">apt-cache depends ansible</div><div class="line"></div><div class="line">#被依赖查询</div><div class="line">apt-cache rdepends kubelet</div><div class="line"></div><div class="line">dpkg -I kubernetes/pool/kubeadm_1.21.0-00_amd64.deb</div><div class="line"></div><div class="line">#下载依赖包</div><div class="line">apt-get download $(apt-rdepends kubeadm|grep -v &quot;^ &quot;)</div><div class="line">aptitude --download-only -y install $(apt-rdepends kubeadm|grep -v &quot;^ &quot;) //不能下载已经安装了的依赖包</div><div class="line"></div><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<h3 id="简单仓库"><a href="#简单仓库" class="headerlink" title="简单仓库"></a><a href="https://zhuanlan.zhihu.com/p/482592599" target="_blank" rel="external">简单仓库</a></h3><p>下载所有 deb 包以及他们的依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<p>到deb 包所在的目录下生成 index</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dpkg-scanpackages -m . &gt; Packages</div></pre></td></tr></table></figure>
<p>apt source 指向这个目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">deb [trusted=yes] file:/polarx/test /</div></pre></td></tr></table></figure>
<h3 id="Kubernetes-仓库"><a href="#Kubernetes-仓库" class="headerlink" title="Kubernetes 仓库"></a>Kubernetes 仓库</h3><p><a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">debian 上通过kubeadm 安装 kubernetes 集群</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">//官方</div><div class="line">echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee -a /etc/apt/sources.list.d/kubernetes.list</div><div class="line"></div><div class="line">//阿里云仓库</div><div class="line">echo &apos;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&apos; &gt; /etc/apt/sources.list.d/kubernetes.list</div><div class="line">curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</div><div class="line">apt-get update</div><div class="line"></div><div class="line">export KUBE_VERSION=&quot;1.20.5&quot;</div><div class="line">apt-get install -y kubeadm=$KUBE_VERSION-00 kubelet=$KUBE_VERSION-00 kubectl=$KUBE_VERSION-00</div><div class="line">sudo apt-mark hold kubelet kubeadm kubectl</div><div class="line"></div><div class="line">[ -d /etc/bash_completion.d ] &amp;&amp; \</div><div class="line">    &#123; kubectl completion bash &gt; /etc/bash_completion.d/kubectl; \</div><div class="line">      kubeadm completion bash &gt; /etc/bash_completion.d/kubadm; &#125;</div><div class="line">      </div><div class="line">[ ! -d /usr/lib/systemd/system/kubelet.service.d ] &amp;&amp; mkdir -p /usr/lib/systemd/system/kubelet.service.d</div><div class="line">cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf</div><div class="line">[Service]</div><div class="line">CPUAccounting=true</div><div class="line">MemoryAccounting=true</div><div class="line">BlockIOAccounting=true</div><div class="line">ExecStartPre=/usr/bin/bash -c &apos;/usr/bin/mkdir -p /sys/fs/cgroup/&#123;cpuset,memory,systemd,pids,&quot;cpu,cpuacct&quot;&#125;/&#123;system,kube,kubepods&#125;.slice&apos;</div><div class="line">Slice=kube.slice</div><div class="line">EOF</div><div class="line">systemctl daemon-reload</div><div class="line"> </div><div class="line">systemctl enable kubelet.service</div></pre></td></tr></table></figure>
<h3 id="docker-仓库"><a href="#docker-仓库" class="headerlink" title="docker 仓库"></a>docker 仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apt-get install -y apt-transport-https ca-certificates curl gnupg2 lsb-release bash-completion</div><div class="line">    </div><div class="line">curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add -</div><div class="line">echo &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs)   stable&quot; &gt; /etc/apt/sources.list.d/docker-ce.list</div><div class="line">sudo apt-get update</div><div class="line">apt-get install -y docker-ce docker-ce-cli containerd.io</div><div class="line">apt-mark hold docker-ce docker-ce-cli containerd.io</div></pre></td></tr></table></figure>
<h3 id="锁定已安装版本"><a href="#锁定已安装版本" class="headerlink" title="锁定已安装版本"></a>锁定已安装版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">//锁定这三个软件的版本，避免意外升级导致版本错误：</div><div class="line">sudo apt-mark hold kubeadm kubelet kubectl</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ruanyifeng.com/blog/2019/08/xargs-tutorial.html" target="_blank" rel="external">xargs 命令教程</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/22/kubernetes service/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/22/kubernetes service/" itemprop="url">kubernetes service</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-22T17:30:03+08:00">
                2020-01-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-service-和-kube-proxy详解"><a href="#kubernetes-service-和-kube-proxy详解" class="headerlink" title="kubernetes service 和 kube-proxy详解"></a>kubernetes service 和 kube-proxy详解</h1><h2 id="service-模式"><a href="#service-模式" class="headerlink" title="service 模式"></a>service 模式</h2><p>根据创建Service的<code>type</code>类型不同，可分成4种模式：</p>
<ul>
<li>ClusterIP： <strong>默认方式</strong>。根据是否生成ClusterIP又可分为普通Service和Headless Service两类：<ul>
<li><code>普通Service</code>：通过为Kubernetes的Service分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问。为最常见的方式。</li>
<li><code>Headless Service</code>：该服务不会分配Cluster IP，也不通过kube-proxy做反向代理和负载均衡。而是通过DNS提供稳定的网络ID来访问，DNS会将headless service的后端直接解析为podIP列表。主要供StatefulSet中对应POD的序列用。</li>
</ul>
</li>
<li><code>NodePort</code>：除了使用Cluster IP之外，还通过将service的port映射到集群内每个节点的相同一个端口，实现通过nodeIP:nodePort从集群外访问服务。NodePort会RR转发给后端的任意一个POD，跟ClusterIP类似</li>
<li><code>LoadBalancer</code>：和nodePort类似，不过除了使用一个Cluster IP和nodePort之外，还会向所使用的公有云申请一个负载均衡器，实现从集群外通过LB访问服务。在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。</li>
<li><code>ExternalName</code>：是 Service 的特例。此模式主要面向运行在集群外部的服务，通过它可以将外部服务映射进k8s集群，且具备k8s内服务的一些特征（如具备namespace等属性），来为集群内部提供服务。此模式要求kube-dns的版本为1.7或以上。这种模式和前三种模式（除headless service）最大的不同是重定向依赖的是dns层次，而不是通过kube-proxy。</li>
</ul>
<p>service yaml案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: nginx-ren</div><div class="line">  labels:</div><div class="line">    app: web</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line"># clusterIP: None  </div><div class="line">  ports:</div><div class="line">  - port: 8080</div><div class="line">    targetPort: 80</div><div class="line">    nodePort: 30080</div><div class="line">  selector:</div><div class="line">    app: ren</div></pre></td></tr></table></figure>
<p><code>ports</code> 字段指定服务的端口信息：</p>
<ul>
<li><code>port</code>：虚拟 ip 要绑定的 port，每个 service 会创建出来一个虚拟 ip，通过访问 <code>vip:port</code> 就能获取服务的内容。这个 port 可以用户随机选取，因为每个服务都有自己的 vip，也不用担心冲突的情况</li>
<li><code>targetPort</code>：pod 中暴露出来的 port，这是运行的容器中具体暴露出来的端口，一定不能写错–一般用name来代替具体的port</li>
<li><code>protocol</code>：提供服务的协议类型，可以是 <code>TCP</code> 或者 <code>UDP</code></li>
<li><code>nodePort</code>： 仅在type为nodePort模式下有用，宿主机暴露端口</li>
</ul>
<p>但是nodePort和loadbalancer可以被外部访问，loadbalancer需要一个外部ip，流量走外部ip进出</p>
<p>NodePort向外部暴露了多个宿主机的端口，外部可以部署负载均衡将这些地址配置进去。</p>
<p>默认情况下，服务会rr转发到可用的后端。如果希望保持会话（同一个 client 永远都转发到相同的 pod），可以把 <code>service.spec.sessionAffinity</code> 设置为 <code>ClientIP</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">iptables-save | grep 3306</div><div class="line"></div><div class="line">iptables-save | grep KUBE-SERVICES</div><div class="line"></div><div class="line">#iptables-save |grep KUBE-SVC-RVEVH2XMONK6VC5O</div><div class="line">:KUBE-SVC-RVEVH2XMONK6VC5O - [0:0]</div><div class="line">-A KUBE-SERVICES -d 10.10.70.95/32 -p tcp -m comment --comment &quot;drds/mysql-read:mysql cluster IP&quot; -m tcp --dport 3306 -j KUBE-SVC-RVEVH2XMONK6VC5O</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-XC4TZYIZFYB653VI</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-MK4XPBZUIJGFXKED</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -j KUBE-SEP-AAYXWGQJBDHUJUQ3</div></pre></td></tr></table></figure>
<p>看起来 service 是个完美的方案，可以解决服务访问的所有问题，但是 service 这个方案（iptables 模式）也有自己的缺点。</p>
<p>首先，<strong>如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod</strong>，当然这个可以通过 <code>readiness probes</code> 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。</p>
<p>另外，<code>nodePort</code> 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。因为只做了NAT</p>
<h2 id="NodePort-的一些问题"><a href="#NodePort-的一些问题" class="headerlink" title="NodePort 的一些问题"></a>NodePort 的一些问题</h2><ul>
<li>首先endpoint回复不能走node 1给client，因为会被client reset（如果在node1上将src ip替换成node2的ip可能会路由不通）。回复包在 node1上要snat给node2</li>
<li>经过snat后endpoint没法拿到client ip（slb之类是通过option带过来）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">          client</div><div class="line">            \ ^</div><div class="line">             \ \</div><div class="line">              v \</div><div class="line">  node 1 &lt;--- node 2</div><div class="line">   | ^   SNAT</div><div class="line">   | |   ---&gt;</div><div class="line">   v |</div><div class="line">endpoint</div></pre></td></tr></table></figure>
<p>可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。</p>
<p>而这个机制的实现原理也非常简单：这时候，<strong>一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod</strong>。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">      client</div><div class="line">      ^ /   \</div><div class="line">     / /     \</div><div class="line">    / v       X</div><div class="line">  node 1     node 2</div><div class="line">   ^ |</div><div class="line">   | |</div><div class="line">   | v</div><div class="line">endpoint</div></pre></td></tr></table></figure>
<p>当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。</p>
<h2 id="Service和kube-proxy的工作原理"><a href="#Service和kube-proxy的工作原理" class="headerlink" title="Service和kube-proxy的工作原理"></a>Service和kube-proxy的工作原理</h2><p>kube-proxy有两种主要的实现（userspace基本没有使用了）：</p>
<ul>
<li>[[iptables使用]]来做NAT以及负载均衡</li>
<li>ipvs来做NAT以及负载均衡</li>
</ul>
<p>Service 是由 kube-proxy 组件通过监听 Pod 的变化事件，在宿主机上维护iptables规则或者ipvs规则。</p>
<p>Kube-proxy 主要监听两个对象，一个是 Service，一个是 Endpoint，监听他们启停。以及通过selector将他们绑定。</p>
<p>IPVS 是专门为LB设计的。它用hash table管理service，对service的增删查找都是<em>O(1)</em>的时间复杂度。不过IPVS内核模块没有SNAT功能，因此借用了iptables的SNAT功能。IPVS 针对报文做DNAT后，将连接信息保存在nf_conntrack中，iptables据此接力做SNAT。该模式是目前Kubernetes网络性能最好的选择。但是由于nf_conntrack的复杂性，带来了很大的性能损耗。</p>
<h3 id="iptables-实现负载均衡的工作流程"><a href="#iptables-实现负载均衡的工作流程" class="headerlink" title="iptables 实现负载均衡的工作流程"></a>iptables 实现负载均衡的工作流程</h3><p>如果kube-proxy不是用的ipvs模式，那么主要靠iptables来做DNAT和SNAT以及负载均衡</p>
<p>iptables+clusterIP工作流程：</p>
<ol>
<li>集群内访问svc 10.10.35.224:3306 命中 kube-services iptables</li>
<li>iptables 规则：KUBE-SEP-F4QDAAVSZYZMFXZQ 对应到  KUBE-SEP-F4QDAAVSZYZMFXZQ</li>
<li>KUBE-SEP-F4QDAAVSZYZMFXZQ 指示 DNAT到 宿主机：192.168.0.83:10379（在内核中将包改写了ip port）</li>
<li>从svc description中可以看到这个endpoint的地址 192.168.0.83:10379（pod使用Host network）</li>
</ol>
<p><img src="/images/oss/52e050ebb7841d70b7e3ea62e18d5b30.png" alt="image.png"></p>
<p>在对应的宿主机上可以清楚地看到容器中的mysqld进程正好监听着 10379端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@az1-drds-83 ~]# ss -lntp |grep 10379</div><div class="line">LISTEN     0      128         :::10379                   :::*                   users:((&quot;mysqld&quot;,pid=17707,fd=18))</div><div class="line">[root@az1-drds-83 ~]# ps auxff | grep 17707 -B2</div><div class="line">root     13606  0.0  0.0  10720  3764 ?        Sl   17:09   0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc</div><div class="line"></div><div class="line">root     13624  0.0  0.0 103044 10424 ?        Ss   17:09   0:00  |   \_ python /entrypoint.py</div><div class="line">root     14835  0.0  0.0  11768  1636 ?        S    17:10   0:00  |   \_ /bin/sh /u01/xcluster/bin/mysqld_safe --defaults-file=/home/mysql/my10379.cnf</div><div class="line">alidb    17707  0.6  0.0 1269128 67452 ?       Sl   17:10   0:25  |       \_ /u01/xcluster_20200303/bin/mysqld --defaults-file=/home/mysql/my10379.cnf --basedir=/u01/xcluster_20200303 --datadir=/home/mysql/data10379/dbs10379 --plugin-dir=/u01/xcluster_20200303/lib/plugin --user=mysql --log-error=/home/mysql/data10379/mysql/master-error.log --open-files-limit=8192 --pid-file=/home/mysql/data10379/dbs10379/az1-drds-83.pid --socket=/home/mysql/data10379/tmp/mysql.sock --port=10379</div></pre></td></tr></table></figure>
<p>对应的这个pod的description：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">#kubectl describe pod apsaradbcluster010-cv6w</div><div class="line">Name:         apsaradbcluster010-cv6w</div><div class="line">Namespace:    default</div><div class="line">Priority:     0</div><div class="line">Node:         az1-drds-83/192.168.0.83</div><div class="line">Start Time:   Thu, 10 Sep 2020 17:09:33 +0800</div><div class="line">Labels:       alisql.clusterName=apsaradbcluster010</div><div class="line">              alisql.pod_name=apsaradbcluster010-cv6w</div><div class="line">              alisql.pod_role=leader</div><div class="line">Annotations:  apsara.metric.pod_name: apsaradbcluster010-cv6w</div><div class="line">Status:       Running</div><div class="line">IP:           192.168.0.83</div><div class="line">IPs:</div><div class="line">  IP:           192.168.0.83</div><div class="line">Controlled By:  ApsaradbCluster/apsaradbcluster010</div><div class="line">Containers:</div><div class="line">  engine:</div><div class="line">    Container ID:   docker://ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97</div><div class="line">    Image:          reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-engine:develop-20200910140415</div><div class="line">    Image ID:       docker://sha256:7ad5cc53c87b34806eefec829d70f5f0192f4127c7ee4e867cb3da3bb6c2d709</div><div class="line">    Ports:          10379/TCP, 20383/TCP, 46846/TCP</div><div class="line">    Host Ports:     10379/TCP, 20383/TCP, 46846/TCP</div><div class="line">    State:          Running</div><div class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</div><div class="line">    Ready:          True</div><div class="line">    Restart Count:  0</div><div class="line">    Environment:</div><div class="line">      ALISQL_POD_NAME:  apsaradbcluster010-cv6w (v1:metadata.name)</div><div class="line">      ALISQL_POD_PORT:  10379</div><div class="line">    Mounts:</div><div class="line">      /dev/shm from devshm (rw)</div><div class="line">      /etc/localtime from etclocaltime (rw)</div><div class="line">      /home/mysql/data from data-dir (rw)</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</div><div class="line">  exporter:</div><div class="line">    Container ID:  docker://b49865b7798f9036b431203d54994ac8fdfcadacb01a2ab4494b13b2681c482d</div><div class="line">    Image:         reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-exporter:latest</div><div class="line">    Image ID:      docker://sha256:432cdd0a0e7c74c6eb66551b6f6af9e4013f60fb07a871445755f6577b44da19</div><div class="line">    Port:          47272/TCP</div><div class="line">    Host Port:     47272/TCP</div><div class="line">    Args:</div><div class="line">      --web.listen-address=:47272</div><div class="line">      --collect.binlog_size</div><div class="line">      --collect.engine_innodb_status</div><div class="line">      --collect.info_schema.innodb_metrics</div><div class="line">      --collect.info_schema.processlist</div><div class="line">      --collect.info_schema.tables</div><div class="line">      --collect.info_schema.tablestats</div><div class="line">      --collect.slave_hosts</div><div class="line">    State:          Running</div><div class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</div><div class="line">    Ready:          True</div><div class="line">    Restart Count:  0</div><div class="line">    Environment:</div><div class="line">      ALISQL_POD_NAME:   apsaradbcluster010-cv6w (v1:metadata.name)</div><div class="line">      DATA_SOURCE_NAME:  root:@(127.0.0.1:10379)/</div><div class="line">    Mounts:</div><div class="line">      /dev/shm from devshm (rw)</div><div class="line">      /etc/localtime from etclocaltime (rw)</div><div class="line">      /home/mysql/data from data-dir (rw)</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</div></pre></td></tr></table></figure>
<p>DNAT 规则的作用，就是<strong>在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口</strong>。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。</p>
<h4 id="哪些组件会修改iptables"><a href="#哪些组件会修改iptables" class="headerlink" title="哪些组件会修改iptables"></a>哪些组件会修改iptables</h4><p><img src="/images/oss/776d057b133692312578f01e74caca5b.png" alt="image.png"></p>
<h3 id="ipvs-实现负载均衡的原理"><a href="#ipvs-实现负载均衡的原理" class="headerlink" title="ipvs 实现负载均衡的原理"></a>ipvs 实现负载均衡的原理</h3><p>ipvs模式下，kube-proxy会先创建虚拟网卡，kube-ipvs0下面的每个ip都对应着svc的一个clusterIP：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># ip addr</div><div class="line">  ...</div><div class="line">5: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default </div><div class="line">    link/ether de:29:17:2a:8d:79 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 10.68.70.130/32 scope global kube-ipvs0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure>
<p>kube-ipvs0下面绑的这些ip就是在发包的时候让内核知道如果目标ip是这些地址的话，这些地址是自身的所以包不会发出去，而是给INPUT链，这样ipvs内核模块有机会改写包做完NAT后再发走。</p>
<p>ipvs会放置DNAT钩子在INPUT链上，因此必须要让内核识别 VIP 是本机的 IP。这样才会过INPUT 链，要不然就通过OUTPUT链出去了。k8s 通过kube-proxy将service cluster ip 绑定到虚拟网卡kube-ipvs0。</p>
<p>同时在路由表中增加一些ipvs 的路由条目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># ip route show table local</div><div class="line">local 10.68.0.1 dev kube-ipvs0 proto kernel scope host src 10.68.0.1 </div><div class="line">local 10.68.0.2 dev kube-ipvs0 proto kernel scope host src 10.68.0.2 </div><div class="line">local 10.68.70.130 dev kube-ipvs0 proto kernel scope host src 10.68.70.130 -- ipvs</div><div class="line">broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1 </div><div class="line">local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 </div><div class="line">local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 </div><div class="line">broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 </div><div class="line">broadcast 172.17.0.0 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">local 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 </div><div class="line">broadcast 172.17.255.255 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">local 172.20.185.192 dev tunl0 proto kernel scope host src 172.20.185.192 </div><div class="line">broadcast 172.20.185.192 dev tunl0 proto kernel scope link src 172.20.185.192 </div><div class="line">broadcast 172.26.128.0 dev eth0 proto kernel scope link src 172.26.137.117 </div><div class="line">local 172.26.137.117 dev eth0 proto kernel scope host src 172.26.137.117 </div><div class="line">broadcast 172.26.143.255 dev eth0 proto kernel scope link src 172.26.137.117</div></pre></td></tr></table></figure>
<p>而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ipvsadm -ln |grep 10.68.114.131 -A5</div><div class="line">TCP  10.68.114.131:3306 rr</div><div class="line">  -&gt; 172.20.120.143:3306          Masq    1      0          0         </div><div class="line">  -&gt; 172.20.185.209:3306          Masq    1      0          0         </div><div class="line">  -&gt; 172.20.248.143:3306          Masq    1      0          0</div></pre></td></tr></table></figure>
<p>172.20.<em>.</em> 是后端真正pod的ip， 10.68.114.131 是cluster-ip.</p>
<p>完整的工作流程如下：</p>
<ol>
<li>因为service cluster ip 绑定到虚拟网卡kube-ipvs0上，内核可以识别访问的 VIP 是本机的 IP.</li>
<li>数据包到达INPUT链.</li>
<li>ipvs监听到达input链的数据包，比对数据包请求的服务是为集群服务，修改数据包的目标IP地址为对应pod的IP，然后将数据包发至POSTROUTING链.</li>
<li>数据包经过POSTROUTING链选路由后，将数据包通过tunl0网卡(calico网络模型)发送出去。从tunl0虚拟网卡获得源IP.</li>
<li>经过tunl0后进行ipip封包，丢到物理网络，路由到目标node（目标pod所在的node）</li>
<li>目标node进行ipip解包后给pod对应的网卡</li>
<li>pod接收到请求之后，构建响应报文，改变源地址和目的地址，返回给客户端。</li>
</ol>
<p><img src="/images/oss/51695ebb1c6b30d95f8ac8d5dcb8dd7f.png" alt="image.png"></p>
<h4 id="ipvs实际案例"><a href="#ipvs实际案例" class="headerlink" title="ipvs实际案例"></a>ipvs实际案例</h4><p>ipvs负载均衡下一次完整的syn握手抓包。</p>
<p>宿主机上访问 curl clusterip+port 后因为这个ip绑定在kube-ipvs0上，本来是应该发出去的包（prerouting）但是内核认为这个包是访问自己，于是给INPUT链，接着被ipvs放置在INPUT中的DNAT钩子勾住，将dest ip根据负载均衡逻辑改成pod-ip，然后将数据包再发至POSTROUTING链。这时因为目标ip是POD-IP了，根据ip route 选择到出口网卡是tunl0。</p>
<p>可以看下内核中的路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># ip route get 10.68.70.130</div><div class="line">local 10.68.70.130 dev lo src 10.68.70.130  //这条规则指示了clusterIP是发给自己的</div><div class="line">    cache &lt;local&gt; </div><div class="line"># ip route get 172.20.185.217</div><div class="line">172.20.185.217 via 172.26.137.117 dev tunl0 src 172.20.22.192  //这条规则指示clusterIP替换成POD IP后发给本地tunl0做ipip封包</div></pre></td></tr></table></figure>
<p>于是cip变成了tunl0的IP，这个tunl0是ipip模式，于是将这个包打包成ipip，也就是外层sip、dip都是宿主机ip，再将这个包丢入到物理网络</p>
<p><img src="/images/oss/84bbd3f10de9e7ec2266a82520876c8c.png" alt=""></p>
<p>网络收包到达内核后的处理流程如下，核心都是查路由表，出包也会查路由表（判断是否本机内部通信，或者外部通信的话需要选用哪个网卡）</p>
<p>补两张内核netfilter框架的图：</p>
<p><strong>packet filtering in IPTables</strong></p>
<p><img src="/images/oss/a10e26828904310633f7bc20d587e547.png" alt="image.png"></p>
<p><a href="https://en.wikipedia.org/wiki/Iptables#/media/File:Netfilter-packet-flow.svg" target="_blank" rel="external">完整版</a>：</p>
<p><img src="/images/oss/02e4e71ea0fae4f087a233faa190d7c7.png" alt="image.png" style="zoom:150%;"></p>
<h3 id="ipvs的一些分析"><a href="#ipvs的一些分析" class="headerlink" title="ipvs的一些分析"></a>ipvs的一些分析</h3><p>ipvs是一个内核态的四层负载均衡，支持NAT以及IPIP隧道模式，但LB和RS不能跨子网，IPIP性能次之，通过ipip隧道解决跨网段传输问题，因此能够支持跨子网。而NAT模式没有限制，这也是唯一一种支持端口映射的模式。</p>
<p>但是ipvs只有NAT（也就是DNAT），NAT也俗称三角模式，要求RS和LVS 在一个二层网络，并且LVS是RS的网关，这样回包一定会到网关，网关再次做SNAT，这样client看到SNAT后的src ip是LVS ip而不是RS-ip。默认实现不支持ful-NAT，所以像公有云厂商为了适应公有云场景基本都会定制实现ful-NAT模式的lvs。</p>
<p>我们不难猜想，由于Kubernetes Service需要使用端口映射功能，因此kube-proxy必然只能使用ipvs的NAT模式。</p>
<p>如下Masq表示MASQUERADE（也就是SNAT），跟iptables里面的 MASQUERADE 是一个意思</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># ipvsadm -L -n  |grep 70.130 -A12</div><div class="line">TCP  10.68.70.130:12380 rr</div><div class="line">  -&gt; 172.20.185.217:9376          Masq    1      0          0</div></pre></td></tr></table></figure>
<h3 id="kuberletes对iptables的修改-图中黄色部分-："><a href="#kuberletes对iptables的修改-图中黄色部分-：" class="headerlink" title="kuberletes对iptables的修改(图中黄色部分)："></a>kuberletes对iptables的修改(图中黄色部分)：</h3><p><img src="/images/oss/b64e5edf67ec76613616efbd7eba20a3.png" alt="image.png"></p>
<h2 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h2><p>在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#iptables-代理模式" target="_blank" rel="external">iptables 代理模式</a>，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#ipvs-代理模式" target="_blank" rel="external">ipvs 代理模式</a></p>
<p>kube-proxy相当于service的管控方，业务流量不会走到kube-proxy，业务流量的负载均衡都是由内核层面的iptables或者ipvs来分发。</p>
<p>kube-proxy的三种模式：</p>
<p><img src="/images/oss/075e2955c5fbd08986bd34afaa5034ba.png" alt="image.png"></p>
<p><strong>一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。</strong></p>
<p>ipvs 就是用于解决在大量 Service 时，iptables 规则同步变得不可用的性能问题。与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，这也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能。</p>
<p>除了能够提升性能之外，ipvs 也提供了多种类型的负载均衡算法，除了最常见的 Round-Robin 之外，还支持最小连接、目标哈希、最小延迟等算法，能够很好地提升负载均衡的效率。</p>
<p>而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价，“将重要操作放入内核态”是提高性能的重要手段。</p>
<p><strong>IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。</strong></p>
<p>ipvs 和 iptables 都是基于 Netfilter 实现的。</p>
<p>Kubernetes 中已经使用 ipvs 作为 kube-proxy 的默认代理模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/opt/kube/bin/kube-proxy --bind-address=172.26.137.117 --cluster-cidr=172.20.0.0/16 --hostname-override=172.26.137.117 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --logtostderr=true --proxy-mode=ipvs</div></pre></td></tr></table></figure>
<p><img src="/images/oss/c44c8b3fbb1b2e0910872a6aecef790c.png" alt="image.png"></p>
<h2 id="为什么clusterIP不能ping通"><a href="#为什么clusterIP不能ping通" class="headerlink" title="为什么clusterIP不能ping通"></a>为什么clusterIP不能ping通</h2><p><a href="https://cizixs.com/2017/03/30/kubernetes-introduction-service-and-kube-proxy/" target="_blank" rel="external">集群内访问cluster ip（不能ping，只能cluster ip+port）就是在到达网卡之前被内核iptalbes做了dnat/snat</a>, cluster IP是一个虚拟ip，可以针对具体的服务固定下来，这样服务后面的pod可以随便变化。</p>
<p>iptables模式的svc会ping不通clusterIP，可以看如下iptables和route（留意：–reject-with icmp-port-unreachable）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#ping 10.96.229.40</div><div class="line">PING 10.96.229.40 (10.96.229.40) 56(84) bytes of data.</div><div class="line">^C</div><div class="line">--- 10.96.229.40 ping statistics ---</div><div class="line">2 packets transmitted, 0 received, 100% packet loss, time 999ms</div><div class="line"></div><div class="line"></div><div class="line">#iptables-save |grep 10.96.229.40</div><div class="line">-A KUBE-SERVICES -d 10.96.229.40/32 -p tcp -m comment --comment &quot;***-service:https has no endpoints&quot; -m tcp --dport 8443 -j REJECT --reject-with icmp-port-unreachable</div><div class="line"></div><div class="line">#ip route get 10.96.229.40</div><div class="line">10.96.229.40 via 11.164.219.253 dev eth0  src 11.164.219.119 </div><div class="line">    cache</div></pre></td></tr></table></figure>
<p>准确来说如果用ipvs实现的clusterIP是可以ping通的：</p>
<ul>
<li>如果用iptables 来做转发是ping不通的，因为iptables里面这条规则只处理tcp包，reject了icmp</li>
<li>ipvs实现的clusterIP都能ping通</li>
<li>ipvs下的clusterIP ping通了也不是转发到pod，ipvs负载均衡只转发tcp协议的包</li>
<li>ipvs 的clusterIP在本地配置了route路由到回环网卡，这个包是lo网卡回复的</li>
</ul>
<p>ipvs实现的clusterIP，在本地有添加路由到lo网卡</p>
<p><img src="/images/oss/1f5539eb4c5fa16b2f66f44056d80d7a.png" alt="image.png"></p>
<p>然后在本机抓包（到ipvs后端的pod上抓不到icmp包）：</p>
<p><img src="/images/oss/1caea5b0eb23a47241191d1b5d8c5001.png" alt="image.png"></p>
<p>从上面可以看出显然ipvs只会转发tcp包到后端pod，所以icmp包不会通过ipvs转发到pod上，同时在本地回环网卡lo上抓到了进去的icmp包。因为本地添加了一条路由规则，目标clusterIP被指示发到lo网卡上，lo网卡回复了这个ping包，所以通了。</p>
<h2 id="port-forward"><a href="#port-forward" class="headerlink" title="port-forward"></a>port-forward</h2><p>port-forward后外部也能够像nodePort一样访问到，但是port-forward不适合大流量，一般用于管理端口，启动的时候port-forward会固定转发到一个具体的Pod上，也没有负载均衡的能力。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#在本机监听1080端口，并转发给后端的svc/nginx-ren(总是给发给svc中的一个pod)</div><div class="line">kubectl port-forward --address 0.0.0.0 svc/nginx-ren 1080:80</div></pre></td></tr></table></figure>
<p><code>kubectl</code> looks up a Pod from the service information provided on the command line and forwards directly to a Pod rather than forwarding to the ClusterIP/Service port and allowing the cluster to load balance the service like regular service traffic.</p>
<p>The <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L225" target="_blank" rel="external">portforward.go <code>Complete</code> function</a> is where <code>kubectl portforward</code> does the first look up for a pod from options via <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L254" target="_blank" rel="external"><code>AttachablePodForObjectFn</code></a>:</p>
<p>The <code>AttachablePodForObjectFn</code> is defined as <code>attachablePodForObject</code> in <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/interface.go#L39-L40" target="_blank" rel="external">this interface</a>, then here is the <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go" target="_blank" rel="external"><code>attachablePodForObject</code> function</a>.</p>
<p>To my (inexperienced) Go eyes, it appears the <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go" target="_blank" rel="external"><code>attachablePodForObject</code></a> is the thing <code>kubectl</code> uses to look up a Pod to from a Service defined on the command line.</p>
<p>Then from there on everything deals with filling in the Pod specific <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L46-L58" target="_blank" rel="external"><code>PortForwardOptions</code></a> (which doesn’t include a service) and is passed to the kubernetes API.</p>
<h2 id="Service-和-DNS-的关系"><a href="#Service-和-DNS-的关系" class="headerlink" title="Service 和 DNS 的关系"></a>Service 和 DNS 的关系</h2><p>Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。</p>
<p>对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。</p>
<p>而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">#kubectl get pod -l app=mysql-r -o wide</div><div class="line">NAME        READY   STATUS    RESTARTS IP               NODE          </div><div class="line">mysql-r-0   2/2     Running   0        172.20.120.143   172.26.137.118</div><div class="line">mysql-r-1   2/2     Running   4        172.20.248.143   172.26.137.116</div><div class="line">mysql-r-2   2/2     Running   0        172.20.185.209   172.26.137.117</div><div class="line"></div><div class="line">/ # nslookup mysql-r-1.mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r-1.mysql-r</div><div class="line">Address 1: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</div><div class="line">/ # </div><div class="line">/ # nslookup mysql-r-2.mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r-2.mysql-r</div><div class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</div><div class="line"></div><div class="line">#如果service是headless(也就是明确指定了 clusterIP: None)</div><div class="line">/ # nslookup mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r</div><div class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</div><div class="line">Address 2: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</div><div class="line">Address 3: 172.20.120.143 mysql-r-0.mysql-r.default.svc.cluster.local</div><div class="line"></div><div class="line">#如果service 没有指定 clusterIP: None，也就是会分配一个clusterIP给集群</div><div class="line">/ # nslookup mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r</div><div class="line">Address 1: 10.68.90.172 mysql-r.default.svc.cluster.local</div></pre></td></tr></table></figure>
<p>不是每个pod都会向DNS注册，只有：</p>
<ul>
<li>StatefulSet中的POD会向dns注册，因为他们要保证顺序行</li>
<li>POD显式指定了hostname和subdomain，说明要靠hostname/subdomain来解析</li>
<li>Headless Service代理的POD也会注册</li>
</ul>
<h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p> <code>kube-proxy</code> 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 <a href="https://jimmysong.io/kubernetes-handbook/concepts/cni.html" target="_blank" rel="external">CNI</a> 创建的外网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/ingress.html" target="_blank" rel="external">ingress</a> 这个资源对象，它由位于 Kubernetes <a href="https://jimmysong.io/kubernetes-handbook/practice/edge-node-configuration.html" target="_blank" rel="external">边缘节点</a>（这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理<strong>南北向流量</strong>，Ingress 必须对接各种 Ingress Controller 才能使用，比如 <a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="external">nginx ingress controller</a>、<a href="https://traefik.io/" target="_blank" rel="external">traefik</a>。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 <a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/" target="_blank" rel="external">nginx ingress controller</a>，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。</p>
<p>Ingress是授权入站连接到达集群服务的规则集合。 你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。 用户通过POST Ingress资源到API server的方式来请求ingress。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> internet</div><div class="line">     |</div><div class="line">[ Ingress ]</div><div class="line">--|-----|--</div><div class="line">[ Services ]</div></pre></td></tr></table></figure>
<p>可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers" target="_blank" rel="external">Ingress 控制器</a> 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。</p>
<p>Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#nodeport" target="_blank" rel="external">Service.Type=NodePort</a> 或 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#loadbalancer" target="_blank" rel="external">Service.Type=LoadBalancer</a> 类型的服务。</p>
<p>Ingress 其实不是Service的一个类型，但是它可以作用于多个Service，作为集群内部服务的入口。Ingress 能做许多不同的事，比如根据不同的路由，将请求转发到不同的Service上等等。</p>
<p><img src="/images/oss/0e100056910df8cfc45403a05838dd34.png" alt="image.png"></p>
<p> Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Ingress</div><div class="line">metadata:</div><div class="line">  name: cafe-ingress</div><div class="line">spec:</div><div class="line">  tls:</div><div class="line">  - hosts:</div><div class="line">    - cafe.example.com</div><div class="line">    secretName: cafe-secret</div><div class="line">  rules:</div><div class="line">  - host: cafe.example.com</div><div class="line">    http:</div><div class="line">      paths:</div><div class="line">      - path: /tea              --入口url路径</div><div class="line">        backend:</div><div class="line">          serviceName: tea-svc  --对应的service</div><div class="line">          servicePort: 80</div><div class="line">      - path: /coffee</div><div class="line">        backend:</div><div class="line">          serviceName: coffee-svc</div><div class="line">          servicePort: 80</div></pre></td></tr></table></figure>
<p>在实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可。然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。</p>
<p>目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。</p>
<p>一个 Ingress Controller 可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。</p>
<p>对service未来的一些探索</p>
<h2 id="eBPF（extended-Berkeley-Packet-Filter）和网络"><a href="#eBPF（extended-Berkeley-Packet-Filter）和网络" class="headerlink" title="eBPF（extended Berkeley Packet Filter）和网络"></a>eBPF（extended Berkeley Packet Filter）和网络</h2><p>eBPF 最早出现在 3.18 内核中，此后原来的 BPF 就被称为 <strong>“经典” BPF</strong>（classic BPF, cBPF），cBPF 现在基本已经废弃了。很多人知道 cBPF 是因为它是 <code>tcpdump</code> 的包过滤语言。<strong>现在，Linux 内核只运行 eBPF，内核会将加载的 cBPF 字节码 透明地转换成 eBPF 再执行</strong>。如无特殊说明，本文中所说的 BPF 都是泛指 BPF 技术。</p>
<p>2015年<strong>eBPF 添加了一个新 fast path：XDP</strong>，XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码，而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。</p>
<p>BPF 当时用于 tcpdump，在内核中尽量前面的位置抓包，它不会 crash 内核；</p>
<p>bcc 是 tracing frontend for eBPF。</p>
<p>内核添加了一个新 socket 类型 AF_XDP。它提供的能力是：在零拷贝（ zero-copy）的前提下将包从网卡驱动送到用户空间。</p>
<p>AF_XDP 提供的能力与 DPDK 有点类似，不过：</p>
<ul>
<li>DPDK 需要重写网卡驱动，需要额外维护用户空间的驱动代码。</li>
<li>AF_XDP 在复用内核网卡驱动的情况下，能达到与 DPDK 一样的性能。</li>
</ul>
<p>而且由于复用了内核基础设施，所有的网络管理工具还都是可以用的，因此非常方便， 而 DPDK 这种 bypass 内核的方案导致绝大大部分现有工具都用不了了。</p>
<p>由于所有这些操作都是发生在 XDP 层的，因此它称为 AF_XDP。插入到这里的 BPF 代码 能直接将包送到 socket。</p>
<p>Facebook 公布了生产环境 XDP+eBPF 使用案例（DDoS &amp; LB）</p>
<ul>
<li>用 XDP/eBPF 重写了原来基于 IPVS 的 L4LB，性能 10x。</li>
<li>eBPF 经受住了严苛的考验：从 2017 开始，每个进入 facebook.com 的包，都是经过了 XDP &amp; eBPF 处理的。</li>
</ul>
<p><strong>Cilium 1.6 发布</strong> 第一次支持完全干掉基于 iptables 的 kube-proxy，全部功能基于 eBPF。Cilium 1.8 支持基于 XDP 的 Service 负载均衡和 host network policies。</p>
<p>传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：</p>
<p><img src="/images/oss/67851ecb88fca18b9745dae4948947a5.png" alt="image.png"></p>
<p>步骤：</p>
<ol>
<li>网卡收到一个包（通过 DMA 放到 ring-buffer）。</li>
<li>包经过 XDP hook 点。</li>
<li>内核给包分配内存，此时才有了大家熟悉的 skb（包的内核结构体表示），然后 送到内核协议栈。</li>
<li>包经过 GRO 处理，对分片包进行重组。</li>
<li>包进入 tc（traffic control）的 ingress hook。接下来，所有橙色的框都是 Netfilter 处理点。</li>
<li>Netfilter：在 PREROUTING hook 点处理 raw table 里的 iptables 规则。</li>
<li>包经过内核的连接跟踪（conntrack）模块。</li>
<li>Netfilter：在 PREROUTING hook 点处理 mangle table 的 iptables 规则。</li>
<li>Netfilter：在 PREROUTING hook 点处理 nat table 的 iptables 规则。</li>
<li>进行路由判断（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。</li>
<li>Netfilter：在 FORWARD hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 FORWARD hook 点处理 filter table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 nat table 里的iptables 规则。</li>
<li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。</li>
<li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：发送到一个本机 veth 设备，或者一个本机 service endpoint， 或者，如果目的 IP 是主机外，就通过网卡发出去。</li>
</ol>
<h3 id="Cilium-如何处理POD之间的流量（东西向流量）"><a href="#Cilium-如何处理POD之间的流量（东西向流量）" class="headerlink" title="Cilium 如何处理POD之间的流量（东西向流量）"></a>Cilium 如何处理POD之间的流量（东西向流量）</h3><p><img src="/images/oss/f6efb2e51abbd2c88a099ee9dc942d37.png" alt="image.png"></p>
<p>如上图所示，Socket 层的 BPF 程序主要处理 Cilium 节点的东西向流量（E-W）。</p>
<ul>
<li>将 Service 的 IP:Port 映射到具体的 backend pods，并做负载均衡。</li>
<li>当应用发起 connect、sendmsg、recvmsg 等请求（系统调用）时，拦截这些请求， 并根据请求的IP:Port 映射到后端 pod，直接发送过去。反向进行相反的变换。</li>
</ul>
<p>这里实现的好处：性能更高。</p>
<ul>
<li>不需要包级别（packet leve）的地址转换（NAT）。在系统调用时，还没有创建包，因此性能更高。</li>
<li>省去了 kube-proxy 路径中的很多中间节点（intermediate node hops） 可以看出，应用对这种拦截和重定向是无感知的（符合 Kubernetes Service 的设计）。</li>
</ul>
<h3 id="Cilium处理外部流量（南北向流量）"><a href="#Cilium处理外部流量（南北向流量）" class="headerlink" title="Cilium处理外部流量（南北向流量）"></a>Cilium处理外部流量（南北向流量）</h3><p><img src="/images/oss/e013d356145d1be6d6a69e2f1b32bdc8.png" alt="image.png"></p>
<p>集群外来的流量到达 node 时，由 XDP 和 tc 层的 BPF 程序进行处理， 它们做的事情与 socket 层的差不多，将 Service 的 IP:Port 映射到后端的 PodIP:Port，如果 backend pod 不在本 node，就通过网络再发出去。发出去的流程我们 在前面 Cilium eBPF 包转发路径 讲过了。</p>
<p>这里 BPF 做的事情：执行 DNAT。这个功能可以在 XDP 层做，也可以在 TC 层做，但 在XDP 层代价更小，性能也更高。</p>
<p>总结起来，Cilium的核心理念就是：</p>
<ul>
<li>将东西向流量放在离 socket 层尽量近的地方做。</li>
<li>将南北向流量放在离驱动（XDP 和 tc）层尽量近的地方做。</li>
</ul>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p>测试环境：两台物理节点，一个发包，一个收包，收到的包做 Service loadbalancing 转发给后端 Pods。</p>
<p><img src="/images/oss/1b69dfd206a91dc4007781163fd55f41.png" alt="image.png"></p>
<p>可以看出：</p>
<ul>
<li>Cilium XDP eBPF 模式能处理接收到的全部 10Mpps（packets per second）。</li>
<li>Cilium tc eBPF 模式能处理 3.5Mpps。</li>
<li>kube-proxy iptables 只能处理 2.3Mpps，因为它的 hook 点在收发包路径上更后面的位置。</li>
<li>kube-proxy ipvs 模式这里表现更差，它相比 iptables 的优势要在 backend 数量很多的时候才能体现出来。</li>
</ul>
<p>cpu：</p>
<ul>
<li>XDP 性能最好，是因为 XDP BPF 在驱动层执行，不需要将包 push 到内核协议栈。</li>
<li>kube-proxy 不管是 iptables 还是 ipvs 模式，都在处理软中断（softirq）上消耗了大量 CPU。</li>
</ul>
<h2 id="标签和选择算符"><a href="#标签和选择算符" class="headerlink" title="标签和选择算符"></a>标签和选择算符</h2><p><em>标签（Labels）</em> 是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。</p>
<h3 id="标签选择符"><a href="#标签选择符" class="headerlink" title="标签选择符"></a>标签选择符</h3><p>selector要和template中的labels一致</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">spec:</div><div class="line">  serviceName: &quot;nginx-test&quot;</div><div class="line">  replicas: 2</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: ren</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: web</div></pre></td></tr></table></figure>
<p>selector就是要找别人的label和自己匹配的，label是给别人来寻找的。如下case，svc中的 Selector:                 app=ren 是表示这个svc要绑定到app=ren的deployment/statefulset上.</p>
<p>被 selector 选中的 Pod，就称为 Service 的 Endpoints</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">[root@poc117 mysql-cluster]# kubectl describe svc nginx-ren </div><div class="line">Name:                     nginx-ren</div><div class="line">Namespace:                default</div><div class="line">Labels:                   app=web</div><div class="line">Annotations:              &lt;none&gt;</div><div class="line">Selector:                 app=ren</div><div class="line">Type:                     NodePort</div><div class="line">IP:                       10.68.34.173</div><div class="line">Port:                     &lt;unset&gt;  8080/TCP</div><div class="line">TargetPort:               80/TCP</div><div class="line">NodePort:                 &lt;unset&gt;  30080/TCP</div><div class="line">Endpoints:                172.20.22.226:80,172.20.56.169:80</div><div class="line">Session Affinity:         None</div><div class="line">External Traffic Policy:  Cluster</div><div class="line">Events:                   &lt;none&gt;</div><div class="line">[root@poc117 mysql-cluster]# kubectl get svc -l app=web</div><div class="line">NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</div><div class="line">nginx-ren   NodePort   10.68.34.173   &lt;none&gt;        8080:30080/TCP   13m</div><div class="line">[root@poc117 mysql-cluster]# kubectl get svc -l app=ren</div><div class="line">No resources found in default namespace.</div><div class="line">[root@poc117 mysql-cluster]# kubectl get svc -l app=web</div><div class="line">NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</div><div class="line">nginx-ren   NodePort   10.68.34.173   &lt;none&gt;        8080:30080/TCP   14m</div></pre></td></tr></table></figure>
<h2 id="service-mesh"><a href="#service-mesh" class="headerlink" title="service mesh"></a>service mesh</h2><ul>
<li>Kubernetes 的本质是应用的生命周期管理，具体来说就是部署和管理（扩缩容、自动恢复、发布）。</li>
<li>Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。</li>
<li>Service Mesh 的基础是透明代理，通过 sidecar proxy 拦截到微服务间流量后再通过控制平面配置管理微服务的行为。</li>
<li>Service Mesh 将流量管理从 Kubernetes 中解耦，Service Mesh 内部的流量无需 <code>kube-proxy</code> 组件的支持，通过为更接近微服务应用层的抽象，管理服务间的流量、安全性和可观察性。</li>
<li>xDS 定义了 Service Mesh 配置的协议标准。</li>
<li>Service Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。</li>
</ul>
<h3 id="Sidecar-注入及流量劫持步骤概述"><a href="#Sidecar-注入及流量劫持步骤概述" class="headerlink" title="Sidecar 注入及流量劫持步骤概述"></a>Sidecar 注入及流量劫持步骤概述</h3><p>下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。</p>
<p><strong>1.</strong> Kubernetes 通过 Admission Controller 自动注入，或者用户使用 <code>istioctl</code> 命令手动注入 sidecar 容器。</p>
<p><strong>2.</strong> 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。</p>
<p><strong>3.</strong> 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式） 将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。</p>
<p><strong>4.</strong> 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考<a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/#通过管理接口获取完整配置" target="_blank" rel="external">通过管理接口获取完整配置</a>。</p>
<p><strong>5.</strong> 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。</p>
<p><strong>6.</strong> Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/" target="_blank" rel="external">https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/</a> Kubernetes 网络疑难杂症排查方法</p>
<p><a href="https://blog.csdn.net/qq_36183935/article/details/90734936" target="_blank" rel="external">https://blog.csdn.net/qq_36183935/article/details/90734936</a>  kube-proxy ipvs模式详解</p>
<p><a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/ebpf-and-k8s-zh/</a>  大规模微服务利器：eBPF 与 Kubernetes</p>
<p><a href="http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/</a>  Life of a Packet in Cilium：实地探索 Pod-to-Service 转发路径及 BPF 处理逻辑</p>
<p><a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/</a>  深入理解 Cilium 的 eBPF 收发包路径（datapath）（KubeCon, 2019）</p>
<p><a href="https://jiayu0x.com/2014/12/02/iptables-essential-summary/" target="_blank" rel="external">https://jiayu0x.com/2014/12/02/iptables-essential-summary/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/21/kubernetes 多集群管理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/21/kubernetes 多集群管理/" itemprop="url">kubernetes 多集群管理</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-21T17:30:03+08:00">
                2020-01-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-多集群管理"><a href="#kubernetes-多集群管理" class="headerlink" title="kubernetes 多集群管理"></a>kubernetes 多集群管理</h1><h2 id="kubectl-管理多集群"><a href="#kubectl-管理多集群" class="headerlink" title="kubectl 管理多集群"></a>kubectl 管理多集群</h2><p>指定config配置文件的方式访问不同的集群</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes</div></pre></td></tr></table></figure>
<p>一个kubectl可以管理多个集群，主要是 ~/.kube/config 里面的配置，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">clusters:</div><div class="line">- cluster:</div><div class="line">    certificate-authority: /root/k8s-cluster.ca</div><div class="line">    server: https://192.168.0.80:6443</div><div class="line">  name: context-az1</div><div class="line">- cluster:</div><div class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCQl0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</div><div class="line">    server: https://192.168.0.97:6443</div><div class="line">  name: context-az3</div><div class="line"></div><div class="line">- context:</div><div class="line">    cluster: context-az1</div><div class="line">    namespace: default</div><div class="line">    user: az1-admin</div><div class="line">  name: az1</div><div class="line">- context:</div><div class="line">    cluster: context-az3</div><div class="line">    namespace: default</div><div class="line">    user: az3-read</div><div class="line">  name: az3</div><div class="line">current-context: az3  //当前使用的集群</div><div class="line"></div><div class="line">kind: Config</div><div class="line">preferences: &#123;&#125;</div><div class="line">users:</div><div class="line">- name: az1-admin</div><div class="line">  user:</div><div class="line">    client-certificate: /root/k8s.crt  //key放在配置文件中</div><div class="line">    client-key: /root/k8s.key</div><div class="line">- name: az3-read</div><div class="line">  user:</div><div class="line">    client-certificate-data: LS0tLS1CRUQ0FURS0tLS0tCg==</div><div class="line">    client-key-data: LS0tLS1CRUdJThuL2VPM0YxSWpEcXBQdmRNbUdiU2c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=</div></pre></td></tr></table></figure>
<p>多个集群中切换的话 ： kubectl config use-context az3</p>
<h3 id="快速合并两个cluster"><a href="#快速合并两个cluster" class="headerlink" title="快速合并两个cluster"></a>快速合并两个cluster</h3><p>简单来讲就是把两个集群的 .kube/config 文件合并，注意context、cluster name别重复了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 必须提前保证两个config文件中的cluster、context名字不能重复</div><div class="line">export KUBECONFIG=~/.kube/config:~/someotherconfig </div><div class="line">kubectl config view --flatten</div><div class="line"></div><div class="line">#激活这个上下文</div><div class="line">kubectl config use-context az1 </div><div class="line"></div><div class="line">#查看所有context</div><div class="line">kubectl config get-contexts </div><div class="line">CURRENT   NAME   CLUSTER       AUTHINFO           NAMESPACE</div><div class="line">          az1    context-az1   az1-admin          default</div><div class="line">*         az2    kubernetes    kubernetes-admin   </div><div class="line">          az3    context-az3   az3-read           default</div></pre></td></tr></table></figure>
<p>背后的原理类似于这个流程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 添加集群 集群地址上一步有获取 ，需要指定ca文件，上一步有获取 </div><div class="line">kubectl config set-cluster cluster-az1 --server https://192.168.146.150:6444  --certificate-authority=/usr/program/k8s-certs/k8s-cluster.ca</div><div class="line"></div><div class="line"># 添加用户 需要指定crt，key文件，上一步有获取</div><div class="line">kubectl config set-credentials az1-admin --client-certificate=/usr/program/k8s-certs/k8s.crt --client-key=/usr/program/k8s-certs/k8s.key</div><div class="line"></div><div class="line"># 指定一个上下文的名字，我这里叫做 az1，随便你叫啥 关联刚才的用户</div><div class="line">kubectl config set-context az1 --cluster=context-az1  --namespace=default --user=az1-admin</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://coreos.com/blog/kubectl-tips-and-tricks" target="_blank" rel="external">http://coreos.com/blog/kubectl-tips-and-tricks</a></p>
<p><a href="https://stackoverflow.com/questions/46184125/how-to-merge-kubectl-config-file-with-kube-config" target="_blank" rel="external">https://stackoverflow.com/questions/46184125/how-to-merge-kubectl-config-file-with-kube-config</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/15/Linux 内存问题汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/15/Linux 内存问题汇总/" itemprop="url">Linux 内存问题汇总</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-15T16:30:03+08:00">
                2020-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-内存问题汇总"><a href="#Linux-内存问题汇总" class="headerlink" title="Linux 内存问题汇总"></a>Linux 内存问题汇总</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="内存使用观察"><a href="#内存使用观察" class="headerlink" title="内存使用观察"></a>内存使用观察</h2><pre><code># free -m
         total       used       free     shared    buffers     cached
Mem:          7515       1115       6400          0        189        492
-/+ buffers/cache:        432       7082
Swap:            0          0          0
</code></pre><p>其中，<a href="https://spongecaptain.cool/SimpleClearFileIO/1.%20page%20cache.html" target="_blank" rel="external">cached 列表示当前的页缓存（Page Cache）占用量</a>，buffers 列表示当前的块缓存（buffer cache）占用量。用一句话来解释：<strong>Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。</strong>页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。</p>
<p><img src="/images/oss/f8d944e2c7a8611384acb820c4471007.png" alt="image.png" style="zoom:80%;"></p>
<p><strong>上图中-/+ buffers/cache: -是指userd去掉buffers/cached后真正使用掉的内存; +是指free加上buffers和cached后真正free的内存大小。</strong></p>
<h2 id="free"><a href="#free" class="headerlink" title="free"></a>free</h2><p>free是从 /proc/meminfo 读取数据然后展示：</p>
<blockquote>
<p>buff/cache = Buffers + Cached + SReclaimable</p>
<p>Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@az1-drds-79 ~]# cat /proc/meminfo |egrep -i &quot;buff|cach|SReclai&quot;</div><div class="line">Buffers:          817764 kB</div><div class="line">Cached:         76629252 kB</div><div class="line">SwapCached:            0 kB</div><div class="line">SReclaimable:    7202264 kB</div><div class="line">[root@az1-drds-79 ~]# free -k</div><div class="line">             total       used       free     shared    buffers     cached</div><div class="line">Mem:      97267672   95522336    1745336          0     817764   76629352</div><div class="line">-/+ buffers/cache:   18075220   79192452</div><div class="line">Swap:            0          0          0</div></pre></td></tr></table></figure>
<p>在内核启动时，物理页面将加入到伙伴系统 （Buddy System）中，用户申请内存时分配，释放时回收。为了照顾慢速设备及兼顾多种 workload，Linux 将页面类型分为匿名页（Anon Page）和文件页 （Page Cache），及 swapness，使用 Page Cache 缓存文件 （慢速设备），通过 swap cache 和 swapness 交由用户根据负载特征决定内存不足时回收二者的比例。</p>
<h2 id="cached过高回收"><a href="#cached过高回收" class="headerlink" title="cached过高回收"></a>cached过高回收</h2><p>系统内存大体可分为三块，应用程序使用内存、系统Cache 使用内存（包括page cache、buffer，内核slab 等）和Free 内存。</p>
<ul>
<li>应用程序使用内存：应用使用都是虚拟内存，应用申请内存时只是分配了地址空间，并未真正分配出物理内存，等到应用真正访问内存时会触发内核的缺页中断，这时候才真正的分配出物理内存，映射到用户的地址空间，因此应用使用内存是不需要连续的，内核有机制将非连续的物理映射到连续的进程地址空间中（mmu），缺页中断申请的物理内存，内核优先给低阶碎内存。</li>
<li><p>系统Cache 使用内存：使用的也是虚拟内存，申请机制与应用程序相同。</p>
</li>
<li><p>Free 内存，未被使用的物理内存，这部分内存以4k 页的形式被管理在内核伙伴算法结构中，相邻的2^n 个物理页会被伙伴算法组织到一起，形成一块连续物理内存，所谓的阶内存就是这里的n (0&lt;= n &lt;=10)，高阶内存指的就是一块连续的物理内存，在OSS 的场景中，如果3阶内存个数比较小的情况下，如果系统有吞吐burst 就会触发Drop cache 情况。</p>
</li>
</ul>
<p>cache回收：<br>    echo 1/2/3 &gt;/proc/sys/vm/drop_cached</p>
<p>查看回收后：</p>
<pre><code>cat /proc/meminfo
</code></pre><p>手动回收系统Cache、Buffer，这个文件可以设置的值分别为1、2、3。它们所表示的含义为：</p>
<p><strong>echo 1 &gt; /proc/sys/vm/drop_caches</strong>:表示清除pagecache。</p>
<p><strong>echo 2 &gt; /proc/sys/vm/drop_caches</strong>:表示清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。</p>
<p><strong>echo 3 &gt; /proc/sys/vm/drop_caches</strong>:表示清除pagecache和slab分配器中的缓存对象。</p>
<h2 id="cached无法回收"><a href="#cached无法回收" class="headerlink" title="cached无法回收"></a>cached无法回收</h2><p>可能是正打开的文件占用了cached，比如 vim 打开了一个巨大的文件；比如 mount的 tmpfs； 比如 journald 日志等等</p>
<h3 id="通过vmtouch-查看"><a href="#通过vmtouch-查看" class="headerlink" title="通过vmtouch 查看"></a>通过<a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a> 查看</h3><pre><code># vmtouch -v test.x86_64.rpm 
test.x86_64.rpm
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 10988/10988

           Files: 1
     Directories: 0
  Resident Pages: 10988/10988  42M/42M  100%
         Elapsed: 0.000594 seconds

# ls -lh test.x86_64.rpm
-rw-r--r-- 1 root root 43M 10月  8 14:11 test.x86_64.rpm
</code></pre><p>如上，表示整个文件 test.x86_64.rpm 都被cached了，回收的话执行：</p>
<pre><code>vmtouch -e test.x86_64.rpm // 或者： echo 3 &gt;/proc/sys/vm/drop_cached
</code></pre><h3 id="遍历某个目录下的所有文件被cached了多少"><a href="#遍历某个目录下的所有文件被cached了多少" class="headerlink" title="遍历某个目录下的所有文件被cached了多少"></a>遍历某个目录下的所有文件被cached了多少</h3><pre><code># vmtouch -vt /var/log/journal/
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000011ba49-00059979e0926f43.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000152f41-00059b2c88eb4344.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-00000000000f2181-000598335fcd492f.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000129aea-000599e83996db80.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000009f171-000595a722ead670.journal
…………
           Files: 48
 Directories: 2
 Touched Pages: 468992 (1G)
 Elapsed: 13.274 seconds
</code></pre><h2 id="消失的内存"><a href="#消失的内存" class="headerlink" title="消失的内存"></a>消失的内存</h2><p>OS刚启动后就报内存不够了，什么都没跑就500G没了，cached和buffer基本没用，纯粹就是used占用高，top按内存排序没有超过0.5%的进程</p>
<p>参考： <a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">[aliyun@uos15 18:40 /u02/backup_15/leo/benchmark/run]</div><div class="line">$free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503         501           1           0           0           1</div><div class="line">Swap:            15          12           3</div><div class="line"></div><div class="line">$cat /proc/meminfo </div><div class="line">MemTotal:       528031512 kB</div><div class="line">MemFree:         1469632 kB</div><div class="line">MemAvailable:          0 kB</div><div class="line">VmallocTotal:   135290290112 kB</div><div class="line">VmallocUsed:           0 kB</div><div class="line">VmallocChunk:          0 kB</div><div class="line">Percpu:            81920 kB</div><div class="line">AnonHugePages:    950272 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:   252557   ----- 预分配太多，一个2M，加起来刚好500G了</div><div class="line">HugePages_Free:    252557</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:        517236736 kB</div><div class="line"></div><div class="line">以下是一台正常的机器对比：</div><div class="line">Percpu:            41856 kB</div><div class="line">AnonHugePages:  11442176 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:       0            ----没有做预分配</div><div class="line">HugePages_Free:        0</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:               0 kB</div><div class="line"></div><div class="line">[aliyun@uos16 18:43 /home/aliyun]</div><div class="line">$free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503          20         481           0           1         480</div><div class="line">Swap:            15           0          15</div><div class="line"></div><div class="line">对有问题的机器执行：</div><div class="line"># echo 1024 &gt; /proc/sys/vm/nr_hugepages</div><div class="line">可以看到内存恢复正常了 </div><div class="line">root@uos15:/u02/backup_15/leo/benchmark/run# free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503          10         492           0           0         490</div><div class="line">Swap:            15          12           3</div><div class="line">root@uos15:/u02/backup_15/leo/benchmark/run# cat /proc/meminfo </div><div class="line">MemTotal:       528031512 kB</div><div class="line">MemFree:        516106832 kB</div><div class="line">MemAvailable:   514454408 kB</div><div class="line">VmallocTotal:   135290290112 kB</div><div class="line">VmallocUsed:           0 kB</div><div class="line">VmallocChunk:          0 kB</div><div class="line">Percpu:            81920 kB</div><div class="line">AnonHugePages:    313344 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:    1024</div><div class="line">HugePages_Free:     1024</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:         2097152 kB</div></pre></td></tr></table></figure>
<h2 id="定制内存"><a href="#定制内存" class="headerlink" title="定制内存"></a>定制内存</h2><p>物理内存700多G，要求OS只能用512G：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">24条32G的内存条，总内存768G</div><div class="line"># dmidecode -t memory |grep &quot;Size: 32 GB&quot;</div><div class="line">  Size: 32 GB</div><div class="line">…………</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">root@uos15:/etc# dmidecode -t memory |grep &quot;Size: 32 GB&quot; | wc -l</div><div class="line">24</div><div class="line"></div><div class="line"># cat /boot/grub/grub.cfg  |grep 512</div><div class="line">  linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</div><div class="line">    linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</div></pre></td></tr></table></figure>
<p>​    </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/13/kubernetes 卷和volume/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/13/kubernetes 卷和volume/" itemprop="url">kubernetes volume and storage</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-13T17:30:03+08:00">
                2020-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-volume-and-storage"><a href="#kubernetes-volume-and-storage" class="headerlink" title="kubernetes volume and storage"></a>kubernetes volume and storage</h1><p>通常部署应用需要一些永久存储，kubernetes提供了PersistentVolume （PV，实际存储）、PersistentVolumeClaim （PVC，Pod访问PV的接口）、StorageClass来支持。</p>
<p>它为 PersistentVolume 定义了 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class" target="_blank" rel="external">StorageClass 名称</a> 为 <code>manual</code>，StorageClass 名称用来将 PersistentVolumeClaim 请求绑定到该 PersistentVolume。</p>
<p>PVC是用来描述希望使用什么样的或者说是满足什么条件的存储，它的全称是Persistent Volume Claim，也就是持久化存储声明。开发人员使用这个来描述该容器需要一个什么存储。</p>
<p>PVC就相当于是容器和PV之间的一个接口，使用人员只需要和PVC打交道即可。另外你可能也会想到如果当前环境中没有合适的PV和我的PVC绑定，那么我创建的POD不就失败了么？的确是这样的，不过如果发现这个问题，那么就赶快创建一个合适的PV，那么这时候持久化存储循环控制器会不断的检查PVC和PV，当发现有合适的可以绑定之后它会自动给你绑定上然后被挂起的POD就会自动启动，而不需要你重建POD。</p>
<p>创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。 如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume，则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。<strong>PVC的大小可以小于PV的大小</strong>。</p>
<p>一旦 PV 和 PVC 绑定后，<code>PersistentVolumeClaim</code> 绑定是排他性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射。</p>
<p><strong>注意</strong>：PV必须先于POD创建，而且只能是网络存储不能属于任何Node，虽然它支持HostPath类型但由于你不知道POD会被调度到哪个Node上，所以你要定义HostPath类型的PV就要保证所有节点都要有HostPath中指定的路径。</p>
<h2 id="PV-和PVC的关系"><a href="#PV-和PVC的关系" class="headerlink" title="PV 和PVC的关系"></a>PV 和PVC的关系</h2><p>PVC就会和PV进行绑定，绑定的一些原则：</p>
<ol>
<li>PV和PVC中的spec关键字段要匹配，比如存储（storage）大小。</li>
<li>PV和PVC中的storageClassName字段必须一致，这个后面再说。</li>
<li>上面的labels中的标签只是增加一些描述，对于PVC和PV的绑定没有关系</li>
</ol>
<p>PV的accessModes：支持三种类型</p>
<ul>
<li>ReadWriteMany 多路读写，卷能被集群多个节点挂载并读写</li>
<li>ReadWriteOnce 单路读写，卷只能被单一集群节点挂载读写</li>
<li>ReadOnlyMany 多路只读，卷能被多个集群节点挂载且只能读</li>
</ul>
<p>PV状态：</p>
<ul>
<li>Available – 资源尚未被claim使用</li>
<li>Bound – 卷已经被绑定到claim了</li>
<li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li>
<li><p>Failed – 卷自动回收失败</p>
<p>PV<strong>回收Recycling</strong>—pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</p>
</li>
<li><p>保留（Retain）： 当删除与之绑定的PVC时候，这个PV被标记为released（PVC与PV解绑但还没有执行回收策略）且之前的数据依然保存在该PV上，但是该PV不可用，需要手动来处理这些数据并删除该PV。</p>
</li>
<li>删除（Delete）：当删除与之绑定的PVC时候</li>
<li>回收（Recycle）：这个在1.14版本中以及被废弃，取而代之的是推荐使用动态存储供给策略，它的功能是当删除与该PV关联的PVC时，自动删除该PV中的所有数据</li>
</ul>
<h3 id="更改-PersistentVolume-的回收策略"><a href="#更改-PersistentVolume-的回收策略" class="headerlink" title="更改 PersistentVolume 的回收策略"></a>更改 PersistentVolume 的回收策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#kubectl patch pv wordpress-data -p &apos;&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Delete&quot;&#125;&#125;&apos;</div><div class="line">persistentvolume/wordpress-data patched</div></pre></td></tr></table></figure>
<p>本地卷（hostPath）也就是LPV不支持动态供给的方式，延迟绑定，就是为了综合考虑所有因素再进行POD调度。其根本原因是动态供给是先调度POD到节点，然后动态创建PV以及绑定PVC最后运行POD；而LPV是先创建与某一节点关联的PV，然后在调度的时候综合考虑各种因素而且要包括PV在哪个节点，然后再进行调度，到达该节点后在进行PVC的绑定。也就说动态供给不考虑节点，LPV必须考虑节点。所以这两种机制有冲突导致无法在动态供给策略下使用LPV。换句话说动态供给是PV跟着POD走，而LPV是POD跟着PV走。</p>
<h2 id="PV-和-PVC"><a href="#PV-和-PVC" class="headerlink" title="PV 和 PVC"></a>PV 和 PVC</h2><p>创建 pv controller 和pvc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#cat mysql-pv.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: simple-pv-volume</div><div class="line">  labels:</div><div class="line">    type: local</div><div class="line">spec:</div><div class="line">  storageClassName: manual</div><div class="line">  capacity:</div><div class="line">    storage: 20Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  hostPath:</div><div class="line">    path: &quot;/mnt/simple&quot;</div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolumeClaim</div><div class="line">metadata:</div><div class="line">  name: pv-claim</div><div class="line">spec:</div><div class="line">  storageClassName: manual</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 20Gi</div></pre></td></tr></table></figure>
<h3 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h3><p>PV是运维人员来创建的，开发操作PVC，可是大规模集群中可能会有很多PV，如果这些PV都需要运维手动来处理这也是一件很繁琐的事情，所以就有了动态供给概念，也就是Dynamic Provisioning。而我们上面的创建的PV都是静态供给方式，也就是Static Provisioning。而动态供给的关键就是StorageClass，它的作用就是创建PV模板。</p>
<p>创建StorageClass里面需要定义PV属性比如存储类型、大小等；另外创建这种PV需要用到存储插件。最终效果是，用户提交PVC，里面指定存储类型，如果符合我们定义的StorageClass，则会为其自动创建PV并进行绑定。</p>
<p><strong>简单可以把storageClass理解为名字，只是这个名字可以重复，然后pvc和pv之间通过storageClass来绑定。</strong></p>
<p>如下case中两个pv和两个pvc的绑定就是通过storageClass(一致)来实现的（当然pvc要求的大小也必须和pv一致）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">#kubectl get pv</div><div class="line">NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS   REASON   AGE</div><div class="line">mariadb-pv       8Gi        RWO            Retain           Bound    default/data-wordpress-mariadb-0   db                      3m54s</div><div class="line">wordpress-data   10Gi       RWO            Retain           Bound    default/wordpress                  wordpress               3m54s</div><div class="line"></div><div class="line">[root@az3-k8s-11 15:35 /root/charts/bitnami/wordpress]</div><div class="line">#kubectl get pvc</div><div class="line">NAME                       STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE</div><div class="line">data-wordpress-mariadb-0   Bound    mariadb-pv       8Gi        RWO            db             4m21s</div><div class="line">wordpress                  Bound    wordpress-data   10Gi       RWO            wordpress      4m21s</div><div class="line"></div><div class="line">#cat create-pv.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: mariadb-pv</div><div class="line">spec:</div><div class="line">  capacity:</div><div class="line">    storage: 8Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  persistentVolumeReclaimPolicy: Retain</div><div class="line">  storageClassName: db</div><div class="line">  hostPath:</div><div class="line">    path: /mnt/mariadb-pv</div><div class="line"></div><div class="line">---</div><div class="line"></div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: wordpress-data</div><div class="line">spec:</div><div class="line">  capacity:</div><div class="line">    storage: 10Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  persistentVolumeReclaimPolicy: Retain</div><div class="line">  storageClassName: wordpress</div><div class="line">  hostPath:</div><div class="line">    path: /mnt/wordpress-pv</div><div class="line"></div><div class="line">----对应 pvc的定义参数：</div><div class="line">persistence:</div><div class="line">  enabled: true</div><div class="line">  storageClass: &quot;wordpress&quot;</div><div class="line">  accessMode: ReadWriteOnce</div><div class="line">  size: 10Gi</div><div class="line">  </div><div class="line">  persistence:</div><div class="line">    enabled: true</div><div class="line">    mountPath: /bitnami/mariadb</div><div class="line">    storageClass: &quot;db&quot;</div><div class="line">    annotations: &#123;&#125;</div><div class="line">    accessModes:</div><div class="line">      - ReadWriteOnce</div><div class="line">    size: 8Gi</div></pre></td></tr></table></figure>
<h4 id="定义StorageClass"><a href="#定义StorageClass" class="headerlink" title="定义StorageClass"></a>定义StorageClass</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">kind: StorageClass</div><div class="line">apiVersion: storage.k8s.io/v1</div><div class="line">metadata:</div><div class="line">  name: local-storage</div><div class="line">provisioner: kubernetes.io/no-provisioner</div><div class="line">volumeBindingMode: WaitForFirstConsumer</div></pre></td></tr></table></figure>
<h4 id="定义PVC"><a href="#定义PVC" class="headerlink" title="定义PVC"></a>定义PVC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">kind: PersistentVolumeClaim</div><div class="line">apiVersion: v1</div><div class="line">metadata:</div><div class="line">  name: local-claim</div><div class="line">spec:</div><div class="line">  accessModes:</div><div class="line">  - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 5Gi</div><div class="line">  storageClassName: local-storage</div></pre></td></tr></table></figure>
<h2 id="delete-pv-卡住"><a href="#delete-pv-卡住" class="headerlink" title="delete pv 卡住"></a>delete pv 卡住</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">#kubectl describe pv wordpress-pv</div><div class="line">Name:            wordpress-pv</div><div class="line">Labels:          &lt;none&gt;</div><div class="line">Annotations:     pv.kubernetes.io/bound-by-controller: yes</div><div class="line">Finalizers:      [kubernetes.io/pv-protection]  --- 问题在finalizers</div><div class="line">StorageClass:    </div><div class="line">Status:          Terminating (lasts 18h)</div><div class="line">Claim:           default/wordpress</div><div class="line">Reclaim Policy:  Retain</div><div class="line">Access Modes:    RWO</div><div class="line">VolumeMode:      Filesystem</div><div class="line">Capacity:        10Gi</div><div class="line">Node Affinity:   &lt;none&gt;</div><div class="line">Message:         </div><div class="line">Source:</div><div class="line">    Type:      NFS (an NFS mount that lasts the lifetime of a pod)</div><div class="line">    Server:    192.168.0.111</div><div class="line">    Path:      /mnt/wordpress-pv</div><div class="line">    ReadOnly:  false</div><div class="line">Events:        &lt;none&gt;</div><div class="line"></div><div class="line">先执行后就能自动删除了：</div><div class="line">kubectl patch pv wordpress-pv -p &apos;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;: []&#125;&#125;&apos; --type=merge</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/12/kubernetes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/12/kubernetes/" itemprop="url">kubernetes 集群部署</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-12T17:30:03+08:00">
                2020-01-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-集群部署"><a href="#kubernetes-集群部署" class="headerlink" title="kubernetes 集群部署"></a>kubernetes 集群部署</h1><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>系统参数修改</p>
<p>docker部署</p>
<p>kubeadm install</p>
<p><a href="https://www.kubernetes.org.cn/4256.html" target="_blank" rel="external">https://www.kubernetes.org.cn/4256.html</a> </p>
<p><a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster" target="_blank" rel="external">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a></p>
<p>镜像源被墙，可以用阿里云镜像源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 配置源</div><div class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</div><div class="line">[kubernetes]</div><div class="line">name=Kubernetes</div><div class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">repo_gpgcheck=1</div><div class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</div><div class="line">EOF</div><div class="line"></div><div class="line"># 安装</div><div class="line">yum install -y kubelet kubeadm kubectl ipvsadm</div></pre></td></tr></table></figure>
<h2 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h2><p>多网卡情况下有必要指定网卡：–apiserver-advertise-address=192.168.0.80</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 使用本地 image repository</span></div><div class="line">kubeadm init --kubernetes-version=1.18.0  --apiserver-advertise-address=192.168.0.110   --image-repository registry:5000/registry.aliyuncs.com/google_containers  --service-cidr=10.10.0.0/16 --pod-network-cidr=10.122.0.0/16 </div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> 给api-server 指定外网地址，在服务器有内网、外网多个ip的时候适用</span></div><div class="line">kubeadm init --control-plane-endpoint 外网-ip:6443 --image-repository=registry:5000/registry.aliyuncs.com/google_containers --kubernetes-version=v1.21.0  --pod-network-cidr=172.16.0.0/16</div><div class="line"><span class="meta">#</span><span class="bash">--apiserver-advertise-address=30.1.1.1，设置 apiserver 的 IP 地址，对于多网卡服务器来说很重要（比如 VirtualBox 虚拟机就用了两块网卡），可以指定 apiserver 在哪个网卡上对外提供服务。</span></div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> node join <span class="built_in">command</span></span></div><div class="line"><span class="meta">#</span><span class="bash">kubeadm token create --<span class="built_in">print</span>-join-command</span></div><div class="line">kubeadm join 192.168.0.110:6443 --token 1042rl.b4qn9iuz6xv1ri7b     --discovery-token-ca-cert-hash sha256:341a4bcfde9668077ef29211c2a151fe6e9334eea8955f645698706b3bf47a49 </div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"><span class="comment"># 查看集群配置</span></span></div><div class="line">kubectl get configmap -n kube-system kubeadm-config -o yaml</div></pre></td></tr></table></figure>
<p>将一个node设置为不可调度，隔离出来，比如master 默认是不可调度的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl cordon &lt;node-name&gt;</div><div class="line">kubectl uncordon &lt;node-name&gt;</div></pre></td></tr></table></figure>
<h2 id="kubectl-管理多集群"><a href="#kubectl-管理多集群" class="headerlink" title="kubectl 管理多集群"></a>kubectl 管理多集群</h2><p>一个kubectl可以管理多个集群，主要是 ~/.kube/config 里面的配置，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">clusters:</div><div class="line">- cluster:</div><div class="line">    certificate-authority: /root/k8s-cluster.ca</div><div class="line">    server: https://192.168.0.80:6443</div><div class="line">  name: context-az1</div><div class="line">- cluster:</div><div class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCQl0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</div><div class="line">    server: https://192.168.0.97:6443</div><div class="line">  name: context-az3</div><div class="line"></div><div class="line">- context:</div><div class="line">    cluster: context-az1</div><div class="line">    namespace: default</div><div class="line">    user: az1-admin</div><div class="line">  name: az1</div><div class="line">- context:</div><div class="line">    cluster: context-az3</div><div class="line">    namespace: default</div><div class="line">    user: az3-read</div><div class="line">  name: az3</div><div class="line">current-context: az3  //当前使用的集群</div><div class="line"></div><div class="line">kind: Config</div><div class="line">preferences: &#123;&#125;</div><div class="line">users:</div><div class="line">- name: az1-admin</div><div class="line">  user:</div><div class="line">    client-certificate: /root/k8s.crt  //key放在配置文件中</div><div class="line">    client-key: /root/k8s.key</div><div class="line">- name: az3-read</div><div class="line">  user:</div><div class="line">    client-certificate-data: LS0tLS1CRUQ0FURS0tLS0tCg==</div><div class="line">    client-key-data: LS0tLS1CRUdJThuL2VPM0YxSWpEcXBQdmRNbUdiU2c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=</div></pre></td></tr></table></figure>
<p>多个集群中切换的话 ： kubectl config use-context az3</p>
<h3 id="快速合并两个cluster"><a href="#快速合并两个cluster" class="headerlink" title="快速合并两个cluster"></a>快速合并两个cluster</h3><p>简单来讲就是把两个集群的 .kube/config 文件合并，注意context、cluster name别重复了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 必须提前保证两个config文件中的cluster、context名字不能重复</div><div class="line">export KUBECONFIG=~/.kube/config:~/someotherconfig </div><div class="line">kubectl config view --flatten</div><div class="line"></div><div class="line">#激活这个上下文</div><div class="line">kubectl config use-context az1 </div><div class="line"></div><div class="line">#查看所有context</div><div class="line">kubectl config get-contexts </div><div class="line">CURRENT   NAME   CLUSTER       AUTHINFO           NAMESPACE</div><div class="line">          az1    context-az1   az1-admin          default</div><div class="line">*         az2    kubernetes    kubernetes-admin   </div><div class="line">          az3    context-az3   az3-read           default</div></pre></td></tr></table></figure>
<p>背后的原理类似于这个流程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 添加集群 集群地址上一步有获取 ，需要指定ca文件，上一步有获取 </div><div class="line">kubectl config set-cluster cluster-az1 --server https://192.168.146.150:6444  --certificate-authority=/usr/program/k8s-certs/k8s-cluster.ca</div><div class="line"></div><div class="line"># 添加用户 需要指定crt，key文件，上一步有获取</div><div class="line">kubectl config set-credentials az1-admin --client-certificate=/usr/program/k8s-certs/k8s.crt --client-key=/usr/program/k8s-certs/k8s.key</div><div class="line"></div><div class="line"># 指定一个上下文的名字，我这里叫做 az1，随便你叫啥 关联刚才的用户</div><div class="line">kubectl config set-context az1 --cluster=context-az1  --namespace=default --user=az1-admin</div></pre></td></tr></table></figure>
<h2 id="apiserver高可用"><a href="#apiserver高可用" class="headerlink" title="apiserver高可用"></a>apiserver高可用</h2><p>默认只有一个apiserver，可以考虑用haproxy和keepalive来做一组apiserver的负载均衡：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">docker run -d --name kube-haproxy \</div><div class="line">-v /etc/haproxy:/usr/local/etc/haproxy:ro \</div><div class="line">-p 8443:8443 \</div><div class="line">-p 1080:1080 \</div><div class="line">--restart always \</div><div class="line">haproxy:1.7.8-alpine</div></pre></td></tr></table></figure>
<p>haproxy配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">#cat /etc/haproxy/haproxy.cfg </div><div class="line">global</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  maxconn 50000</div><div class="line">  uid 99</div><div class="line">  gid 99</div><div class="line">  #daemon</div><div class="line">  nbproc 1</div><div class="line">  pidfile haproxy.pid</div><div class="line"></div><div class="line">defaults</div><div class="line">  mode http</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  maxconn 50000</div><div class="line">  retries 3</div><div class="line">  timeout connect 5s</div><div class="line">  timeout client 30s</div><div class="line">  timeout server 30s</div><div class="line">  timeout check 2s</div><div class="line"></div><div class="line">listen admin_stats</div><div class="line">  mode http</div><div class="line">  bind 0.0.0.0:1080</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  stats refresh 30s</div><div class="line">  stats uri     /haproxy-status</div><div class="line">  stats realm   Haproxy\ Statistics</div><div class="line">  stats auth    will:will</div><div class="line">  stats hide-version</div><div class="line">  stats admin if TRUE</div><div class="line"></div><div class="line">frontend k8s-https</div><div class="line">  bind 0.0.0.0:8443</div><div class="line">  mode tcp</div><div class="line">  #maxconn 50000</div><div class="line">  default_backend k8s-https</div><div class="line"></div><div class="line">backend k8s-https</div><div class="line">  mode tcp</div><div class="line">  balance roundrobin</div><div class="line">  server lab1 192.168.1.81:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div><div class="line">  server lab2 192.168.1.82:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div><div class="line">  server lab3 192.168.1.83:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div></pre></td></tr></table></figure>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</div><div class="line"></div><div class="line">#或者老版本的calico</div><div class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</div></pre></td></tr></table></figure>
<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>在所有node节点都在一个二层网络时候，flannel提供hostgw实现，避免vxlan实现的udp封装开销，估计是目前最高效的；calico也针对L3 Fabric，推出了IPinIP的选项，利用了GRE隧道封装；因此这些插件都能适合很多实际应用场景。</p>
<p>Service cluster IP尽可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问</p>
<p>网络插件由 containernetworking-plugins rpm包来提供，一般里面会有flannel、vlan等，安装在 /usr/libexec/cni/ 下（老版本没有带calico）</p>
<p>kubelet启动参数会配置 KUBELET_NETWORK_ARGS=–network-plugin=cni –cni-conf-dir=/etc/cni/net.d –cni-bin-dir=/usr/libexec/cni </p>
<h2 id="kubectl-启动容器"><a href="#kubectl-启动容器" class="headerlink" title="kubectl 启动容器"></a>kubectl 启动容器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl run -i --tty busybox --image=registry:5000/busybox -- sh</div><div class="line">kubectl attach busybox -c busybox -i -t</div></pre></td></tr></table></figure>
<h2 id="dashboard"><a href="#dashboard" class="headerlink" title="dashboard"></a>dashboard</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc7/aio/deploy/recommented.yaml</div><div class="line"></div><div class="line">#暴露 dashboard 服务端口 (recommended中如果已经定义了 30000这个nodeport，所以这个命令不需要了)</div><div class="line">kubectl port-forward -n kubernetes-dashboard  svc/kubernetes-dashboard 30000:443 --address 0.0.0.0</div></pre></td></tr></table></figure>
<p>dashboard login token：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#kubectl describe secrets -n kubernetes-dashboard   | grep token | awk &apos;NR==3&#123;print $2&#125;&apos;</div><div class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IndRc0hiMkdpWHRwN1FObTcyeUdhOHI0eUxYLTlvODd2U0NBcU1GY0t1Sk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLXRia3o5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYzM2MzBhOS0xMjBjLTRhNmYtYjM0ZS0zM2JhMTE1OWU1OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6ZGVmYXVsdCJ9.SP4JEw0kGDmyxrtcUC3HALq99Xr99E-tie5fk4R8odLJBAYN6HxEx80RbTSnkeSMJNApbtwXBLrp4I_w48kTkr93HJFM-oxie3RVLK_mEpZBF2JcfMk6qhfz4RjPiqmG6mGyW47mmY4kQ4fgpYSmZYR4LPJmVMw5W2zo5CGhZT8rKtgmi5_ROmYpWcd2ZUORaexePgesjjKwY19bLEXFOwdsqekwEvj1_zaJhKAehF_dBdgW9foFXkbXOX0xAC0QNnKUwKPanuFOVZDg1fhyV-eyi6c9-KoTYqZMJTqZyIzscIwruIRw0oauJypcdgi7ykxAubMQ4sWEyyFafSEYWg</div></pre></td></tr></table></figure>
<p>dashboard 显示为空的话(留意报错信息，一般是用户权限，重新授权即可)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl delete clusterrolebinding kubernetes-dashboard</div><div class="line">kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard --user=&quot;system:serviceaccount:kubernetes-dashboard:default&quot;</div></pre></td></tr></table></figure>
<p>其中：system:serviceaccount:kubernetes-dashboard:default 来自于报错信息中的用户名</p>
<p>默认dashboard login很快expired，可以设置不过期：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ kubectl -n kubernetes-dashboard edit deployments kubernetes-dashboard</div><div class="line">...</div><div class="line">spec:</div><div class="line">      containers:</div><div class="line">      - args:</div><div class="line">        - --auto-generate-certificates</div><div class="line">        - --token-ttl=0                //增加这行表示不expire</div><div class="line">        </div><div class="line">        --enable-skip-login            //增加这行表示不需要token 就能login，不推荐</div></pre></td></tr></table></figure>
<p>kubectl proxy –address 0.0.0.0 –accept-hosts ‘.*’</p>
<h2 id="node管理调度"><a href="#node管理调度" class="headerlink" title="node管理调度"></a>node管理调度</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">//如何优雅删除node</div><div class="line">kubectl drain my-node        # 对 my-node 节点进行清空操作，为节点维护做准备</div><div class="line">kubectl drain ky4 --ignore-daemonsets --delete-local-data # 驱逐pod</div><div class="line">kubectl delete node ky4			 # 删除node</div><div class="line"></div><div class="line">kubectl cordon my-node       # 标记 my-node 节点为不可调度</div><div class="line">kubectl uncordon my-node     # 标记 my-node 节点为可以调度</div><div class="line">kubectl top node my-node     # 显示给定节点的度量值</div><div class="line">kubectl cluster-info         # 显示主控节点和服务的地址</div><div class="line">kubectl cluster-info dump    # 将当前集群状态转储到标准输出</div><div class="line">kubectl cluster-info dump --output-directory=/path/to/cluster-state   # 将当前集群状态输出到 /path/to/cluster-state</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> 如果已存在具有指定键和效果的污点，则替换其值为指定值</span></div><div class="line">kubectl taint nodes foo dedicated=special-user:NoSchedule</div><div class="line">kubectl taint nodes poc65 node-role.kubernetes.io/master:NoSchedule-</div></pre></td></tr></table></figure>
<h3 id="地址"><a href="#地址" class="headerlink" title="地址 "></a>地址<a href="https://kubernetes.io/zh/docs/concepts/architecture/nodes/#addresses" target="_blank" rel="external"> </a></h3><p>这些字段的用法取决于你的云服务商或者物理机配置。</p>
<ul>
<li>HostName：由节点的内核设置。可以通过 kubelet 的 <code>--hostname-override</code> 参数覆盖。</li>
<li>ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。</li>
<li>InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。</li>
</ul>
<h3 id="状况"><a href="#状况" class="headerlink" title="状况"></a>状况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># kubectl get node -o wide</div><div class="line">NAME             STATUS                     ROLES    AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</div><div class="line">172.26.137.114   Ready                      master   6d1h   v1.19.0   172.26.137.114   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</div><div class="line">172.26.137.115   Ready                      node     6d1h   v1.19.0   172.26.137.115   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</div><div class="line">172.26.137.116   Ready,SchedulingDisabled   node     6d1h   v1.19.0   172.26.137.116   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</div></pre></td></tr></table></figure>
<p>如果 Ready 条件处于 <code>Unknown</code> 或者 <code>False</code> 状态的时间超过了 <code>pod-eviction-timeout</code> 值， （一个传递给 <a href="https://kubernetes.io/docs/reference/generated/kube-controller-manager/" target="_blank" rel="external">kube-controller-manager</a> 的参数）， 节点上的所有 Pod 都会被节点控制器计划删除。默认的逐出超时时长为 <strong>5 分钟</strong>。 某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。 删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。 与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。</p>
<h2 id="node-cidr-缺失"><a href="#node-cidr-缺失" class="headerlink" title="node cidr 缺失"></a>node cidr 缺失</h2><p>flannel pod 运行正常，pod无法创建，检查flannel日志发现该node cidr缺失</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">I0818 08:06:38.951132       1 main.go:733] Defaulting external v6 address to interface address (&lt;nil&gt;)</div><div class="line">I0818 08:06:38.951231       1 vxlan.go:137] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false</div><div class="line">E0818 08:06:38.951550       1 main.go:325] Error registering network: failed to acquire lease: node &quot;ky3&quot; pod cidr not assigned</div><div class="line">I0818 08:06:38.951604       1 main.go:439] Stopping shutdownHandler...</div></pre></td></tr></table></figure>
<p>正常来说describe node会看到如下的cidr信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"> Kube-Proxy Version:         v1.15.8-beta.0</div><div class="line">PodCIDR:                     172.19.1.0/24</div><div class="line">Non-terminated Pods:         (3 in total)</div></pre></td></tr></table></figure>
<p>可以手工给node添加cidr</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl patch node ky3 -p &apos;&#123;&quot;spec&quot;:&#123;&quot;podCIDR&quot;:&quot;172.19.3.0/24&quot;&#125;&#125;&apos;</div></pre></td></tr></table></figure>
<h2 id="prometheus"><a href="#prometheus" class="headerlink" title="prometheus"></a>prometheus</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/coreos/kube-prometheus.git</div><div class="line">kubectl apply -f manifests/setup</div><div class="line">kubectl apply -f manifests/</div></pre></td></tr></table></figure>
<p>暴露grafana端口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl port-forward --address 0.0.0.0 svc/grafana -n monitoring 3000:3000</div></pre></td></tr></table></figure>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><h3 id="DRDS-deployment"><a href="#DRDS-deployment" class="headerlink" title="DRDS deployment"></a>DRDS deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: drds</div><div class="line"></div><div class="line">---</div><div class="line"></div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: drds-deployment</div><div class="line">  namespace: drds</div><div class="line">  labels:</div><div class="line">    app: drds-server</div><div class="line">spec:</div><div class="line">  # 创建2个nginx容器</div><div class="line">  replicas: 3</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: drds-server</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: drds-server</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: drds-server</div><div class="line">        image: registry:5000/drds-image:v5_wisp_5.4.5-15940932</div><div class="line">        ports:</div><div class="line">        - containerPort: 8507</div><div class="line">        - containerPort: 8607</div><div class="line">        env:</div><div class="line">        - name: diamond_server_port</div><div class="line">          value: &quot;8100&quot;</div><div class="line">        - name: diamond_server_list</div><div class="line">          value: &quot;192.168.0.79,192.168.0.82&quot;</div><div class="line">        - name: drds_server_id</div><div class="line">          value: &quot;1&quot;</div></pre></td></tr></table></figure>
<h3 id="DRDS-Service"><a href="#DRDS-Service" class="headerlink" title="DRDS Service"></a>DRDS Service</h3><p>每个 drds 容器会通过8507提供服务，service通过3306来为一组8507做负载均衡，这个service的3306是在cluster-ip上，外部无法访问</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: drds-service</div><div class="line">  namespace: drds</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: drds-server</div><div class="line">  ports:</div><div class="line">    - protocol: TCP</div><div class="line">      port: 3306</div><div class="line">      targetPort: 8507</div></pre></td></tr></table></figure>
<p>通过node port来访问 drds service（同时会有负载均衡）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl port-forward --address 0.0.0.0 svc/drds-service -n drds 3306:3306</div></pre></td></tr></table></figure>
<h3 id="部署mysql-statefulset应用"><a href="#部署mysql-statefulset应用" class="headerlink" title="部署mysql statefulset应用"></a>部署mysql statefulset应用</h3><p>drds-pv-mysql-0 后面的mysql 会用来做存储，下面用到了三个mysql(需要三个pvc)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">#cat mysql-deployment.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">  - port: 3306</div><div class="line">  selector:</div><div class="line">    app: mysql</div><div class="line">  clusterIP: None</div><div class="line">---</div><div class="line">apiVersion: apps/v1 </div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: mysql</div><div class="line">  strategy:</div><div class="line">    type: Recreate</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: mysql</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - image: mysql:5.7</div><div class="line">        name: mysql</div><div class="line">        env:</div><div class="line">          # Use secret in real usage</div><div class="line">        - name: MYSQL_ROOT_PASSWORD</div><div class="line">          value: &quot;123456&quot;</div><div class="line">        ports:</div><div class="line">        - containerPort: 3306</div><div class="line">          name: mysql</div><div class="line">        volumeMounts:</div><div class="line">        - name: mysql-persistent-storage</div><div class="line">          mountPath: /var/lib/mysql</div><div class="line">      volumes:</div><div class="line">      - name: mysql-persistent-storage</div><div class="line">        persistentVolumeClaim:</div><div class="line">          claimName: pv-claim</div></pre></td></tr></table></figure>
<p>清理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl delete deployment,svc mysql</div><div class="line">kubectl delete pvc mysql-pv-claim</div><div class="line">kubectl delete pv mysql-pv-volume</div></pre></td></tr></table></figure>
<p>查看所有pod ip以及node ip：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get pods -o wide</div></pre></td></tr></table></figure>
<h2 id="配置-Pod-使用-ConfigMap"><a href="#配置-Pod-使用-ConfigMap" class="headerlink" title="配置 Pod 使用 ConfigMap"></a>配置 Pod 使用 ConfigMap</h2><p>ConfigMap 允许你将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"># cat mysql-configmap.yaml  //mysql配置文件放入： configmap</div><div class="line">apiVersion: v1</div><div class="line">kind: ConfigMap</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">  labels:</div><div class="line">    app: mysql</div><div class="line">data:</div><div class="line">  master.cnf: |</div><div class="line">    # Apply this config only on the master.</div><div class="line">    [mysqld]</div><div class="line">    log-bin</div><div class="line"></div><div class="line">  mysqld.cnf: |</div><div class="line">    [mysqld]</div><div class="line">    pid-file        = /var/run/mysqld/mysqld.pid</div><div class="line">    socket          = /var/run/mysqld/mysqld.sock</div><div class="line">    datadir         = /var/lib/mysql</div><div class="line">    #log-error      = /var/log/mysql/error.log</div><div class="line">    # By default we only accept connections from localhost</div><div class="line">    #bind-address   = 127.0.0.1</div><div class="line">    # Disabling symbolic-links is recommended to prevent assorted security risks</div><div class="line">    symbolic-links=0</div><div class="line">   sql_mode=&apos;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&apos;</div><div class="line">    # 慢查询阈值，查询时间超过阈值时写入到慢日志中</div><div class="line">    long_query_time = 2</div><div class="line">    innodb_buffer_pool_size = 257M</div><div class="line"></div><div class="line"></div><div class="line">  slave.cnf: |</div><div class="line">    # Apply this config only on slaves.</div><div class="line">    [mysqld]</div><div class="line">    super-read-only</div><div class="line"></div><div class="line">  786  26/08/20 15:27:00 kubectl create configmap game-config-env-file --from-env-file=configure-pod-container/configmap/game-env-file.properties</div><div class="line">  787  26/08/20 15:28:10 kubectl get configmap -n kube-system kubeadm-config -o yaml</div><div class="line">  788  26/08/20 15:28:11 kubectl get configmap game-config-env-file -o yaml</div></pre></td></tr></table></figure>
<p>将mysql root密码放入secret并查看 secret密码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> cat mysql-secret.yaml</span></div><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: mysql-root-password</div><div class="line">type: Opaque</div><div class="line">data:</div><div class="line">  password: MTIz</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> <span class="built_in">echo</span> -n <span class="string">'123'</span> | base64  //生成密码编码  </span></div><div class="line"><span class="meta">#</span><span class="bash"> kubectl get secret mysql-root-password -o jsonpath=<span class="string">'&#123;.data.password&#125;'</span> | base64 --decode -</span></div><div class="line"></div><div class="line">或者创建一个新的 secret：</div><div class="line">kubectl create secret generic my-secret --from-literal=password="Password"</div></pre></td></tr></table></figure>
<p>在mysql容器中使用以上configmap中的参数： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">spec:</div><div class="line">  volumes:</div><div class="line">  - name: conf</div><div class="line">    emptyDir: &#123;&#125;</div><div class="line">  - name: myconf</div><div class="line">    emptyDir: &#123;&#125;</div><div class="line">  - name: config-map</div><div class="line">    configMap:</div><div class="line">      name: mysql</div><div class="line">  initContainers:</div><div class="line">  - name: init-mysql</div><div class="line">    image: mysql:5.7</div><div class="line">    command:</div><div class="line">    - bash</div><div class="line">    - &quot;-c&quot;</div><div class="line">    - |</div><div class="line">      set -ex</div><div class="line">      # Generate mysql server-id from pod ordinal index.</div><div class="line">      [[ `hostname` =~ -([0-9]+)$ ]] || exit 1</div><div class="line">      ordinal=$&#123;BASH_REMATCH[1]&#125;</div><div class="line">      echo [mysqld] &gt; /mnt/conf.d/server-id.cnf</div><div class="line">      # Add an offset to avoid reserved server-id=0 value.</div><div class="line">      echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf</div><div class="line">      #echo &quot;innodb_buffer_pool_size=512m&quot; &gt; /mnt/rds.cnf</div><div class="line">      # Copy appropriate conf.d files from config-map to emptyDir.</div><div class="line">      #if [[ $ordinal -eq 0 ]]; then</div><div class="line">      cp /mnt/config-map/master.cnf /mnt/conf.d/</div><div class="line">      cp /mnt/config-map/mysqld.cnf /mnt/mysql.conf.d/</div><div class="line">      #else</div><div class="line">      #  cp /mnt/config-map/slave.cnf /mnt/conf.d/</div><div class="line">      #fi</div><div class="line">    volumeMounts:</div><div class="line">    - name: conf</div><div class="line">      mountPath: /mnt/conf.d</div><div class="line">    - name: myconf</div><div class="line">      mountPath: /mnt/mysql.conf.d</div><div class="line">    - name: config-map</div><div class="line">      mountPath: /mnt/config-map</div><div class="line">  containers:</div><div class="line">  - name: mysql</div><div class="line">    image: mysql:5.7</div><div class="line">    env:</div><div class="line">    #- name: MYSQL_ALLOW_EMPTY_PASSWORD</div><div class="line">    #  value: &quot;1&quot;</div><div class="line">    - name: MYSQL_ROOT_PASSWORD</div><div class="line">      valueFrom:</div><div class="line">        secretKeyRef:</div><div class="line">          name: mysql-root-password</div><div class="line">          key: password</div></pre></td></tr></table></figure>
<p><strong>通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。</strong></p>
<p>集群会自动创建一个 default-token-<em>**</em> 的secret，然后所有pod都会自动将这个 secret通过 Porjected Volume挂载到容器，也叫 ServiceAccountToken，是一种特殊的Secret</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">    Environment:    &lt;none&gt;</div><div class="line">    Mounts:</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ncgdl (ro)</div><div class="line">Conditions:</div><div class="line">  Type              Status</div><div class="line">  Initialized       True </div><div class="line">  Ready             True </div><div class="line">  ContainersReady   True </div><div class="line">  PodScheduled      True </div><div class="line">Volumes:</div><div class="line">  default-token-ncgdl:</div><div class="line">    Type:        Secret (a volume populated by a Secret)</div><div class="line">    SecretName:  default-token-ncgdl</div><div class="line">    Optional:    false</div><div class="line">QoS Class:       BestEffort</div></pre></td></tr></table></figure>
<h2 id="apply-create操作"><a href="#apply-create操作" class="headerlink" title="apply create操作"></a>apply create操作</h2><p>先 kubectl create，再 replace 的操作，我们称为命令式配置文件操作</p>
<p>kubectl apply 命令才是“声明式 API”</p>
<blockquote>
<p>kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；</p>
<p>而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。</p>
<p>kubectl set image 和 kubectl edit 也是对已有 API 对象的修改</p>
</blockquote>
<p> kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力</p>
<p>声明式 API，相当于对外界所有操作（并发接收）串行merge，才是 Kubernetes 项目编排能力“赖以生存”的核心所在</p>
<blockquote>
<p>如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。</p>
</blockquote>
<h2 id="label"><a href="#label" class="headerlink" title="label"></a>label</h2><p>给多个节点加标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl label  --overwrite=true nodes 10.0.0.172 10.0.1.192 10.0.2.48 topology.kubernetes.io/region=cn-hangzhou</div><div class="line"></div><div class="line">//查看</div><div class="line">kubectl get nodes --show-labels</div></pre></td></tr></table></figure>
<h2 id="helm"><a href="#helm" class="headerlink" title="helm"></a>helm</h2><p>Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。</p>
<p>建立local repo index：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">helm repo index [DIR] [flags]</div></pre></td></tr></table></figure>
<p>仓库只能index 到 helm package 发布后的tgz包，意义不大。每次index后需要 helm repo update</p>
<p>然后可以启动一个http服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python -m SimpleHTTPServer 8089 &amp;</div></pre></td></tr></table></figure>
<p>将local repo加入到仓库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> helm repo add local http://127.0.0.1:8089</div><div class="line"> </div><div class="line"> # helm repo list</div><div class="line">NAME 	URL                  </div><div class="line">local	http://127.0.0.1:8089</div></pre></td></tr></table></figure>
<p>install chart：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//helm3 默认不自动创建namespace，不带参数就报没有 ame 的namespace错误</div><div class="line">helm install -name wordpress -n test --create-namespace .</div><div class="line"></div><div class="line">helm list -n test</div><div class="line"></div><div class="line">&#123;&#123; .Release.Name &#125;&#125; 这种是helm内部自带的值，都是一些内建的变量，所有人都可以访问</div><div class="line"></div><div class="line">image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag | default .Chart.AppVersion &#125;&#125;&quot;  这种是我们从values.yaml文件中获取或者从命令行中获取的值。</div></pre></td></tr></table></figure>
<p>quote是一个模板方法，可以将输入的参数添加双引号</p>
<h3 id="模板片段"><a href="#模板片段" class="headerlink" title="模板片段"></a>模板片段</h3><p>之前我们看到有个文件叫做_helpers.tpl，我们介绍是说存储模板片段的地方。</p>
<p>模板片段其实也可以在文件中定义，但是为了更好管理，可以在_helpers.tpl中定义，使用时直接调用即可。</p>
<h2 id="自动补全"><a href="#自动补全" class="headerlink" title="自动补全"></a>自动补全</h2><p>kubernetes自动补全：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">source &lt;(kubectl completion bash) </div><div class="line"></div><div class="line">echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<p>helm自动补全：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd ~</div><div class="line">helm completion bash &gt; .helmrc &amp;&amp; echo &quot;source .helmrc&quot; &gt;&gt; .bashrc &amp;&amp; source .bashrc</div></pre></td></tr></table></figure>
<p>两者都需要依赖 auto-completion，所以得先：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># yum install -y bash-completion</div><div class="line"># source /usr/share/bash-completion/bash_completion</div></pre></td></tr></table></figure>
<p>kubectl -s polarx-test-ackk8s-atp-3826.adbgw.alibabacloud.test exec -it bushu016polarx282bc7216f-5161 bash</p>
<h2 id="启动时间排序"><a href="#启动时间排序" class="headerlink" title="启动时间排序"></a>启动时间排序</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">532  [2021-08-24 18:37:19] kubectl get po --sort-by=.status.startTime -ndrds</div><div class="line">533  [2021-08-24 18:37:41] kubectl get pods --sort-by=.metadata.creationTimestamp -ndrds</div></pre></td></tr></table></figure>
<h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><p>初始化集群的时候第一看kubelet能否起来（cgroup配置），第二就是看kubelet静态起pod，kubelet参数指定yaml目录，然后kubelet拉起这个目录下的所有yaml。</p>
<p>kubeadm启动集群就是如此。kubeadm生成证书、etcd.yaml等yaml、然后拉起kubelet，kubelet拉起etcd、apiserver等pod，kubeadm init 的时候主要是在轮询等待apiserver的起来。</p>
<p>可以通过kubelet –v 256来看详细日志，kubeadm本身所做的事情并不多，所以日志没有太多的信息，主要是等待轮询apiserver的拉起。</p>
<h3 id="Kubeadm-config"><a href="#Kubeadm-config" class="headerlink" title="Kubeadm config"></a>Kubeadm config</h3><p>Init 可以指定仓库以及版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubeadm init --image-repository=registry:5000/registry.aliyuncs.com/google_containers --kubernetes-version=v1.14.6  --pod-network-cidr=10.244.0.0/16</div></pre></td></tr></table></figure>
<p>查看并修改配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sudo kubeadm config view &gt; kubeadm-config.yaml</div><div class="line">edit kubeadm-config.yaml and replace k8s.gcr.io with your repo</div><div class="line">sudo kubeadm upgrade apply --config kubeadm-config.yaml</div><div class="line"></div><div class="line">kubeadm config images pull --config="/root/kubeadm-config.yaml"</div><div class="line"></div><div class="line">kubectl get cm -n kube-system kubeadm-config -o yaml</div></pre></td></tr></table></figure>
<p>pod镜像拉取不到的话可以在kebelet启动参数中写死pod镜像（pod_infra_container_image）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span></div><div class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --pod_infra_container_image=registry:5000/registry.aliyuncs.com/google_containers/pause:3.1</div></pre></td></tr></table></figure>
<h3 id="构建离线镜像库"><a href="#构建离线镜像库" class="headerlink" title="构建离线镜像库"></a>构建离线镜像库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubeadm config images list &gt;1.24.list</div><div class="line"></div><div class="line">cat 1.24.list | awk -F / &apos;&#123; print $0 &quot;    &quot; $3&#125;&apos; &gt; 1.24.aarch.list</div></pre></td></tr></table></figure>
<h3 id="cni-报x509-certificate-signed-by-unknown-authority"><a href="#cni-报x509-certificate-signed-by-unknown-authority" class="headerlink" title="cni 报x509: certificate signed by unknown authority"></a><a href="https://www.cnblogs.com/huiyichanmian/p/15760579.html" target="_blank" rel="external">cni 报x509: certificate signed by unknown authority</a></h3><p>一个集群下反复部署calico/flannel插件后，在 /etc/cni/net.d/ 下会有cni 网络配置文件残留，导致 flannel 创建容器网络的时候报证书错误。其实这不只是证书错误，还可能报其它cni配置错误，总之这是因为 10-calico.conflist 不符合 flannel要求所导致的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># find /etc/cni/net.d/</div><div class="line">/etc/cni/net.d/</div><div class="line">/etc/cni/net.d/calico-kubeconfig</div><div class="line">/etc/cni/net.d/10-calico.conflist   //默认读取了这个配置文件，不符合flannel</div><div class="line">/etc/cni/net.d/10-flannel.conflist</div></pre></td></tr></table></figure>
<p>因为calico 排在 flannel前面，所以即使用flannel配置文件也是用的 10-calico.conflist。每次 kubeadm reset 的时候是不会去做 cni 的reset 的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</div><div class="line">[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]</div><div class="line"></div><div class="line">The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d</div></pre></td></tr></table></figure>
<h2 id="kubernetes-API-案例"><a href="#kubernetes-API-案例" class="headerlink" title="kubernetes API 案例"></a><a href="https://mp.weixin.qq.com/s/1ouLZbw-Z7G-fKz53uJZag" target="_blank" rel="external">kubernetes API 案例</a></h2><p>用kubeadm部署kubernetes集群，会生成如下证书：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#ls /etc/kubernetes/pki/</div><div class="line">apiserver-etcd-client.crt  apiserver-kubelet-client.crt  apiserver.crt  ca.crt  etcd  front-proxy-ca.key      front-proxy-client.key  sa.pub</div><div class="line">apiserver-etcd-client.key  apiserver-kubelet-client.key  apiserver.key  ca.key  front-proxy-ca.crt  front-proxy-client.crt  sa.key</div></pre></td></tr></table></figure>
<p>curl访问api必须提供证书</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://ip:6443/apis/apps/v1/deployments</div></pre></td></tr></table></figure>
<p>/etc/kubernetes/pki/ca.crt —- CA机构</p>
<p>由CA机构签发：/etc/kubernetes/pki/apiserver-kubelet-client.crt </p>
<p><img src="/images/951413iMgBlog/640-5609125.jpeg" alt="Image"></p>
<p><a href="https://kubernetes.io/docs/reference/using-api/api-concepts/" target="_blank" rel="external">获取default namespace下的deployment</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># JWT_TOKEN_DEFAULT_DEFAULT=$(kubectl get secrets \</div><div class="line">    $(kubectl get serviceaccounts/default -o jsonpath=&apos;&#123;.secrets[0].name&#125;&apos;) \</div><div class="line">    -o jsonpath=&apos;&#123;.data.token&#125;&apos; | base64 --decode)</div><div class="line"></div><div class="line">#curl --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/apis/apps/v1/namespaces/default/deployments --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;</div><div class="line">&#123;</div><div class="line">  &quot;kind&quot;: &quot;DeploymentList&quot;,</div><div class="line">  &quot;apiVersion&quot;: &quot;apps/v1&quot;,</div><div class="line">  &quot;metadata&quot;: &#123;</div><div class="line">    &quot;resourceVersion&quot;: &quot;1233307&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;items&quot;: [</div><div class="line">    &#123;</div><div class="line">      &quot;metadata&quot;: &#123;</div><div class="line">        &quot;name&quot;: &quot;nginx-deployment&quot;,</div><div class="line"> </div><div class="line">//列出default namespace下所有的pod </div><div class="line">#curl  --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/api/v1/namespaces/default/pods --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;       </div><div class="line"></div><div class="line">//对应的kubectl生成的curl命令</div><div class="line">curl  --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -v -XGET  -H &quot;Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json&quot; -H &quot;User-Agent: kubectl/v1.23.3 (linux/arm64) kubernetes/816c97a&quot; &apos;https://11.158.239.200:6443/api/v1/namespaces/default/pods?limit=500&apos;</div></pre></td></tr></table></figure>
<p>对应地可以通过 kubectl -v 256 get pods 来看kubectl的处理过程，以及具体访问的api、参数、返回结果等。实际kubectl最终也是通过libcurl来访问的这些api。这样也不用对api-server抓包分析了。</p>
<p>或者将kube api-server 代理成普通http服务</p>
<blockquote>
<p><em># Make Kubernetes API available on localhost:8080</em><br><em># to bypass the auth step in subsequent queries:</em><br>$ kubectl proxy –port=8080 </p>
<p>然后</p>
<p>curl <a href="http://localhost:8080/api/v1/namespaces" target="_blank" rel="external">http://localhost:8080/api/v1/namespaces</a></p>
</blockquote>
<p><img src="/images/951413iMgBlog/640-5609622.png" alt="Image"></p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>用curl调用kubernetes api-server来调试，需要抓包，先在执行curl的服务器上配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export SSLKEYLOGFILE=/root/ssllog/apiserver-ssl.log</div></pre></td></tr></table></figure>
<p>然后执行tcpdump对api-server的6443端口抓包，然后将/root/ssllog/apiserver-ssl.log和抓包文件下载到本地，wireshark打开抓包文件，同时配置tls。</p>
<p>以下是个完整case（技巧指定curl的本地端口为12345，然后tcpdump只抓12345，所得的请求、response结果都会解密–如果抓api-server的6443则只能看到请求被解密）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curl --local-port 12345 --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/apis/apps/v1/namespaces/default/deployments --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;</div><div class="line"></div><div class="line">#cat $JWT_TOKEN_DEFAULT_DEFAULT eyJhbGciOiJSUzI1NiIsImtpZCI6ImlNVVFVNmxUM2t4c3Y2Q3IyT1BzV2hDZGRVSmVxTHc5RV8wUXZ4RVM5REEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJ: File name too long</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20220223170008311.png" alt="image-20220223170008311"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/" target="_blank" rel="external">https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/04/获取一直FullGC下的java进程HeapDump的小技巧/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/获取一直FullGC下的java进程HeapDump的小技巧/" itemprop="url">获取一直FullGC下的java进程HeapDump的小技巧</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T17:30:03+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="获取一直FullGC下的java进程HeapDump的小技巧"><a href="#获取一直FullGC下的java进程HeapDump的小技巧" class="headerlink" title="获取一直FullGC下的java进程HeapDump的小技巧"></a>获取一直FullGC下的java进程HeapDump的小技巧</h1><p>就是小技巧，操作步骤需要查询，随手记录</p>
<ul>
<li>找到java进程，gdb attach上去， 例如 <code>gdb -p 12345</code></li>
<li>找到这个<code>HeapDumpBeforeFullGC</code>的地址（这个flag如果为true，会在FullGC之前做HeapDump，默认是false）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) p &amp;HeapDumpBeforeFullGC</div><div class="line">$2 = (&lt;data variable, no debug info&gt; *) 0x7f7d50fc660f &lt;HeapDumpBeforeFullGC&gt;</div></pre></td></tr></table></figure>
<ul>
<li>Copy 地址：0x7f7d50fc660f</li>
<li>然后把他设置为true，这样下次FGC之前就会生成一份dump文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) set *0x7f7d50fc660f = 1</div><div class="line">(gdb) quit</div></pre></td></tr></table></figure>
<ul>
<li>最后，等一会，等下次FullGC触发，你就有HeapDump了！<br>(如果没有指定heapdump的名字，默认是 java_pidxxx.hprof)</li>
</ul>
<p>(PS. <code>jstat -gcutil pid</code> 可以查看gc的概况)</p>
<p>(操作完成后记得gdb上去再设置回去，不然可能一直fullgc，导致把磁盘打满).</p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>在jvm还有响应的时候可以： jinfo -flag +HeapDumpBeforeFullGC pid 设置HeapDumpBeforeFullGC 为true（- 为false，+-都不要为只打印值）</p>
<p>kill -3 产生coredump  存放在 kernel.core_pattern=/root/core （/etc/sysctl.conf)</p>
<p>得到core文件后，采用 gdb -c 执行文件 core文件 进入调试模式，对于java，有以下2个技巧：</p>
<p>进入gdb调试模式后，输入如下命令： info threads，观察异常的线程，定位到异常的线程后，则可以输入如下命令：thread 线程编号，则会打印出当前java代码的工作流程。</p>
<p> 而对于这个core，亦可以用jstack jmap打印出堆信息，线程信息，具体命令：</p>
<p>  jmap -heap 执行文件 core文件   jstack -F -l 执行文件 core文件</p>
<p><strong>容器中的进程的话需要到宿主机操作，并且将容器中的 jdk文件夹复制到宿主机对应的位置。</strong></p>
<p>  <strong>ps auxff |grep 容器id -A10 找到JVM在宿主机上的进程id</strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/02/Linux 问题总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/02/Linux 问题总结/" itemprop="url">Linux 问题总结</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-02T17:30:03+08:00">
                2020-01-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-问题总结"><a href="#Linux-问题总结" class="headerlink" title="Linux 问题总结"></a>Linux 问题总结</h1><h2 id="crond文件权限的坑"><a href="#crond文件权限的坑" class="headerlink" title="crond文件权限的坑"></a>crond文件权限的坑</h2><p>crond第一次加载的时候（刚启动）会去检查文件属性，不是644的话以后都不会执行了，即使后面chmod改成了644. </p>
<p>手工随便修改一下该文件的内容就能触发自动执行了，或者重启crond, 或者 sudo service crond reload， 或者 /etc/cron.d/下有任何修改都会触发crond reload配置(包含 touch )。</p>
<p>总之 crond会每分钟去检查job有没有change，有的话才触发reload，这个change看的时候change time有没有变化，不看权限的变化，仅仅是权限的变化不会触发crond reload。</p>
<p> crond会每分钟去检查一下job有没有修改，有修改的话会reload，但是这个<strong>修改不包含权限的修改</strong>。可以简单地理解这个修改是指文件的change time。</p>
<h2 id="cgroup目录报No-space-left-on-device"><a href="#cgroup目录报No-space-left-on-device" class="headerlink" title="cgroup目录报No space left on device"></a><a href="https://rotadev.com/cgroup-no-space-left-on-device-server-fault/" target="_blank" rel="external">cgroup目录报No space left on device</a></h2><p>可能是因为某个规则下的 cpuset.cpus 文件是空导致的</p>
<h2 id="容器中root用户执行-su-admin-切换失败"><a href="#容器中root用户执行-su-admin-切换失败" class="headerlink" title="容器中root用户执行 su - admin 切换失败"></a>容器中root用户执行 su - admin 切换失败</h2><p>问题原因：<a href="https://access.redhat.com/solutions/30316" target="_blank" rel="external">https://access.redhat.com/solutions/30316</a></p>
<p><img src="/images/oss/63a4ac6669f820156bff035e7dc49ac2.png" alt="image.png"></p>
<p>如上图去掉 admin nproc限制就可以了</p>
<p>这是因为root用户的nproc是unlimited，但是admin的是65535，所以切不过去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@i22h08323 /home/admin]</div><div class="line">#ulimit -u</div><div class="line">unlimited</div></pre></td></tr></table></figure>
<h2 id="容器中ulimit限制了sudo的执行"><a href="#容器中ulimit限制了sudo的执行" class="headerlink" title="容器中ulimit限制了sudo的执行"></a>容器中ulimit限制了sudo的执行</h2><p>容器启动的时候默认nofile为65535（可以通过 docker run –ulimit nofile=655360 来设置），如果容器中的 /etc/security/limits.conf 中设置的nofile大于 65535就会报错，因为容器的1号进程就是65535了，比如在容器中用root用户执行sudo ls报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#sudo ls</div><div class="line">sudo: pam_open_session: Permission denied</div><div class="line">sudo: policy plugin failed session initialization</div></pre></td></tr></table></figure>
<p>可以修改容器中的 ulimit 不要超过默认的65535或者修改容器的启动参数来解决。</p>
<p>子进程都会继承父进程的一些环境变量，比如 limits.conf, sudo/su/crond/passwd等都会触发重新加载limits, </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep -rin pam_limit /etc/pam.d //可以看到触发重新加载的场景</div></pre></td></tr></table></figure>
<h2 id="systemd-limits"><a href="#systemd-limits" class="headerlink" title="systemd limits"></a>systemd limits</h2><p>/etc/security/limits.conf 的配置，只适用于通过PAM 认证登录用户的资源限制，它对systemd 的service 的资源限制不生效。</p>
<p>因此登录用户的限制，通过/etc/security/limits.conf 与/etc/security/limits.d 下的文件设置即可。</p>
<p>对于systemd service 的资源设置，则需修改全局配置，全局配置文件放在/etc/systemd/system.conf 和/etc/systemd/user.conf，同时也会加载两个对应目录中的所有.conf 文件/etc/systemd/system.conf.d/.conf 和/etc/systemd/user.conf.d/.conf。</p>
<h3 id="关于ulimit的一些知识点"><a href="#关于ulimit的一些知识点" class="headerlink" title="关于ulimit的一些知识点"></a>关于ulimit的一些知识点</h3><p>参考 <a href="https://feichashao.com/ulimit_demo/" target="_blank" rel="external">Ulimit</a> <a href="http://blog.yufeng.info/archives/2568" target="_blank" rel="external">http://blog.yufeng.info/archives/2568</a></p>
<ul>
<li>limit的设定值是 per-process 的</li>
<li>在 Linux 中，每个普通进程可以调用 getrlimit() 来查看自己的 limits，也可以调用 setrlimit() 来改变自身的 soft limits</li>
<li>要改变 hard limit, 则需要进程有 CAP_SYS_RESOURCE 权限</li>
<li>进程 fork() 出来的子进程，会继承父进程的 limits 设定</li>
<li><code>ulimit</code> 是 shell 的内置命令。在执行<code>ulimit</code>命令时，其实是 shell 自身调用 getrlimit()/setrlimit() 来获取/改变自身的 limits. 当我们在 shell 中执行应用程序时，相应的进程就会继承当前 shell 的 limits 设定</li>
<li>shell 的初始 limits 通常是 pam_limits 设定的。顾名思义，pam_limits 是一个 PAM 模块，用户登录后，pam_limits 会给用户的 shell 设定在 limits.conf 定义的值</li>
</ul>
<p>ulimit, limits.conf 和 pam_limits模块 的关系，大致是这样的：</p>
<ol>
<li>用户进行登录，触发 pam_limits;</li>
<li>pam_limits 读取 limits.conf，相应地设定用户所获得的 shell 的 limits；</li>
<li>用户在 shell 中，可以通过 ulimit 命令，查看或者修改当前 shell 的 limits;</li>
<li>当用户在 shell 中执行程序时，该程序进程会继承 shell 的 limits 值。于是，limits 在进程中生效了</li>
</ol>
<p>判断要分配的句柄号是不是超过了 limits.conf 中 nofile 的限制。fd 是当前进程相关的，是一个从 0 开始的整数<br>结论1：soft nofile 和 fs.nr_open的作用一样，它两都是限制的单个进程的最大文件数量。区别是 soft nofile 可以按用户来配置，而 fs.nr_open 所有用户只能配一个。注意 hard nofile 一定要比 fs.nr_open 要小，否则可能导致用户无法登陆。<br>结论2：fs.file-max: 整个系统上可打开的最大文件数，但不限制 root 用户</p>
<h2 id="pam-权限报错"><a href="#pam-权限报错" class="headerlink" title="pam 权限报错"></a>pam 权限报错</h2><p><img src="/images/oss/b646979272e71e015de4a47c62b89747.png" alt="image.png"></p>
<p>从debug信息看如果是pam权限报错的话，需要将 required 改成 sufficient</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$cat /etc/pam.d/crond </div><div class="line">#</div><div class="line"># The PAM configuration file for the cron daemon</div><div class="line">#</div><div class="line">#</div><div class="line"># No PAM authentication called, auth modules not needed</div><div class="line">account    required   pam_access.so</div><div class="line">account    include    system-auth</div><div class="line">session    required   pam_loginuid.so //required 改成 sufficient</div><div class="line">session    include    system-auth</div><div class="line">auth       include    system-auth</div></pre></td></tr></table></figure>
<p>PAM 提供四个安全领域的特性，但是应用程序不太可能同时需要所有这些方面。例如，<code>passwd</code> 命令只需要下面列表中的第三组：</p>
<ul>
<li><code>account</code> 处理账户限制。对于有效的用户，允许他做什么？</li>
<li><code>auth</code> 处理用户识别 — 例如，通过输入用户名和密码。</li>
<li><code>password</code> 只处理与密码相关的问题，比如设置新密码。</li>
<li><code>session</code> 处理连接管理，包括日志记录。</li>
</ul>
<p>在 /etc/pam.d 目录中为将使用 PAM 的每个应用程序创建一个配置文件，文件名与应用程序名相同。例如，<code>login</code> 命令的配置文件是 /etc/pam.d/login。</p>
<p>必须定义将应用哪些模块，创建一个动作 “堆”。PAM 运行堆中的所有模块，根据它们的结果允许或拒绝用户的请求。还必须定义检查是否是必需的。最后，<em>other</em> 文件为没有特殊规则的所有应用程序提供默认规则。</p>
<ul>
<li><code>optional</code> 模块可以成功，也可以失败；PAM 根据模块是否最终成功返回 <code>success</code> 或 <code>failure</code>。</li>
<li><code>required</code> 模块必须成功。如果失败，PAM 返回 <code>failure</code>，但是会在运行堆中的其他模块之后返回。</li>
<li><code>requisite</code> 模块也必须成功。但是，如果失败，PAM 立即返回 <code>failure</code>，不再运行其他模块。</li>
<li><code>sufficient</code> 模块在成功时导致 PAM 立即返回 <code>success</code>，不再运行其他模块。</li>
</ul>
<p>当pam安装之后有两大部分：在/lib64/security目录下的各种pam模块以及/etc/pam.d和/etc/pam.d目录下的针对各种服务和应用已经定义好的pam配置文件。当某一个有认证需求的应用程序需要验证的时候，一般在应用程序中就会定义负责对其认证的PAM配置文件。以vsftpd为例，在它的配置文件/etc/vsftpd/vsftpd.conf中就有这样一行定义：</p>
<blockquote>
<p>pam_service_name=vsftpd</p>
</blockquote>
<p>表示登录FTP服务器的时候进行认证是根据/etc/pam.d/vsftpd文件定义的内容进行。</p>
<h3 id="PAM-认证过程"><a href="#PAM-认证过程" class="headerlink" title="PAM 认证过程"></a>PAM 认证过程</h3><p>当程序需要认证的时候已经找到相关的pam配置文件，认证过程是如何进行的？下面我们将通过解读/etc/pam.d/system-auth文件予以说明。</p>
<p>首先要声明一点的是：system-auth是一个非常重要的pam配置文件，主要负责用户登录系统的认证工作。而且该文件不仅仅只是负责用户登录系统认证，其它的程序和服务通过include接口也可以调用到它，从而节省了很多重新自定义配置的工作。所以应该说该文件是系统安全的总开关和核心的pam配置文件。</p>
<p>下面是/etc/pam.d/system-auth文件的全部内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">$cat /etc/pam.d/system-auth</div><div class="line">#%PAM-1.0</div><div class="line"># This file is auto-generated.</div><div class="line"># User changes will be destroyed the next time authconfig is run.</div><div class="line">auth        required      pam_env.so</div><div class="line">auth        required      pam_faildelay.so delay=2000000</div><div class="line">auth        sufficient    pam_unix.so nullok try_first_pass</div><div class="line">auth        requisite     pam_succeed_if.so uid &gt;= 1000 quiet_success</div><div class="line">auth        required      pam_deny.so</div><div class="line"></div><div class="line">account     required      pam_unix.so</div><div class="line">account     sufficient    pam_localuser.so</div><div class="line">account     sufficient    pam_succeed_if.so uid &lt; 1000 quiet</div><div class="line">account     required      pam_permit.so</div><div class="line"></div><div class="line">password    requisite     pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type=</div><div class="line">password    sufficient    pam_unix.so sha512 shadow nullok try_first_pass use_authtok</div><div class="line">password    required      pam_deny.so</div><div class="line"></div><div class="line">session     optional      pam_keyinit.so revoke</div><div class="line">session     required      pam_limits.so</div><div class="line">-session     optional      pam_systemd.so</div><div class="line">session     [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid</div><div class="line">session     required      pam_unix.so</div></pre></td></tr></table></figure>
<h4 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h4><p>当用户登录的时候，首先会通过auth类接口对用户身份进行识别和密码认证。所以在该过程中验证会经过几个带auth的配置项。</p>
<p>其中的第一步是通过pam_env.so模块来定义用户登录之后的环境变量， pam_env.so允许设置和更改用户登录时候的环境变量，默认情况下，若没有特别指定配置文件，将依据/etc/security/pam_env.conf进行用户登录之后环境变量的设置。</p>
<p>然后通过pam_unix.so模块来提示用户输入密码，并将用户密码与/etc/shadow中记录的密码信息进行对比，如果密码比对结果正确则允许用户登录，而且<strong>该配置项的使用的是“sufficient”控制位，即表示只要该配置项的验证通过，用户即可完全通过认证而不用再去走下面的认证项</strong>。不过在特殊情况下，用户允许使用空密码登录系统，例如当将某个用户在/etc/shadow中的密码字段删除之后，该用户可以只输入用户名直接登录系统。</p>
<p>下面的配置项中，通过pam_succeed_if.so对用户的登录条件做一些限制，表示允许uid大于500的用户在通过密码验证的情况下登录，在Linux系统中，一般系统用户的uid都在500之内，所以该项即表示允许使用useradd命令以及默认选项建立的普通用户直接由本地控制台登录系统。</p>
<p>最后通过pam_deny.so模块对所有不满足上述任意条件的登录请求直接拒绝，pam_deny.so是一个特殊的模块，该模块返回值永远为否，类似于大多数安全机制的配置准则，在所有认证规则走完之后，对不匹配任何规则的请求直接拒绝。</p>
<h4 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h4><p>三个配置项主要表示通过account账户类接口来识别账户的合法性以及登录权限。</p>
<p>第一行仍然使用pam_unix.so模块来声明用户需要通过密码认证。第二行承认了系统中uid小于500的系统用户的合法性。之后对所有类型的用户登录请求都开放控制台。</p>
<h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a>第三部分</h4><p>会通过password口令类接口来确认用户使用的密码或者口令的合法性。第一行配置项表示需要的情况下将调用pam_cracklib来验证用户密码复杂度。如果用户输入密码不满足复杂度要求或者密码错，最多将在三次这种错误之后直接返回密码错误的提示，否则期间任何一次正确的密码验证都允许登录。需要指出的是，pam_cracklib.so是一个常用的控制密码复杂度的pam模块，关于其用法举例我们会在之后详细介绍。之后带pam_unix.so和pam_deny.so的两行配置项的意思与之前类似。都表示需要通过密码认证并对不符合上述任何配置项要求的登录请求直接予以拒绝。不过用户如果执行的操作是单纯的登录，则这部分配置是不起作用的。</p>
<h4 id="第四部分"><a href="#第四部分" class="headerlink" title="第四部分"></a>第四部分</h4><p>主要将通过session会话类接口为用户初始化会话连接。其中几个比较重要的地方包括，使用pam_keyinit.so表示当用户登录的时候为其建立相应的密钥环，并在用户登出的时候予以撤销。不过该行配置的控制位使用的是optional，表示这并非必要条件。之后通过pam_limits.so限制用户登录时的会话连接资源，相关pam_limit.so配置文件是/etc/security/limits.conf，默认情况下对每个登录用户都没有限制。关于该模块的配置方法在后面也会详细介绍。</p>
<h3 id="常用的PAM模块介绍"><a href="#常用的PAM模块介绍" class="headerlink" title="常用的PAM模块介绍"></a>常用的PAM模块介绍</h3><table>
<thead>
<tr>
<th>PAM模块</th>
<th>结合管理类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>pam_unix.so</td>
<td>auth</td>
<td>提示用户输入密码,并与/etc/shadow文件相比对.匹配返回0</td>
</tr>
<tr>
<td>pam_unix.so</td>
<td>account</td>
<td>检查用户的账号信息(包括是否过期等).帐号可用时,返回0.</td>
</tr>
<tr>
<td>pam_unix.so</td>
<td>password</td>
<td>修改用户的密码. 将用户输入的密码,作为用户的新密码更新shadow文件</td>
</tr>
<tr>
<td>pam_shells.so</td>
<td>auth、account</td>
<td>如果用户想登录系统，那么它的shell必须是在/etc/shells文件中之一的shell</td>
</tr>
<tr>
<td>pam_deny.so</td>
<td>account、auth、password、session</td>
<td>该模块可用于拒绝访问</td>
</tr>
<tr>
<td>pam_permit.so</td>
<td>account、auth、password、session</td>
<td>模块任何时候都返回成功.</td>
</tr>
<tr>
<td>pam_securetty.so</td>
<td>auth</td>
<td>如果用户要以root登录时,则登录的tty必须在/etc/securetty之中.</td>
</tr>
<tr>
<td>pam_listfile.so</td>
<td>account、auth、password、session</td>
<td>访问应用程的控制开关</td>
</tr>
<tr>
<td>pam_cracklib.so</td>
<td>password</td>
<td>这个模块可以插入到一个程序的密码栈中,用于检查密码的强度.</td>
</tr>
<tr>
<td>pam_limits.so</td>
<td>session</td>
<td>定义使用系统资源的上限，root用户也会受此限制，可以通过/etc/security/limits.conf或/etc/security/limits.d/*.conf来设定</td>
</tr>
</tbody>
</table>
<h2 id="debug-crond"><a href="#debug-crond" class="headerlink" title="debug crond"></a>debug crond</h2><p>先停掉 crond service，然后开启debug参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl stop crond</div><div class="line">crond -x proc //不想真正执行的话：test</div></pre></td></tr></table></figure>
<p>或者增加更多的debug信息， debug sudo/sudoers , 在 /etc/sudo.conf 中增加了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Debug sudo /var/log/sudo_debug all@warn</div><div class="line">Debug sudoers.so /var/log/sudoers_debug all@debug</div></pre></td></tr></table></figure>
<h2 id="crond-ERROR-getpwnam-failed"><a href="#crond-ERROR-getpwnam-failed" class="headerlink" title="crond ERROR (getpwnam() failed)"></a>crond ERROR (getpwnam() failed)</h2><p><a href="https://www.ibm.com/support/pages/cron-job-fails-error-message-getpwnam-failed-no-such-file-or-directory" target="_blank" rel="external">报错信息</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">crond[246590]: (/usr/bin/ssh) ERROR (getpwnam() failed)</div></pre></td></tr></table></figure>
<p>要特别注意crond格式是 时间  <strong>用户</strong>  命令</p>
<p>有时候我们可以省略用户，但是在 <strong>/etc/cron.d/</strong> 中省略用户后报错如上</p>
<h2 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h2><p>把进程看做是资源分配的单位，把线程才看成一个具体的执行实体。</p>
<h2 id="deleted-文件"><a href="#deleted-文件" class="headerlink" title="deleted 文件"></a>deleted 文件</h2><p><code>lsof +L1</code> 或者<code>lsof | grep delete</code> 发现有被删除的文件，且占用大量磁盘空间</p>
<h2 id="No-route-to-host"><a href="#No-route-to-host" class="headerlink" title="No route to host"></a>No route to host</h2><p>如果ping ip能通,但是curl/telnet 访问 ip+port 报not route to host 错误,这肯定不是route问题(因为ping能通), 一般都是目标机器防火墙的问题</p>
<p>可以停掉防火墙验证,或者添加端口到防火墙:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#firewall-cmd --permanent --add-port=8090/tcp</div><div class="line">success</div><div class="line">#firewall-cmd --reload</div></pre></td></tr></table></figure>
<h2 id="强制重启系统"><a href="#强制重启系统" class="headerlink" title="强制重启系统"></a>强制重启系统</h2><p><img src="/images/oss/ee2e438907fa72c70d5393a651dc9113.png" alt="image.png"></p>
<h2 id="hostname"><a href="#hostname" class="headerlink" title="hostname"></a>hostname</h2><p>hostname -i 是根据机器的hostname去解析ip，如果 /etc/hosts里面没有指定hostname对应的ip就会走dns 流程然后libnss_myhostname 返回所有ip</p>
<p>getHostName获取的机器名如果对应的ip不是127.0.0.1，那么就用这个ip，否则就需要通过getHostByName获取所有网卡选择一个</p>
<h2 id="tsar-Floating-point-execption"><a href="#tsar-Floating-point-execption" class="headerlink" title="tsar Floating point execption"></a>tsar Floating point execption</h2><p><img src="/images/oss/72197d600425656ec9a8ed18bcc5853b.png" alt="image.png"></p>
<p>因为 /etc/localtime 是deleted状态</p>
<h2 id="奇怪的文件大小-sparse-file"><a href="#奇怪的文件大小-sparse-file" class="headerlink" title="奇怪的文件大小 sparse file"></a>奇怪的文件大小 <a href="https://unix.stackexchange.com/questions/259932/strange-discrepancy-of-file-sizes-from-ls" target="_blank" rel="external">sparse file</a></h2><p><img src="/images/oss/720f618d-2911-4bfd-a63e-33399532b6e5.png" alt="img"></p>
<p>如上图 gc.log 实际为5.6M，但是通过 ls -lh 就变成74G了，但实际上总文件夹才63M。因为写文件的时候lseek了74G的地方写入5.6M的内容就看到是这个样子了，而前面lseek的74G是不需要从磁盘上分配出来的.</p>
<p><a href="https://www.lisenet.com/2014/so-what-is-the-size-of-that-file/" target="_blank" rel="external">而 ls -s 中的 -s就是只看实际大小</a></p>
<p><img src="/images/oss/19b5f6cc-6fc4-4ad6-854c-6164705d343a.png" alt="img"></p>
<p><a href="https://www.systutorials.com/handling-sparse-files-on-linux/" target="_blank" rel="external">图片来源</a></p>
<p><a href="https://www.mankier.com/1/fallocate" target="_blank" rel="external">回收文件中的空洞</a>：sudo fallocate -c –length 70G gc.log</p>
<p>如果文件一直打开写入中是没法回收的，因为一回收又被重新lseek到之前的末尾重新写入了！</p>
<h2 id="增加dmesg-buffer"><a href="#增加dmesg-buffer" class="headerlink" title="增加dmesg buffer"></a>增加dmesg buffer</h2><p>If dmesg does not show any information about NUMA, then increase the Ring Buffer size:<br>Boot with ‘log_buf_len=16M’ (or some other big value). Refer the following kbase article <a href="https://access.redhat.com/solutions/47276" target="_blank" rel="external">How do I increase the kernel log ring buffer size?</a> for steps on how to increase the ring buffer</p>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a href="https://access.redhat.com/solutions/641093" target="_blank" rel="external">Yum commands error “pycurl.so: undefined symbol”</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># yum check update</div><div class="line">There was a problem importing one of the Python modules</div><div class="line">required to run yum. The error leading to this problem was:</div><div class="line"></div><div class="line">/usr/lib64/python2.6/site-packages/pycurl.so: undefined symbol: CRYPTO_set_locking_callback</div><div class="line"></div><div class="line">Please install a package which provides this module, or</div><div class="line">verify that the module is installed correctly.</div><div class="line"></div><div class="line">It&apos;s possible that the above module doesn&apos;t match the</div><div class="line">current version of Python, which is:</div><div class="line">2.6.6 (r266:84292, Sep  4 2013, 07:46:00)</div><div class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)]</div><div class="line"></div><div class="line">If you cannot solve this problem yourself, please go to</div><div class="line">the yum faq at:</div><div class="line">http://yum.baseurl.org/wiki/Faq</div></pre></td></tr></table></figure>
<ul>
<li>Check and fix the related library paths or remove 3rd party libraries, usually <code>libcurl</code> or <code>libssh2</code>. On a x86_64 system, the standard paths for those libraries are <code>/usr/lib64/libcurl.so.4</code> and <code>/usr/lib64/libssh2.so.1</code></li>
</ul>
<h2 id="软中断、系统调用和上下文切换"><a href="#软中断、系统调用和上下文切换" class="headerlink" title="软中断、系统调用和上下文切换"></a>软中断、系统调用和上下文切换</h2><p>“你可以把内核看做是不断对请求进行响应的服务器，这些请求可能来自在CPU上执行的进程，也可能来自发出中断的外部设备。老板的请求相当于中断，而顾客的请求相当于用户态进程发出的系统调用”。</p>
<p>软中断和系统调用一样，都是CPU停止掉当前用户态上下文，保存工作现场，然后陷入到内核态继续工作。二者的唯一区别是系统调用是切换到同进程的内核态上下文，而软中断是则是切换到了另外一个内核进程ksoftirqd上。</p>
<blockquote>
<p>系统调用开销是200ns起步</p>
<p>从实验数据来看，一次软中断CPU开销大约3.4us左右</p>
<p>实验结果显示进程上下文切换平均耗时 3.5us，lmbench工具显示的进程上下文切换耗时从2.7us到5.48之间</p>
<p>大约每次线程切换开销大约是3.8us左右。<strong>从上下文切换的耗时上来看，Linux线程（轻量级进程）其实和进程差别不太大</strong>。</p>
</blockquote>
<p>软中断和进程上下文切换比较起来，进程上下文切换是从用户进程A切换到了用户进程B。而软中断切换是从用户进程A切换到了内核线程ksoftirqd上。而ksoftirqd作为一个内核控制路径，其处理程序比一个用户进程要轻量，所以上下文切换开销相对比进程切换要少一些（实际数据基本差不多）。</p>
<p>系统调用只是在进程内将用户态切换到内核态，然后再切回来，而上下文切换可是直接从进程A切换到了进程B。显然这个上下文切换需要完成的工作量更大。</p>
<h3 id="软中断开销计算"><a href="#软中断开销计算" class="headerlink" title="软中断开销计算"></a><a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247483827&amp;idx=3&amp;sn=8b897c8d6d3038ea79bd156a0e88db10&amp;scene=21#wechat_redirect" target="_blank" rel="external">软中断开销计算</a></h3><ul>
<li><strong>查看软中断总耗时</strong>， 首先用top命令可以看出每个核上软中断的开销占比，是在si列（1.2%–1秒[1000ms]中的1.2%）</li>
<li><strong>查看软中断次数</strong>，再用vmstat命令可以看到软中断的次数（in列 56000）</li>
<li><strong>计算每次软中断的耗时</strong>，该机器是16核的物理实机，故可以得出每个软中断需要的CPU时间是=12ms/(56000/16)次=3.428us。从实验数据来看，一次软中断CPU开销大约3.4us左右</li>
</ul>
<h2 id="Linux-启动进入紧急模式"><a href="#Linux-启动进入紧急模式" class="headerlink" title="Linux 启动进入紧急模式"></a>Linux 启动进入紧急模式</h2><p>可能是因为磁盘挂载不上，检查 /etc/fstab 中需要挂载的磁盘，尝试 mount -a 是否能全部挂载，麒麟下容易出现弄丢磁盘的标签和uuid</p>
<p>否则的话debug为啥，比如检查设备标签（e2label）是否冲突之类的</p>
<h2 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h2><p><a href="https://zhuanlan.zhihu.com/p/401910162" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/401910162</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">PROCESS STATE CODES</div><div class="line">  Here are the different values that the s, stat and state output specifiers(header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of a process:</div><div class="line"> </div><div class="line">    D    uninterruptible sleep (usually IO)  #不可中断睡眠 不接受任何信号，因此kill对它无效，一般是磁盘io,网络io读写时出现</div><div class="line">    R    running or runnable (on run queue)  #可运行状态或者运行中，可运行状态表明进程所需要的资源准备就绪，待内核调度</div><div class="line">    S    interruptible sleep (waiting for an event to complete) #可中断睡眠，等待某事件到来而进入睡眠状态</div><div class="line">    T    stopped by job control signal #进程暂停状态 平常按下的ctrl+z,实际上是给进程发了SIGTSTP 信号 （kill -l可查看系统所有的信号量）</div><div class="line">    t    stopped by debugger during the tracing #进程被ltrace、strace attach后就是这种状态</div><div class="line">    W    paging (not valid since the 2.6.xx kernel) #没有用了</div><div class="line">    X    dead (should never be seen) #进程退出时的状态</div><div class="line">    Z    defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent #进程退出后父进程没有正常回收，俗称僵尸进程</div></pre></td></tr></table></figure>
<h3 id="D状态的进程"><a href="#D状态的进程" class="headerlink" title="D状态的进程"></a><a href="https://gohalo.me/post/linux-kernel-hang-task-panic-introduce.html" target="_blank" rel="external">D状态的进程</a></h3><p>D： Disk sleep（task_uninterruptible)–比如，磁盘满，导致进程D，无法kill</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">echo 1 &gt;  /proc/sys/kernel/hung_task_panic  </div><div class="line"></div><div class="line">----- 处于D状态的超时时间，默认是120s</div><div class="line">$ cat /proc/sys/kernel/hung_task_timeout_secs</div><div class="line"></div><div class="line">----- 发现hung task之后是否触发panic操作</div><div class="line">$ cat /proc/sys/kernel/hung_task_panic</div><div class="line"></div><div class="line">----- 每次检查的进程数</div><div class="line">$ cat /proc/sys/kernel/hung_task_check_count</div><div class="line"></div><div class="line">----- 为了防止日志被刷爆，设置最多的打印次数</div><div class="line">$ cat /proc/sys/kernel/hung_task_warnings</div></pre></td></tr></table></figure>
<p>这个参数可以用来处理 D 状态进程 </p>
<p>内核在 3.10.0 版本之后提供了 hung task 机制，用来检测系统中长期处于 D 状体的进程，如果存在，则打印相关警告和进程堆栈。</p>
<p>如果配置了 <code>hung_task_panic</code> ，则会直接发起 panic 操作，然后结合 kdump 可以搜集到相关的 vmcore 文件，用于定位分析。</p>
<p>其基本原理也很简单，系统启动时会创建一个内核线程 <code>khungtaskd</code>，定期遍历系统中的所有进程，检查是否存在处于 D 状态且超过 120s 的进程，如果存在，则打印相关警告和进程堆栈，并根据参数配置决定是否发起 panic 操作。</p>
<h3 id="T-状态进程"><a href="#T-状态进程" class="headerlink" title="T 状态进程"></a>T 状态进程</h3><p>kill -CONT pid 来恢复</p>
<p>jmap -heap/histo和大家使用-F参数是一样的，底层都是通过serviceability agent来实现的，并不是jvm attach的方式，通过sa连上去之后会挂起进程，在serviceability agent里存在bug可能<strong>导致detach的动作不会被执行</strong>，从而会让进程一直挂着，可以通过top命令验证进程是否处于T状态，如果是说明进程被挂起了，如果进程被挂起了，可以通过kill -CONT [pid]来恢复。</p>
<h2 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h2><p>『你所规划的路由必须要是你的网卡 (如 eth0) 或 IP 可以直接沟通 (broadcast) 的情况』才行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">$sudo route add -net 11.164.191.0  gw 11.164.191.247 netmask 255.255.255.0 bond0</div><div class="line">SIOCDELRT: No such process // 从bond0没法广播到 11.164.191.247</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  gw 100.81.183.247 netmask 255.255.255.0 bond0.700</div><div class="line">SIOCADDRT: Network is unreachable //从bond0.700 没法广播到 100.81.183.247，实际目前从bond0.700没法广播到任何地方</div><div class="line"></div><div class="line">$sudo route add** **11.164.191.247** **dev** **bond0.700</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  **gw 100.81.183.247** netmask 255.255.255.0 bond0.700</div><div class="line">SIOCADDRT: Network is unreachable  //从bond0.700 没法广播到 100.81.183.247</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  gw 11.164.191.247 netmask 255.255.255.0 bond0</div><div class="line">SIOCADDRT: Network is unreachable//从bond0没法广播到 11.164.191.247但是从bond0.700可以</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  **gw 11.164.191.247** netmask 255.255.255.0 bond0.700</div></pre></td></tr></table></figure>
<p><a href="https://serverfault.com/questions/581159/unable-to-add-a-static-route-sioaddrt-network-is-unreachable" target="_blank" rel="external">https://serverfault.com/questions/581159/unable-to-add-a-static-route-sioaddrt-network-is-unreachable</a></p>
<h2 id="linux-2-6-32内核高精度定时器带来的cpu-sy暴涨的“问题”"><a href="#linux-2-6-32内核高精度定时器带来的cpu-sy暴涨的“问题”" class="headerlink" title="linux 2.6.32内核高精度定时器带来的cpu sy暴涨的“问题”"></a>linux 2.6.32内核高精度定时器带来的cpu sy暴涨的“问题”</h2><p>在 2.6.32 以前的内核里，即使你在java里写queue.await(1ns)之类的代码，其实都是需要1ms左右才会执行的，但.32以后则可以支持ns级的调度，对于实时性要求非常非常高的性能而言，这本来是个好特性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat /proc/timer_list | grep .resolution</div></pre></td></tr></table></figure>
<p>可以通过在 /boot/grub2/grub.cfg 中相应的kernel行的最后增加highres=off nohz=off来关闭高精度（不建议这样做，最好还是程序本身做相应的修改）</p>
<h2 id="后台执行"><a href="#后台执行" class="headerlink" title="后台执行"></a>后台执行</h2><p>将任务放到后台，断开ssh后还能运行：</p>
<ol>
<li>“ctrl-Z”将当前任务挂起；</li>
<li>“disown -h”让该任务忽略 SIGHUP 信号（不会因为掉线而终止执行）；</li>
<li>“bg”让该任务在后台恢复运行。</li>
</ol>
<h2 id="Linux-进程调度"><a href="#Linux-进程调度" class="headerlink" title="Linux 进程调度"></a>Linux 进程调度</h2><p>Linux的进程调度有一个不太为人熟知的特性，叫做<strong>wakeup affinity</strong>，它的初衷是这样的：如果两个进程频繁互动，那么它们很有可能共享同样的数据，把它们放到亲缘性更近的scheduling domain有助于提高缓存和内存的访问性能，所以当一个进程唤醒另一个的时候，被唤醒的进程可能会被放到相同的CPU core或者相同的NUMA节点上。</p>
<p>这个特性缺省是打开的，它有时候很有用，但有时候却对性能有伤害作用。设想这样一个应用场景：一个主进程给成百上千个辅进程派发任务，这成百上千个辅进程被唤醒后被安排到与主进程相同的CPU core或者NUMA节点上，就会导致负载严重失衡，CPU忙的忙死、闲的闲死，造成性能下降。<a href="https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA" target="_blank" rel="external">https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA</a></p>
<h2 id="tty"><a href="#tty" class="headerlink" title="tty"></a><a href="https://www.cnblogs.com/liqiuhao/p/9031803.html" target="_blank" rel="external">tty</a></h2><p>tty（teletype–最早的一种终端设备，远程打字机） stty 设置tty的相关参数</p>
<p>tty都在 /dev 下，通过 ps -ax 可以看到进程的tty；通过tty 可以看到本次的终端</p>
<p>/dev/pty（Pseudo Terminal） 伪终端</p>
<p>/dev/tty 控制终端</p>
<p>远古时代tty是物理形态的存在</p>
<p><img src="/images/951413iMgBlog/v2-7aa6997d017d876543671e4113048a62_1440w.jpg" alt="img"></p>
<p>PC时代，物理上的terminal已经没有了（用虚拟的伪终端代替，pseudo tty, 简称pty），相对kernel增加了shell，这是terminal和shell容易混淆，他们的含义</p>
<p><img src="/images/951413iMgBlog/v2-63cdd117f1026c2bbf455920b29c4454_1440w.jpg" alt="img"></p>
<p>实际像如下图的工作协作:</p>
<p><img src="/images/951413iMgBlog/case3.png" alt="Diagram"></p>
<h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a><a href="https://wangdoc.com/ssh/rsync.html" target="_blank" rel="external">rsync</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">将本地yum备份到150上的/data/yum/ 下</div><div class="line">rsync -arv ./yum/ root@11.167.60.150:/data/yum/</div><div class="line"></div><div class="line">走ssh的8022端口把目录备份到本地</div><div class="line">rsync -e &apos;ssh -p 8022&apos; -arv gcsql@10.237.3.100:/home/gcsql/doc/ ./</div></pre></td></tr></table></figure>
<p><code>-a</code>、<code>--archive</code>参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。</p>
<p><code>--delete</code>参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。</p>
<p><code>-i</code>参数表示输出源目录与目标目录之间文件差异的详细情况。</p>
<p><code>--link-dest</code>参数指定增量备份的基准目录。</p>
<p><code>-n</code>参数或<code>--dry-run</code>参数模拟将要执行的操作，而并不真的执行。配合<code>-v</code>参数使用，可以看到哪些内容会被同步过去。</p>
<p><code>--partial</code>参数允许恢复中断的传输。不使用该参数时，<code>rsync</code>会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与<code>--append</code>或<code>--append-verify</code>配合使用。</p>
<p><code>--progress</code>参数表示显示进展。</p>
<p><code>-r</code>参数表示递归，即包含子目录。</p>
<p><code>-v</code>参数表示输出细节。<code>-vv</code>表示输出更详细的信息，<code>-vvv</code>表示输出最详细的信息。</p>
<h2 id="Shebang"><a href="#Shebang" class="headerlink" title="Shebang"></a>Shebang</h2><p>Shebang 的东西 <code>#!/bin/bash</code></p>
<p>对 Shebang 的处理是内核在进行。当内核加载一个文件时，会首先读取文件的前 128 个字节，根据这 128 个字节判断文件的类型，然后调用相应的加载器来加载。</p>
<h3 id="ELF（Executable-and-Linkable-Format）"><a href="#ELF（Executable-and-Linkable-Format）" class="headerlink" title="ELF（Executable and Linkable Format）"></a>ELF（Executable and Linkable Format）</h3><p>对应windows下的exe</p>
<h2 id="修改启动参数"><a href="#修改启动参数" class="headerlink" title="修改启动参数"></a>修改启动参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">$cat change_kernel_parameter.sh </div><div class="line">#cat /sys/devices/system/cpu/vulnerabilities/*</div><div class="line">#grep &apos;&apos; /sys/devices/system/cpu/vulnerabilities/*</div><div class="line">#https://help.aliyun.com/document_detail/102087.html?spm=a2c4g.11186623.6.721.4a732223pEfyNC</div><div class="line"></div><div class="line">#cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">#transparent_hugepage=always</div><div class="line">#noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off</div><div class="line">#追加nopti nospectre_v2到内核启动参数中</div><div class="line">sudo sed -i &apos;s/\(GRUB_CMDLINE_LINUX=&quot;.*\)&quot;/\1 nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off transparent_hugepage=always&quot;/&apos; /etc/default/grub</div><div class="line"></div><div class="line">//从修改的 /etc/default/grub 生成 /boot/grub2/grub.cfg 配置</div><div class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</div><div class="line"></div><div class="line">#limit the journald log to 500M</div><div class="line">sed -i &apos;s/^#SystemMaxUse=$/SystemMaxUse=500M/g&apos; /etc/systemd/journald.conf</div><div class="line">#重启系统</div><div class="line">#sudo reboot</div><div class="line"></div><div class="line">## 选择不同的kernel启动</div><div class="line">#sudo grep &quot;menuentry &quot; /boot/grub2/grub.cfg | grep -n menu</div><div class="line">##grub认的index从0开始数的</div><div class="line">#sudo grub2-reboot 0; sudo reboot</div><div class="line"></div><div class="line">$cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">always [madvise] never</div></pre></td></tr></table></figure>
<h2 id="制作启动盘"><a href="#制作启动盘" class="headerlink" title="制作启动盘"></a>制作启动盘</h2><p>Windows 上用 UltraISO 烧制，Mac 上就比较简单了，直接用 dd 就可以搞</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ diskutil list</div><div class="line">/dev/disk6 (external, physical):</div><div class="line">   #:                       TYPE NAME                    SIZE       IDENTIFIER</div><div class="line">   0:                                                   *31.5 GB    disk6</div><div class="line">                        </div><div class="line"># 找到 U 盘的那个设备，umount</div><div class="line">$ diskutil unmountDisk /dev/disk3</div><div class="line"></div><div class="line"># 用 dd 把 ISO 文件写进设备，注意这里是 rdisk3 而不是 disk3，在 BSD 中 r(IDENTIFIER)</div><div class="line"># 代表了 raw device，会快很多</div><div class="line">$ sudo dd if=/path/image.iso of=/dev/rdisk3 bs=1m</div><div class="line"></div><div class="line"># 弹出 U 盘</div><div class="line">$ sudo diskutil eject /dev/disk3</div></pre></td></tr></table></figure>
<p><a href="https://linuxiac.com/how-to-create-bootable-usb-drive-using-dd-command/" target="_blank" rel="external">Linux 下制作步骤</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">umount /dev/sdn1</div><div class="line">sudo mkfs.vfat /dev/sdn1</div><div class="line">dd if=/polarx/uniontechos-server-20-1040d-amd64.iso of=/dev/sdn1 status=progress</div></pre></td></tr></table></figure>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>为保证服务性能应选用 performance 模式，将 CPU 频率固定工作在其支持的最高运行频率上，不进行动态调节，操作命令为 <code>cpupower frequency-set --governor performance</code>。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ul>
<li>dmesg | tail</li>
<li>vmstat 1</li>
<li>mpstat -P ALL 1</li>
<li>pidstat 1</li>
<li>iostat -xz 1</li>
<li>free -m</li>
<li>sar -n DEV 1</li>
<li>sar -n TCP,ETCP 1</li>
</ul>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">//检查sda磁盘中哪个应用程序占用的io比较高</div><div class="line">pidstat -d  1</div><div class="line"></div><div class="line">//分析应用程序中哪一个线程占用的io比较高</div><div class="line">pidstat -dt -p 73739 1  执行两三秒即可,得到74770线程io高</div><div class="line"></div><div class="line">//分析74770这个线程在干什么</div><div class="line">perf trace -t 74770 -o /tmp/tmp_aa.pstrace</div><div class="line">cat /tmp/tmp_aa.pstrace</div><div class="line">  2850.656 ( 1.915 ms): futex(uaddr: 0x653ae9c4, op: WAIT|PRIVATE_FLAG, val: 1)               = 0</div><div class="line">  2852.572 ( 0.001 ms): futex(uaddr: 0x653ae990, op: WAKE|PRIVATE_FLAG, val: 1)               = 0</div><div class="line">  2852.601 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2852.690 ( 0.040 ms): write(fd: 159, buf: 0xd7a30020, count: 65536)                         = 65536</div><div class="line">  2852.796 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2852.798 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58)             = 0</div><div class="line">  2852.939 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38)             = 0</div><div class="line">  2852.950 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2852.977 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2853.029 ( 0.035 ms): write(fd: 64, buf: 0xcd51e020, count: 65536)                          = 65536</div><div class="line">  2853.164 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2853.167 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58)             = 0</div><div class="line">  2853.302 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38)             = 0</div></pre></td></tr></table></figure>
<h3 id="内存——虚拟内存参数"><a href="#内存——虚拟内存参数" class="headerlink" title="内存——虚拟内存参数"></a>内存——虚拟内存参数</h3><ul>
<li><code>dirty_ratio</code> 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统将开始使用 pdflush 操作将脏的 page cache 写入磁盘。默认值为 20％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，降低其值有利于提高内存回收时的效率。</li>
<li><code>dirty_background_ratio</code> 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统开始在后台将脏的 page cache 写入磁盘。默认值为 10％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，设置较低的值有利于提高内存回收时的效率。</li>
</ul>
<h3 id="I-O-调度器"><a href="#I-O-调度器" class="headerlink" title="I/O 调度器"></a>I/O 调度器</h3><p>I/O 调度程序确定 I/O 操作何时在存储设备上运行以及持续多长时间。也称为 I/O 升降机。对于 SSD 设备，宜设置为 noop。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> noop &gt; /sys/block/<span class="variable">$&#123;SSD_DEV_NAME&#125;</span>/queue/scheduler</div></pre></td></tr></table></figure>
<h3 id="磁盘挂载参数"><a href="#磁盘挂载参数" class="headerlink" title="磁盘挂载参数"></a>磁盘挂载参数</h3><p><code>noatime</code> 读取文件时，将禁用对元数据的更新。它还启用了 nodiratime 行为，该行为会在读取目录时禁用对元数据的更新。</p>
<h2 id="Unix-Linux关系"><a href="#Unix-Linux关系" class="headerlink" title="Unix Linux关系"></a>Unix Linux关系</h2><p><img src="/images/951413iMgBlog/image-20211210085124387.png" alt="image-20211210085124387"></p>
<p><img src="/images/951413iMgBlog/G2Xri.png" alt="img"></p>
<h3 id="linux-发行版关系"><a href="#linux-发行版关系" class="headerlink" title="linux 发行版关系"></a><a href="https://blog.51cto.com/wangyafei/1881605" target="_blank" rel="external">linux 发行版关系</a></h3><p><img src="/images/951413iMgBlog/5cc164f5d79a11261.jpg_fo742.jpg" alt="细数各家linux之间的区别_软件应用_什么值得买"></p>
<p>Fedora：基于Red Hat Linux，在Red Hat Linux终止发行后，红帽公司计划以Fedora来取代Red Hat Linux在个人领域的应用，而另外发行的Red Hat Enterprise Linux取代Red Hat Linux在商业应用的领域。Fedora的功能对于用户而言，它是一套功能完备、更新快速的免费操作系统，而对赞助者Red Hat公司而言，它是许多新技术的测试平台，被认为可用的技术最终会加入到Red Hat Enterprise Linux中。Fedora大约每六个月发布新版本。</p>
<p>不同发行版几乎采用了不同包管理器（SLES、Fedora、openSUSE、centos、RHEL使用rmp包管理系统，包文件以RPM为扩展名；Ubuntu系列，Debian系列使用基于DPKG包管理系统，包文件以deb为扩展名。)</p>
<p>69年Unix诞生在贝尔实验室，80年 DARPA（国防部高级计划局）请人在Unix实现全新的TCP、IP协议栈。ARPANET最先搞出以太网</p>
<p>Linux 从91年到95年处于成长期，真正大规模应用是Linux+Apache提供的WEB服务被大家大规模采用</p>
<p>rpm:  centos/fedora/suse</p>
<p>deb:  debian/ubuntu/uos(早期基于ubuntu定制，后来基于debian定制，再到最近开始直接基于kernel定制)</p>
<p>ARPANET：<strong>高等研究計劃署網路</strong>（英語：Advanced Research Projects Agency Network），通称<strong>阿帕网</strong>（英語：ARPANET）是美國<a href="https://zh.m.wikipedia.org/wiki/國防高等研究計劃署" target="_blank" rel="external">國防高等研究計劃署</a>开发的世界上第一个运营的<a href="https://zh.m.wikipedia.org/wiki/封包交換" target="_blank" rel="external">封包交换</a>网络，是全球<a href="https://zh.m.wikipedia.org/wiki/互联网" target="_blank" rel="external">互联网</a>的鼻祖。</p>
<p>TCP/IP：1974年，卡恩和瑟夫带着研究成果，在IEEE期刊上，发表了一篇题为《关于分组交换的网络通信协议》的论文，正式提出TCP/IP，用以实现计算机网络之间的互联。</p>
<p>在1983年，美国国防部高级研究计划局决定淘汰NCP协议（ARPANET最早使用的协议），TCP/IP取而代之。</p>
<h3 id="Deepin-UOS"><a href="#Deepin-UOS" class="headerlink" title="Deepin UOS"></a>Deepin UOS</h3><p><strong><em>\</em>Deepin 与统信 UOS 类似于红帽的 Fedora 与 RHEL 的上下游关系，Deepin 依然保持着原来的社区运营模式，而统信 UOS 则是基于社区版 Deepin 构建的商业发行版，为 Deepin 挖掘更多的商业机会和更大的商业价值，进而反哺社区，形成良性循环**</strong>。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.cnblogs.com/kevingrace/p/8671964.html" target="_blank" rel="external">https://www.cnblogs.com/kevingrace/p/8671964.html</a></p>
<p><a href="https://www.jianshu.com/p/ac3e7009a764" target="_blank" rel="external">https://www.jianshu.com/p/ac3e7009a764</a></p>
<p>B 站哈工大操作系统视频地址：<a href="https://www.bilibili.com/video/BV1d4411v7u7?from=search&amp;seid=2361361014547524697" target="_blank" rel="external">https://www.bilibili.com/video/BV1d4411v7u7?from=search&amp;seid=2361361014547524697</a></p>
<p>B 站清华大学操作系统视频地址：<a href="https://www.bilibili.com/video/BV1js411b7vg?from=search&amp;seid=2361361014547524697" target="_blank" rel="external">https://www.bilibili.com/video/BV1js411b7vg?from=search&amp;seid=2361361014547524697</a></p>
<p><a href="https://linux.cn/article-10465-1.html" target="_blank" rel="external">Linux 工具：点的含义</a> <a href="https://www.linux.com/training-tutorials/linux-tools-meaning-dot/" target="_blank" rel="external">英文版</a></p>
<p><a href="http://coolnull.com/4432.html" target="_blank" rel="external">linux cp实现强制覆盖</a></p>
<p><a href="https://wangdoc.com/bash/startup.html" target="_blank" rel="external">https://wangdoc.com/bash/startup.html</a></p>
<p><a href="https://cjting.me/2020/12/10/tiny-x64-helloworld/" target="_blank" rel="external">编写一个最小的 64 位 Hello World</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/01/2010到2020这10年的碎碎念念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/01/2010到2020这10年的碎碎念念/" itemprop="url">2010到2020这10年的碎碎念念</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-01T00:30:03+08:00">
                2020-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/others/" itemprop="url" rel="index">
                    <span itemprop="name">others</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="2010到2020这10年的碎碎念念"><a href="#2010到2020这10年的碎碎念念" class="headerlink" title="2010到2020这10年的碎碎念念"></a>2010到2020这10年的碎碎念念</h1><h2 id="来自网络的一些数据"><a href="#来自网络的一些数据" class="headerlink" title="来自网络的一些数据"></a>来自网络的一些数据</h2><p>这十年，中国的人均GDP从大约3300美金干到了9800美金。这意味着：更多的中国人脱贫，更多的中国人变成了中产。这是这一轮消费升级的核心原动力，没有之一。</p>
<p>这十年，中国的进出口的总额从2009年占GDP的44.86%，降至34.35%。</p>
<p>互联网从美国copy开始变成创新、走在前列，因为有庞大的存量市场</p>
<p>2010年，一个数据发生了逆势波动。那就是中国的适龄小学人口增速。在此之前从1997年后，基本呈负增长。这是因为中国80后家长开始登上历史舞台。这带动了诸多产业的蓬勃发展，比如互联网教育，当然还有学区房。</p>
<p>10年吉利收购沃尔沃，18年吉利收购戴姆勒10%的股份。</p>
<p>微信崛起、头条崛起、百度走下神坛。美团、pdd崛起</p>
<p>12年2月6号的王护士长外逃美国大使馆也让大家兴奋了，11年的郭美美红十字会事件快要被忘记了，但是也让大家对慈善事件更加警惕，倒是谅解了汶川地震的王石10块捐款事件，不过老王很快因为娶了年轻的影星田朴珺一下子人设坍塌，大家更热衷老王的负面言论了。</p>
<p>温州动车事件让高铁降速了</p>
<p>我爸是李刚、药家鑫、李天一、邓玉娇（09年），陈冠希艳照门、三鹿奶粉、汶川地震、奥运会（08年）</p>
<p>2018年：中美贸易站、问题疫苗、个税改革、中兴被美制裁，北京驱赶低端人口，鸿茅药酒，p2p暴雷，昆山反杀案，相互宝</p>
<p>2015年：雾霾、柴静纪录片《穹顶之下》，屠呦呦诺贝尔奖，放开二胎</p>
<p>2014年：东莞扫黄、马航370事件；周师傅被查、占中</p>
<p>2013年：劳教正式被废除，想起2003年的孙志刚事件废除收容制度</p>
<p>2012年：方韩之争，韩寒走下神坛</p>
<p>2011年：日本海啸地震，中国抢盐事件；郭美美，温州动车</p>
<p>2010年：google退出中国，上海世博会开幕，富士康N连跳楼事件；我爸是李刚，腾讯大战360</p>
<h2 id="自我记忆"><a href="#自我记忆" class="headerlink" title="自我记忆"></a>自我记忆</h2><p>刚看到有人在说乐清钱云会事件，一晃10年了，10年前微博开始流行改变了好多新闻、热点事件的引爆方式。</p>
<p>这十年BBS、门户慢慢在消亡，10年前大家都知道三大门户网站和天涯，现在的新网民应该知道的不多了。</p>
<p>影响最大的还是移动网络的崛起，这也取决于4G和山寨机以及后来的小米手机，真正给中国的移动互联网带来巨大的红利，注入的巨大的增长。<br>我自己对移动互联网的判断是极端错误的，即使09年我就开始用上了iphone手机，那又怎么样，看问题还是用静态的视觉观点。手机没有键盘、手机屏幕狭小，这些确实是限制，到2014年我还想不明白为什么要在手机上购物，比较、看物品图片太不方便了，结果便利性秒杀了这些不方便；只有手机的群体秒杀了办公室里的白领，最后大家都很高兴地用手机购物了，甚至PC端bug更多，更甚至有些网站不提供PC端。</p>
<p>移动网络的崛起和微信的成功也相辅相成的，在移动网络时代每个人都有自己的手机，所以账号系统的打通不再是问题，尤其是都被微信这个移动航母在吞噬，其它公司都活在微信的阴影里。</p>
<p>当然移动支付的崛起就理所当然了。</p>
<p>即使今天网上购物还是PC上要方便，那又怎么样，很多时候网上购物都是不在电脑前的零碎时间。</p>
<p>10多年前第一次看到智能手机是室友购买的多普达，20年前也是这个室友半夜里很兴奋地播报台湾大选，让我知道了台湾大选这个事情。</p>
<p>基本的价值观、世界观，没怎么改变，不应该是年龄大了僵化了，应该是掌握信息的手段和能力增强了，翻墙获取信息也很容易，基本的逻辑还在也没那么容易跑偏了。可能就是别人看到的年纪大了脑子僵化了吧，自我感觉不一定对。</p>
<p>最近10年经济发展的非常好，政府对言论的控制越来越精准，舆论引导也非常”成功”,所以网络上看到这5年和5年前基本差别很大，5年前公知是个褒义词，5年后居然成了贬义词。</p>
<p>房价自然是这10年最火的话题，07年大家开始感觉到房价上涨快、房价高，08年金融危机本来是最好的机会，结果4万亿刺激下09年年底房价开始翻倍，到10年面对翻倍了的房价政府、媒体、老百姓都在喊高，实际也只是横盘，13-14年小拉一波，16年涨价去库存再大拉一波。基本让很多人绝望了</p>
<p>这十年做的最错的事情除了没有早点买房外就是想搞点投资收入投了制造外加炒股，踩点能力太差了，虽然前5年像任志强一样一直看多房价的不多，这个5年都被现实教育了，房价也基本到头了。</p>
<p>工作上应该更早地、坚定地进入互联网、移动互联网，这10年互联网对人才的需求实在太大了，虽然最终能伴随公司成长的太少，毕竟活下来长大的公司不多。</p>
<p>Google退出中国、看着小杨同学和一些同事移民、360大战QQ、诺贝尔和平奖、华为251事件都算是自己在一些公众事情上投入比较多的。非常不舍google的离开，这些年也基本还是只用google，既是无奈中用下百度也还是觉得搜不到什么有效信息；好奇移民的想法和他们出去后的各种生活；360跟QQ大战的时候觉得腾讯的垄断太牛叉了，同时认为可能360有这种资源的话会更作恶和垄断的更厉害，至少腾讯还是在乎外面的看法和要面子的；LXB到现在也是敏感词，直到病死在软禁中，这些年敏感词越来越多，言论的控制更严厉了；华为251也是个奇葩事件了，暴露了资本家的粗野和枉法。</p>
<p>自己工作上跳槽一次，继续做一个北漂。公司对自己的方法论改变确实比较大，近距离看到了一些成功因素方面的逻辑（更有效的激励和企业文化）。</p>
<p>经历了从外企到私企，从小公司到大公司的不同，外企英语是天花板，也看到了华为所谓的狼性、在金钱激励下的狼性，和对企业文化的维护，不能否认90%以上的人工作是为了钱</p>
<p>这几年也开始习惯写技术文章来总结了，这得益于Markdown+截图表述的便利，也深刻感受到深抠，然后总结分享的方法真的很好（高斯学习方法），也体会到了抓手、触类旁通的运用。10年前在搜狐blog写过一两年的博客放弃的很快，很难一直有持续的高水平总结和输出。</p>
<p>10年前还在比较MSN和QQ谁更好（我是坚定站在QQ这边的），10年后MSN再也看不见了，QQ也有了更好的替代工具微信。用处不大的地方倒是站对了，对自己最有用的关键地方都站错了。</p>
<p>10年前差点要去豆瓣，10年后豆瓣还活着，依然倔强地保持自己的品味，这太难得了。相反十年前好用得不得了的RSS订阅，从抓虾转到google reader再到feedly好东西就是活得这么艰难。反过来公众号起来了、贴吧式微了，公众号运作新闻类是没问题的（看完就过），但是对技术类深度一点的就很不合适了，你看看一篇文章24小时内的阅读量占据了98%以上，再到后面就存亡了！但是公众号有流量，流量可以让大家跪在地上。</p>
<p>10年前啃老是被看不起的，10年后早结婚、多啃老也基本成了这10年更对的事情，结婚得买房，啃老买得更早，不对的事情变对了（结婚早没错）。</p>
<p>很成功地组织了一次同学20周年的聚会，也看到了远则亲、近则仇的现实情况，自己组织统筹能力还可以。</p>
<p>情绪控制能力太差、容易失眠。这十年爱上了羽毛球和滑雪，虽然最近几年滑雪少了。</p>
<p>体会到自小贫穷带来的一些抠门的坏习惯。</p>
<p>2015年的股票大跌让自己很痛苦，这个过程反馈出来的不愿意撒手、在股市上的鸵鸟方式，股市上总是踩不到正确的点。割肉太难，割掉的总是错误的。</p>
<p>15、16年我认为云计算不怎么样，觉得无非就是新瓶装旧酒，现在云计算不再有人质疑了，即使现在都还是亏钱。</p>
<p>当然我也质疑过外卖就是一跑腿的，确实撑不起那么大的盘子，虽然没有像团购一样消亡，基本跟共享单车一样了，主要因为我是共享单车的重度用户，而我极端不喜欢外卖，所以要站出来看问题、屁股坐在哪边会严重影响看法，也就是不够客观。</p>
<p>网约车和移动支付一起在硝烟中混战</p>
<p>电动车开始起来，主要受政府弯道超车的刺激，目前看取决于自有充电位（适合三四线城市），可是三四线城市用户舍不得花这个溢价，汽油车都还没爽够呢。</p>
<p>对世界杯不再那么关注，对AlphaGo的新闻倒是很在意了。魏则西事件牢牢地把百度钉死在耻辱柱上。</p>
<p>随着12306的发展和高铁的起来，终于过年回家的火车票不用再靠半夜排队了。</p>
<p>2019年年末行政强制安装ETC，让我想起20年前物理老师在课堂上跟我们描述的将来小汽车走高速公路再也不用停下来收费了，会自动感应，开过去就自动扣钱了。我一直对这个未来场景念念不忘，最近10年我经常问别人为什么不办ETC，这个年底看到的是行政命令下的各种抱怨。</p>
<h3 id="看到："><a href="#看到：" class="headerlink" title="看到："></a>看到：</h3><ul>
<li><p>老人、家人更不愿意听身边亲近人员的建议；</p>
</li>
<li><p>老人思维为什么固化、怎么样在自己老后不是那样固化；</p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="twitter @plantegg" />
          <p class="site-author-name" itemprop="name">twitter @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">145</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">240</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
