<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/5/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/page/5/"/>





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/23/一次春节大促性能压测不达标的瓶颈推演/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/23/一次春节大促性能压测不达标的瓶颈推演/" itemprop="url">一次春节大促性能压测不达标的瓶颈推演</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-23T11:30:03+08:00">
                2020-11-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一次春节大促性能压测不达标的瓶颈推演"><a href="#一次春节大促性能压测不达标的瓶颈推演" class="headerlink" title="一次春节大促性能压测不达标的瓶颈推演"></a>一次春节大促性能压测不达标的瓶颈推演</h1><p>本文示范了教科书式的在分布式应用场景下如何通过一个节点的状态来推演分析瓶颈出在上下游的哪个环节上。</p>
<h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>某客户通过PTS（一个打压力工具）来压选号业务(HTTP服务在9108端口上），一个HTTP请求对应一次select seq-id 和 一次insert</p>
<p>PTS端看到RT900ms+，QPS大概5万（期望20万）， 数据库代理服务 rt 5ms，QPS 10万+</p>
<h3 id="链路："><a href="#链路：" class="headerlink" title="链路："></a>链路：</h3><p>pts发起压力 -&gt; 5个eip -&gt; slb -&gt; app(300个容器运行tomcat监听9108端口上） -&gt; slb -&gt; 数据库代理服务集群 -&gt; RDS集群</p>
<p>性能不达标，怀疑数据库代理服务或者RDS性能不行，作为数据库需要自证清白，所以从RDS和数据库代理服务开始分析问题在哪里。</p>
<p>略过一系列在数据库代理服务、RDS上分析数据和监控图表都证明数据库代理服务和RDS没问题的过程。</p>
<p>在明确给出证据数据库代理服务和RDS都没问题后还是要解决问题，所以只能进一步帮助前面的app来分析为什么性能不达标。</p>
<h2 id="在其中一个app应用上抓包（00-18秒到1-04秒），到数据库代理服务的一个连接分析："><a href="#在其中一个app应用上抓包（00-18秒到1-04秒），到数据库代理服务的一个连接分析：" class="headerlink" title="在其中一个app应用上抓包（00:18秒到1:04秒），到数据库代理服务的一个连接分析："></a>在其中一个app应用上抓包（00:18秒到1:04秒），到数据库代理服务的一个连接分析：</h2><p><img src="/images/oss/80374e55936bc36bbd243f79fcdb5f8d.png" alt="image.png"></p>
<p>数据库代理服务每个HTTP请求的响应时间都控制在15ms(一个前端HTTP请求对应一个select seq-id，一个 select readonly, 一个insert， 这个响应时间符合预期）。一个连接每秒才收到20 tps（因为压力不够，压力加大的话这个单连接tps还可以增加）， 20*3000 = 6万 ， 跟压测看到基本一致</p>
<p>300个容器，每个容器 10个连接到数据库代理服务</p>
<p>如果300个容器上的并发压力不够的话就没法将3000个连接跑满，所以看到的QPS是5万。</p>
<p><strong>从300个容器可以计算得到这个集群能支持的tps： 300*10（10个连接）* 1000/15(每秒钟每个连接能处理的请求数）=20万个tps （关键分析能力）</strong></p>
<p>也就是说通过单QPS 15ms，我们计算可得整个后端的吞吐能力在20万QPS。所以目前问题不在后端，而是压力没有打到后端就出现瓶颈了。</p>
<h2 id="9108的HTTP服务端口上的抓包分析"><a href="#9108的HTTP服务端口上的抓包分析" class="headerlink" title="9108的HTTP服务端口上的抓包分析"></a>9108的HTTP服务端口上的抓包分析</h2><p><img src="/images/oss/e239a12a1c3612263736256c8efc06e4.png" alt="image.png"></p>
<p>9108服务的每个HTTP response差不多都是15ms（<strong>这个响应时间基本符合预期</strong>），一个HTTP连接上在45秒的抓包时间范围只收到23个HTTP Request。</p>
<p>或者下图：</p>
<p><img src="/images/951413iMgBlog/image-20220627164250973.png" alt="image-20220627164250973" style="zoom:50%;"></p>
<p><img src="/images/951413iMgBlog/image-20220630101036341.png" alt="image-20220630101036341" style="zoom:50%;"></p>
<p>统计9108端口在45秒总共收到的HTTP请求数量是6745（如下图），也就是每个app每秒钟收到的请求是150个，300<em>150=4.5万（理论值，300个app可能压力分布不一样？），<em>*从这里看app收到的压力还不够</em></em>，所以压力还没有打到应用容器中的app，还在更前面</p>
<p><img src="/images/oss/6a289d1bba1e875d215032b6fdc7b084.png" alt="image.png"></p>
<p>后来从容器app监控也确认了这个响应时间和抓包看到的一致，所以从抓包分析http响应时间也基本得到15ms的rt关键结论</p>
<p>从wireshark IO Graphs 也能看到RT 和 QPS</p>
<p><img src="/images/951413iMgBlog/image-20220623003026351.png" alt="image-20220623003026351"></p>
<h2 id="从应用容器上的netstat统计来看，也是压力端回复太慢"><a href="#从应用容器上的netstat统计来看，也是压力端回复太慢" class="headerlink" title="从应用容器上的netstat统计来看，也是压力端回复太慢"></a>从应用容器上的netstat统计来看，也是压力端回复太慢</h2><p><img src="/images/oss/938ce314d19b47cba99e2a09c753f606.png" alt="image.png"></p>
<p>send-q表示回复从9108发走了，没收到对方的ack</p>
<h2 id="ARMS监控分析9108端口上的RT"><a href="#ARMS监控分析9108端口上的RT" class="headerlink" title="ARMS监控分析9108端口上的RT"></a>ARMS监控分析9108端口上的RT</h2><p>后来PTS的同学说ARMS可以捞到监控数据，如下是对rt时间降序排</p>
<p><img src="/images/oss/a479bad250c03aee41d58850afab9c14.png" alt="image.png"></p>
<p>中的rt平均时间，可以看到http的rt确实14.4ms，表现非常平稳，从这个监控也发现实际app是330个而不是用户自己描述的300个，这也就是为什么实际是tps是5万，但是按300个去算的话tps是4.5万（不要纠结客户为什么告诉你是300个容器而不是330个，有时候他们也搞不清楚，业务封装得太好了）</p>
<p><img src="/images/oss/2f3b76be63d331510eb6f2cecd91747f.png" alt="image.png"></p>
<p>5分钟时间，QPS是5万+，HTTP的平均rt是15ms， HTTP的最大rt才79ms，和前面抓包分析一致。</p>
<h2 id="从后端分析的总结"><a href="#从后端分析的总结" class="headerlink" title="从后端分析的总结"></a>从后端分析的总结</h2><p><strong>从9108端口响应时间15ms来看是符合预期的，为什么PTS看到的RT是900ms+，所以压力还没有打到APP上（也就是9108端口）</strong></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>最后发现是 eip 带宽不足，只有200M，调整到1G后 tps 也翻了5倍到了25万。</p>
<p>pts -&gt; 5个eip(总带宽200M) -&gt; slb -&gt; app(330个HTTP容器） -&gt; slb -&gt; 数据库代理服务 -&gt; RDS</p>
<p>这个案例有意思的地方是可以通过抓包就能分析出集群能扛的QPS20万（实际只有5万），那么可以把这个分析原则在每个角色上挨个分析一下，来看瓶颈出在了哪个环节。</p>
<p>应用端看到的rt是900ms，从后段开始往前面应用端来撸，看看每个环节的rt数据。</p>
<h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><ul>
<li>搞清楚 请求 从发起端到DB的链路路径，比如 pts -&gt; 5个eip(总带宽200M) -&gt; slb -&gt;  app(330个HTTP容器） -&gt; slb -&gt; 数据库代理服务 -&gt; RDS </li>
<li>压不上去得从发压力端开始往后端撸，撸每个产品的rt，每个产品给出自己的rt来自证清白</li>
<li>应用有arms的话学会看arms对平均rt和QPS的统计，不要纠结个别请求的rt抖动，看平均rt</li>
<li>通过抓包完全可以分析出来系统能扛多少并发，以及可能的瓶颈位置</li>
</ul>
<p>一包在手 万事无忧</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/18/TCP连接为啥互串了/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/18/TCP连接为啥互串了/" itemprop="url">活久见，TCP连接互串了</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-18T17:30:03+08:00">
                2020-11-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="活久见，TCP连接互串了"><a href="#活久见，TCP连接互串了" class="headerlink" title="活久见，TCP连接互串了"></a>活久见，TCP连接互串了</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>应用每过一段时间总是会抛出几个连接异常的错误，需要查明原因。</p>
<p>排查后发现是TCP连接互串了，这个案例实在是很珍惜，所以记录一下。</p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>业务结构： 应用-&gt;MySQL(10.112.61.163)</p>
<p>在 应用 机器上抓包这个异常连接如下（3269为MySQL服务端口）：</p>
<p><img src="/images/oss/dd657fee9d961a786c05e8d3cccbc297.png" alt="image.png"></p>
<p>粗一看没啥奇怪的，就是应用发查询给3269，但是一直没收到3269的ack，所以一直重传。这里唯一的解释就是网络不通。最后MySQL的3269还回复了一个rst，这个rst的id是42889，引起了我的好奇，跟前面的16439不连贯，正常应该是16440才对。（请记住上图中的绿框中的数字）</p>
<p>于是我过滤了一下端口61902上的所有包：</p>
<p><img src="/images/oss/8ca7da8ccec0041dd5d3f66f94d1f574.png" alt="image.png"></p>
<p>可以看到绿框中的查询从61902端口发给3269后，很奇怪居然收到了一个来自别的IP+3306端口的reset，这个包对这个连接来说自然是不认识（这个连接只接受3269的回包），就扔掉了。但是也没收到3269的ack，所以只能不停地重传，然后每次都收到3306的reset，reset包的seq、id都能和上图的绿框对应上。</p>
<p>明明他们应该是两个连接：</p>
<blockquote>
<p> 61902-&gt;10.141.16.0:3306</p>
<p> 61902-&gt;10.112.61.163:3269</p>
</blockquote>
<p>他们虽然用的本地ip端口（61902）是一样的， 但是根据四元组不一样，还是不同的TCP连接，所以应该是不会互相干扰的。但是实际看起来<strong>seq、id都重复了</strong>，不会有这么巧，非常像是TCP互串了。</p>
<h2 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h2><p>10.141.16.0 这个ip看起来像是lvs的ip，查了一下系统，果然是lvs，然后这个lvs 后面的rs就是10.112.61.163</p>
<p>那么这个连结构就是10.141.16.0:3306：</p>
<blockquote>
<p>应用 -&gt; lvs(10.141.16.0:3306)-&gt; 10.112.61.163:3269  跟应用直接连MySQL是一回事了</p>
</blockquote>
<p>所以这里的疑问就变成了：<strong>10.141.16.0 这个IP的3306端口为啥能知道 10.112.61.163:3269端口的seq和id，也许是TCP连接串了</strong></p>
<p>接着往下排查</p>
<h3 id="先打个岔，分析下这里的LVS的原理"><a href="#先打个岔，分析下这里的LVS的原理" class="headerlink" title="先打个岔，分析下这里的LVS的原理"></a><a href="/2019/06/20/就是要你懂负载均衡--lvs和转发模式/">先打个岔，分析下这里的LVS的原理</a></h3><p>这里使用的是 full NAT模型(full NetWork Address Translation-全部网络地址转换)</p>
<p>基本流程（类似NAT）：</p>
<ol>
<li>client发出请求（sip 200.200.200.2 dip 200.200.200.1）</li>
<li>请求包到达lvs，lvs修改请求包为<strong>（sip 200.200.200.1， dip rip）</strong> 注意这里sip/dip都被修改了</li>
<li>请求包到达rs， rs回复（sip rip，dip 200.200.200.1）</li>
<li>这个回复包的目的IP是VIP(不像NAT中是 cip)，所以LVS和RS不在一个vlan通过IP路由也能到达lvs</li>
<li>lvs修改sip为vip， dip为cip，修改后的回复包（sip 200.200.200.1，dip 200.200.200.2）发给client</li>
</ol>
<p><img src="/images/oss/94d55b926b5bb1573c4cab8353428712.png" alt="image.png"></p>
<p><strong>注意上图中绿色的进包和红色的出包他们的地址变化</strong></p>
<p>本来这个模型下都是正常的，但是为了Real Server能拿到client ip，也就是Real Server记录来源ip的时候希望记录的是client ip而不是LVS ip。这个时候LVS会将client ip放在tcp的options里面，然后在RealServer机器的内核里面将options中的client ip取出替换掉 lvs ip。所以Real Server上感知到的对端ip就是client ip。</p>
<p>回包的时候RealServer上的内核模块同样将目标地址从client ip改成lvs ip，同时将client ip放入options中。</p>
<h2 id="回到问题"><a href="#回到问题" class="headerlink" title="回到问题"></a>回到问题</h2><p>看完理论，再来分析这两个连接的行为</p>
<p>fulnat模式下连接经过lvs到达mysql后，mysql上看到的连接信息是，cip+port，也就是在MySQL上的连接</p>
<p><strong>lvs-ip:port -&gt; 10.112.61.163:3269  被修改成了 </strong>client-ip:61902 **-&gt; 10.112.61.163:3269</p>
<p>那么跟不走LVS的连接：</p>
<p><strong>client-ip:61902 -&gt;  10.112.61.163:3269 (直连) 完全重复了。</strong></p>
<p>MySQL端看到的两个连接四元组一模一样了：</p>
<blockquote>
<p>10.112.61.163:3269 -&gt; client-ip:61902 (走LVS，本来应该是lvs ip的，但是被替换成了client ip) </p>
<p>10.112.61.163:3269 -&gt; client-ip:61902 (直连) </p>
</blockquote>
<p>这个时候应用端看到的还是两个连接：</p>
<blockquote>
<p>client-ip:61902 -&gt; 10.141.16.0:3306 （走LVS） </p>
<p>client-ip:61902 -&gt;  10.112.61.163:3269 (直连) </p>
</blockquote>
<p>总结下，也就是这个连接经过LVS转换后在服务端（MYSQL）跟直连MySQL的连接四元组完全重复了，也就是MySQL会认为这两个连接就是同一个连接，所以必然出问题了。</p>
<p>实际两个连接建立的情况：</p>
<blockquote>
<p> 和mysqlserver的61902是04:22建起来的，和lvs的61902端口 是42:10建起来的，和lvs的61902建起来之后马上就出问题了</p>
</blockquote>
<h2 id="问题出现的条件"><a href="#问题出现的条件" class="headerlink" title="问题出现的条件"></a>问题出现的条件</h2><ul>
<li>fulnat模式的LVS，RS上装有slb_toa内核模块（RS上会将LVS ip还原成client ip）</li>
<li>client端正好重用一个相同的本地端口分别和RS以及LVS建立了两个连接</li>
</ul>
<p>这个时候这两个连接在MySQL端就会变成一个，然后两个连接的内容互串，必然导致rst</p>
<p>这个问题还挺有意思的，估计没几个程序员一辈子能碰上一次。推荐另外一个好玩的连接：<a href="/2020/07/01/如何创建一个自己连自己的TCP连接/">如何创建一个自己连自己的TCP连接</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="/2019/06/20/就是要你懂负载均衡--lvs和转发模式/">就是要你懂负载均衡–lvs和转发模式</a></p>
<p><a href="https://idea.popcount.org/2014-04-03-bind-before-connect/" target="_blank" rel="external">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>
<p><a href="https://github.com/kubernetes/kubernetes/issues/81775" target="_blank" rel="external">no route to host</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/127099484" target="_blank" rel="external">另一种形式的tcp连接互串，新连接重用了time_wait的port，导致命中lvs内核表中的维护的旧连接发给了老的realserver</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/18/MySQL针对秒杀场景的优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/18/MySQL针对秒杀场景的优化/" itemprop="url">MySQL针对秒杀场景的优化</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-18T07:30:03+08:00">
                2020-11-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MySQL针对秒杀场景的优化"><a href="#MySQL针对秒杀场景的优化" class="headerlink" title="MySQL针对秒杀场景的优化"></a>MySQL针对秒杀场景的优化</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>对于秒杀热点场景，MySQL官方版本500 TPS每秒，在对MySQL优化前只能用redis来扛，redis没有事务能力，比如一个item下有多个sku就搞不定了。同时在前端搞限流、答题等让秒杀流量控制在可以承受的范围内。</p>
<h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>对于秒杀热点场景，MySQL官方版本扣减只能做到 500 TPS每秒，扛不住大促的流量，需要优化。从控制并发量将500优化到1400，再通过新语法来消除网络rtt对加锁时间的控制这样达到了 4000 TPS。最后合并多个扣减到一个，累积比如10ms提交，能将TPS 能升到4万以上这个能力。</p>
<h3 id="排队控制并发"><a href="#排队控制并发" class="headerlink" title="排队控制并发"></a>排队控制并发</h3><p>拍减模式在整个交易过程中只有一次扣减交互，所以是不需要付款减库存那样的判重逻辑，就是说，拍减的减库存sql只有一条update语句就搞定了。而付减有两条，一条insert判重+一条update减库存（双十一拍减接口在高峰的rt约为8ms，而付减接口在高峰的rt约为15ms）；</p>
<p>其次，当大量请求（线程）落到mysql的同一条记录上进行减库存时，线程之间会存在竞争关系，因为要争夺InnoDB的行锁，当一个线程获得了行锁，其他并发线程就只能等待（InnoDB内部还有死锁检测等机制会严重影响性能），当并发度越高时，等待的线程就越多，此时tps会急剧下降，rt会飙升，性能就不能满足要求了。那如何减少锁竞争？答案是：排队！库存中心从几个层面做了排队策略。</p>
<p>首先，在应用端进行排队，因为很多商品都是有sku的，当sku库存变化时item的库存也要做相应变化，所以需要根据itemId来进行排队，相同itemId的减库存操作会进入串行化排队处理逻辑，不过应用端的排队只能做到单机内存排队，当应用服务器数量过多时，落到db的并发请求仍然很多，所以最好的办法是在db端也加上排队策略，今年库存中心db部署了两个的排队patch，一个叫“并发控制”，是做在InnoDB层的，另一个叫“queue on pk”，是做在mysql的server层的，两个patch各有优缺点，前者不需要应用修改代码，db自动判断，后者需要应用程序写特殊的sql hint，前者控制的全局的sql，后者是根据hint来控制指定sql，两个patch的本质和应用端的排队逻辑是一致的，具体实现不同。双十一库存中心使用的是“并发控制”的patch。</p>
<blockquote>
<p>2013年的单减库存TPS最高记录是1381次每秒。</p>
</blockquote>
<p>对于秒杀热点场景，官方版本500tps每秒，问题在于同时涌入的请求太多，每次取锁都要检查其它等锁的线程（防止死锁），这个线程队列太长的话导致这个检查时间太长； 继续在前面增加能够进入到后面的并发数的控制，通过增加线程池、控制并发能到1400（no deadlock list check）；</p>
<blockquote>
<p><strong>热点更新下的死锁检测(</strong>no deadlock list check<strong>)</strong></p>
<p>由于热点更新是分布式的客户端并发的向单点的数据库进行了并行更新一条记录，到数据库最后是把并行的线程转行成串行的操作。但在串行操作的时候，由于对同一记录的锁申请列表过大，死锁检测的机制在检测锁队列的时候，反而拖慢了每一个更新。</p>
</blockquote>
<h3 id="缩短锁时间"><a href="#缩短锁时间" class="headerlink" title="缩短锁时间"></a>缩短锁时间</h3><p>接下来的问题在于一个事务中有多条语句（最少也有一个update+一个commit），这样update(减库存，开始锁表），走网络，查询结果（走网络），commit，两次跨网络调用导致update锁行比较久，于是可以新造一个语法 select update一次搞定，继续优化 select update commit_on_success_or_fail_rollback，将所有操作一次网络操作全部搞定，能到4000；</p>
<p>比如库存扣减的业务逻辑可以简化为下面这个事务:</p>
<p>（1）begin;</p>
<p>（2）insert 交易流水表; – 交易流水对账</p>
<p>（3）update 库存明细表 where id in (sku_id，item_id);</p>
<p>（4）select 库存明细表;</p>
<p>（5）commit</p>
<p><img src="/images/951413iMgBlog/TB1yvFqOpXXX.png" alt="Snip20161116_88.png"></p>
<p>SQL case：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">4059550 Query   <span class="keyword">SET</span> autocommit=<span class="number">0</span></div><div class="line"><span class="number">4059550</span> <span class="keyword">Query</span>   <span class="keyword">update</span> ROLLBACK_ON_FAIL TARGET_AFFECT_ROW <span class="number">1</span> trade <span class="keyword">set</span> <span class="keyword">version</span> = <span class="keyword">version</span>+<span class="number">3</span> ,gmt_modified = <span class="keyword">now</span>()    ,           optype = <span class="number">2</span>          ,      feature = <span class="string">';abc;'</span>  <span class="keyword">where</span> sub_biz_order_id = <span class="string">'15'</span> <span class="keyword">and</span> biz_order_type = <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">id</span> = <span class="number">5</span> <span class="keyword">and</span> ti_id = <span class="number">1</span> <span class="keyword">and</span>      optype = <span class="number">3</span>          <span class="keyword">and</span>      root_id = <span class="number">11</span></div><div class="line"><span class="number">4059550</span> <span class="keyword">Query</span>   <span class="keyword">select</span>      <span class="keyword">id</span>,*     <span class="keyword">from</span>   <span class="keyword">update</span> COMMIT_ON_SUCCESS ROLLBACK_ON_FAIL TARGET_AFFECT_ROW <span class="number">1</span> invetory   <span class="keyword">set</span>                                           withholding_quantity = withholding_quantity + <span class="number">-1</span>,  flag=flag &amp;~ (<span class="number">1</span>&lt;&lt;<span class="number">10</span>) &amp;~ (<span class="number">1</span>&lt;&lt;<span class="number">11</span>) , <span class="keyword">version</span>=<span class="keyword">version</span>+<span class="number">3</span>,gmt_modified = <span class="keyword">now</span>()         <span class="keyword">WHERE</span>  root_id = <span class="number">11</span> <span class="keyword">and</span> <span class="keyword">status</span> = <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">in</span>    (     <span class="number">1</span>    )  <span class="keyword">and</span> (withholding_quantity + <span class="number">-1</span>) &gt;= <span class="number">0</span></div><div class="line"><span class="number">4059550</span> <span class="keyword">Query</span>   <span class="keyword">commit</span></div></pre></td></tr></table></figure>
<h3 id="批量提交"><a href="#批量提交" class="headerlink" title="批量提交"></a>批量提交</h3><p>其主要的核心思想是：针对应用层SQL做轻量化改造，带上”热点行SQL”的hint，当这种SQL进入内核后，在内存中维护一个hash表，将主键或唯一键相同的请求(一般也就是同一商品id)hash到同一个地方做请求的合并，经过一段时间后(默认100us)统一提交，从而实现了将串行处理变成了批处理，让每个热点行更新请求并不需要都去扫描和更新btree。</p>
<ol>
<li>热点的自动识别:前面已经讲过了，库存的扣减SQL都会有commit on success标记。mysql内部分为普通通道和热点扣减通道。普通通道里是正常的事务。热点通道里收集带有commit on success标记的事务。在一定的时间区间段内(0.1ms)，将收集到的热点按照主键或者唯一键进行hash; hash到同一个桶中为相同的sku; 分批组提交这0.1ms收集到的热点商品。</li>
<li>轮询处理: 第一批进行提交时，第二批进行收集； 当第一批完成了提交开始收集时，第二批就可以进行提交了。不断轮询，提高效率</li>
</ol>
<p>通过内存合并库存减操作，干到100000（每个减库存操作生成一条独立的update binlog，不影响其他业务2016年双11），实际这里还可以调整批提交时间间隔来进一步提升扣减QPS</p>
<p><img src="/images/951413iMgBlog/TB1I_BvOpXXXXasXVXXXXXXXXXX.png" alt="Snip20161116_87.png"></p>
<p>超卖：付款减库存会超卖，拍减库存要防止恶意拍不付款。拍减的话可以通过增加SQL新语法来进一步优化DB响应(select update)</p>
<p>innodb_buffer_pool_instance: 将buffer pool 分成几个（hash），避免高并发修改的时候一个大锁mutex导致性能不高</p>
<h3 id="业务优化"><a href="#业务优化" class="headerlink" title="业务优化"></a>业务优化</h3><p>延迟扣减item，一般一个item下会有多个sku（比如 iPhone14 不同的颜色、配置就是一个不同的sku），而库存会有总库存（item），也有sku 库存，sku库存加起来就是item库存</p>
<p>导致扣减的时候 item库存更热</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/17/MySQL线程池导致的延时卡顿排查/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/17/MySQL线程池导致的延时卡顿排查/" itemprop="url">MySQL线程池导致的延时卡顿排查</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-17T07:30:03+08:00">
                2020-11-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MySQL-线程池导致的延时卡顿排查"><a href="#MySQL-线程池导致的延时卡顿排查" class="headerlink" title="MySQL 线程池导致的延时卡顿排查"></a>MySQL 线程池导致的延时卡顿排查</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>简单小表的主键点查SQL，单条执行很快，但是放在业务端，有时快有时慢，取了一条慢sql，在MySQL侧查看，执行时间很短。</p>
<p>通过Tomcat业务端监控有显示慢SQL，取slow.log里显示有12秒执行时间的SQL，但是这次12秒的执行在MySQL上记录下来的执行时间都不到1ms。</p>
<p>所在节点的tsar监控没有异常，Tomcat manager监控上没有fgc，Tomcat实例规格 16C32g<em>8, MySQL  32c128g  </em>32 。</p>
<p>5-28号现象复现，从监控图上CPU、内存、网络都没发现异常，MySQL侧查到的SQL依然执行很快，Tomcat侧记录12S执行时间，当时Tomcat节点的网络流量、CPU压力都很小。</p>
<p>所以客户怀疑Tomcat有问题或者Tomcat上的代码写得有问题导致了这个问题，需要排查和解决掉。</p>
<p>接下来我们会先分析这个问题出现的原因，然后会分析这类问题的共性同时拓展到其它场景下的类似问题。</p>
<h2 id="Tomcat上抓包分析"><a href="#Tomcat上抓包分析" class="headerlink" title="Tomcat上抓包分析"></a>Tomcat上抓包分析</h2><h3 id="慢的连接"><a href="#慢的连接" class="headerlink" title="慢的连接"></a>慢的连接</h3><p>经过抓包分析发现在慢的连接上，所有操作都很慢，包括set 命令，慢的时间主要分布在3秒以上，1-3秒的慢查询比较少，这明显不太符合分布规律。并且目前看慢查询基本都发生在MySQL的0库的部分连接上（后端有一堆MySQL组成的集群），下面抓包的4637端口是MySQL的服务端口：</p>
<p><img src="/images/oss/b8ed95b7081ee80eb23465ee0e9acc74.png" alt="image.png"></p>
<p>以上两个连接都很慢，对应的慢查询在MySQL里面记录很快。</p>
<p>慢的SQL的response按时间排序基本都在3秒以上：</p>
<p><img src="/images/oss/36a2a60f64011bc73fee06c291bcd79f.png" alt="image.png" style="zoom:67%;"></p>
<p>或者只看response time 排序，中间几个1秒多的都是 Insert语句。也就是1秒到3秒之间的没有，主要是3秒以上的查询</p>
<p><img src="/images/oss/07146ff29534a1070adbdb8cedd280c9.png" alt="image.png" style="zoom:67%;"></p>
<h3 id="快的连接"><a href="#快的连接" class="headerlink" title="快的连接"></a>快的连接</h3><p>同样一个查询SQL，发到同一个MySQL上(4637端口)，下面的连接上的所有操作都很快，下面是两个快的连接上的执行截图</p>
<p><img src="/images/oss/d129dfe1a50b182f4d100ac7147f9099.png" alt="image.png"></p>
<p>别的MySQL上都比较快，比如5556分片上的所有response RT排序，只有偶尔极个别的慢SQL</p>
<p><img src="/images/oss/01531d138b9bc8dafda76b7c8bbb5bc9.png" alt="image.png"></p>
<h2 id="MySQL相关参数"><a href="#MySQL相关参数" class="headerlink" title="MySQL相关参数"></a>MySQL相关参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">mysql&gt; show variables like &apos;%thread%&apos;;</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">| Variable_name                              | Value           |</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">| innodb_purge_threads                       | 1               |</div><div class="line">| innodb_MySQL_thread_extra_concurrency        | 0               |</div><div class="line">| innodb_read_io_threads                     | 16              |</div><div class="line">| innodb_thread_concurrency                  | 0               |</div><div class="line">| innodb_thread_sleep_delay                  | 10000           |</div><div class="line">| innodb_write_io_threads                    | 16              |</div><div class="line">| max_delayed_threads                        | 20              |</div><div class="line">| max_insert_delayed_threads                 | 20              |</div><div class="line">| myisam_repair_threads                      | 1               |</div><div class="line">| performance_schema_max_thread_classes      | 50              |</div><div class="line">| performance_schema_max_thread_instances    | -1              |</div><div class="line">| pseudo_thread_id                           | 12882624        |</div><div class="line">| MySQL_is_dump_thread                         | OFF             |</div><div class="line">| MySQL_threads_running_ctl_mode               | SELECTS         |</div><div class="line">| MySQL_threads_running_high_watermark         | 50000           |</div><div class="line">| rocksdb_enable_thread_tracking             | OFF             |</div><div class="line">| rocksdb_enable_write_thread_adaptive_yield | OFF             |</div><div class="line">| rocksdb_signal_drop_index_thread           | OFF             |</div><div class="line">| thread_cache_size                          | 100             |</div><div class="line">| thread_concurrency                         | 10              |</div><div class="line">| thread_handling                            | pool-of-threads |</div><div class="line">| thread_pool_high_prio_mode                 | transactions    |</div><div class="line">| thread_pool_high_prio_tickets              | 4294967295      |</div><div class="line">| thread_pool_idle_timeout                   | 60              |</div><div class="line">| thread_pool_max_threads                    | 100000          |</div><div class="line">| thread_pool_oversubscribe                  | 10              |</div><div class="line">| thread_pool_size                           | 96              |</div><div class="line">| thread_pool_stall_limit                    | 30              |</div><div class="line">| thread_stack                               | 262144          |</div><div class="line">| threadpool_workaround_epoll_bug            | OFF             |</div><div class="line">| tokudb_cachetable_pool_threads             | 0               |</div><div class="line">| tokudb_checkpoint_pool_threads             | 0               |</div><div class="line">| tokudb_client_pool_threads                 | 0               |</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">33 rows in set (0.00 sec)</div></pre></td></tr></table></figure>
<h2 id="综上结论"><a href="#综上结论" class="headerlink" title="综上结论"></a>综上结论</h2><p>问题原因跟MySQL线程池比较相关，慢的连接总是慢，快的连接总是快。需要到MySQL Server下排查线程池相关参数。</p>
<p>同一个慢的连接上的回包，所有 ack 就很快（OS直接回，不需要进到MySQL），但是set就很慢，基本理解只要进到MySQL的就慢了，所以排除了网络原因（流量本身也很小，也没看到乱序、丢包之类的）</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>18点的时候将4637端口上的MySQL thread_pool_oversubscribe 从10调整到20后，基本没有慢查询了：</p>
<p><img src="/images/oss/92069e7521368e4d2519b3b861cc7faa.png" alt="image.png" style="zoom:50%;"></p>
<p>当时从MySQL的观察来看，并发压力很小，很难抓到running thread比较高的情况（update: 可能是任务积压在队列中，只是96个thread pool中的一个thread全部running，导致整体running不高）</p>
<p>MySQL记录的执行时间是指SQL语句开始解析后统计，中间的等锁、等Worker都不会记录在执行时间中，所以当时对应的SQL在MySQL日志记录中很快。</p>
<p>thread_pool_stall_limit 会控制一个SQL过长时间（默认60ms）占用线程，如果出现stall_limit就放更多的SQL进入到thread pool中直到达到thread_pool_oversubscribe个</p>
<blockquote>
<p>The thread_pool_stall_limit affects executing statements. The value is the amount of time a statement has to finish after starting to execute before it becomes defined as stalled, at which point the thread pool permits the thread group to begin executing another statement. The value is measured in 10 millisecond units, so the default of 6 means 60ms. Short wait values permit threads to start more quickly. Short values are also better for avoiding deadlock situations. Long wait values are useful for workloads that include long-running statements, to avoid starting too many new statements while the current ones execute.</p>
</blockquote>
<h2 id="Thread-Pool原理"><a href="#Thread-Pool原理" class="headerlink" title="Thread Pool原理"></a>Thread Pool原理</h2><p><img src="/images/oss/6fbe1c10f07dd1c26eba0c0e804fa9a8.png" alt="image.png"></p>
<p>MySQL 原有线程调度方式有每个连接一个线程(one-thread-per-connection)和所有连接一个线程（no-threads）。</p>
<p>no-threads一般用于调试，生产环境一般用one-thread-per-connection方式。one-thread-per-connection 适合于低并发长连接的环境，而在高并发或大量短连接环境下，大量创建和销毁线程，以及线程上下文切换，会严重影响性能。另外 one-thread-per-connection 对于大量连接数扩展也会影响性能。</p>
<p>为了解决上述问题，MariaDB、Percona、Aliyun RDS、Oracle MySQL 都推出了线程池方案，它们的实现方式大体相似，这里以 Percona 为例来简略介绍实现原理，同时会介绍我们在其基础上的一些改进。</p>
<p>线程池由一系列 worker 线程组成，这些worker线程被分为<code>thread_pool_size</code>个group。用户的连接按 round-robin 的方式映射到相应的group 中，一个连接可以由一个group中的一个或多个worker线程来处理。</p>
<p>thread_pool_oversubscribe  一个group中活跃线程和等待中的线程超过<code>thread_pool_oversubscribe</code>时，不会创建新的线程。 此参数可以控制系统的并发数，同时可以防止调度上的死锁，考虑如下情况，A、B、C三个事务，A、B 需等待C提交。A、B先得到调度，同时活跃线程数达到了<code>thread_pool_max_threads</code>上限，随后C继续执行提交，此时已经没有线程来处理C提交，从而导致A、B一直等待。<code>thread_pool_oversubscribe</code>控制group中活跃线程和等待中的线程总数，从而防止了上述情况。</p>
<p><strong>MySQL Thread Pool之所以分成多个小的Thread Group Pool而不是一个大的Pool，是为了分解锁（每个group中都有队列，队列需要加锁。类似ConcurrentHashMap提高并发的原理），提高并发效率。</strong></p>
<p>group中又有多个队列，用来区分优先级的，事务中的语句会放到高优先队列（非事务语句和autocommit 都会在低优先队列）；等待太久的SQL也会挪到高优先队列，防止饿死。</p>
<p>比如启用Thread Pool后，如果出现多个慢查询，容易导致拨测类请求超时，进而出现Server异常的判断（类似Nginx 边缘触发问题）；或者某个group满后导致慢查询和拨测失败之类的问题</p>
<h3 id="thread-pool-size过小的案例"><a href="#thread-pool-size过小的案例" class="headerlink" title="thread_pool_size过小的案例"></a>thread_pool_size过小的案例</h3><p>应用出现大量1秒超时报错：</p>
<p><img src="/images/951413iMgBlog/52dbeb1c1058e6dbff0a790b4b4ba477.png" alt="image.png"></p>
<p><img src="/images/951413iMgBlog/image-20211104130625676.png" alt="image-20211104130625676"></p>
<p>分析代码，这个Druid报错堆栈是数据库连接池在创建到MySQL的连接后或者从连接池取一个连接给业务使用前会发送一个ping来验证下连接是否有效，有效后才给应用使用。说明TCP连接创建成功，但是MySQL 超过一秒钟都没有响应这个 ping，说明 MySQL处理指令缓慢。</p>
<p>继续分析MySQL的参数：</p>
<p><img src="/images/oss/8987545cc311fdd3ae232aee8c3f855a.png" alt="image.png"></p>
<p>可以看到thread_pool_size是1，太小了，将所有MySQL线程都放到一个buffer里面来抢锁，锁冲突的概率太高。调整到16后可以明显看到MySQL的RT从原来的12ms下降到了3ms不到，整个QPS大概有8%左右的提升。这是因为pool size为1的话所有sql都在一个队列里面，多个worker thread加锁等待比较严重，导致rt延迟增加。</p>
<p><img src="/images/oss/114b5b71468b33128e76129bbc7fb8f4.png" alt="image.png"></p>
<p>这个问题发现是因为压力一上来的时候要创建大量新的连接，这些连结创建后会去验证连接的有效性，也就是Druid给MySQL发一个ping指令，一般都很快，同时Druid对这个valid操作设置了1秒的超时时间，从实际看到大量超时异常堆栈，从而发现MySQL内部响应有问题。</p>
<h3 id="MySQL-ping和MySQL协议相关知识"><a href="#MySQL-ping和MySQL协议相关知识" class="headerlink" title="MySQL ping和MySQL协议相关知识"></a>MySQL ping和MySQL协议相关知识</h3><blockquote>
<p><a href="https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-usagenotes-j2ee-concepts-connection-pooling.html#idm47306928802368" target="_blank" rel="external">Ping</a> use the JDBC method <a href="http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#isValid(int" target="_blank" rel="external">Connection.isValid(int timeoutInSecs)</a>). Digging into the MySQL Connector/J source, the actual implementation uses com.mysql.jdbc.ConnectionImpl.pingInternal() to send a simple ping packet to the DB and returns true as long as a valid response is returned.</p>
</blockquote>
<p>MySQL ping protocol是发送了一个 <code>0e</code> 的byte标识给Server，整个包加上2byte的Packet Length（内容为：1），2byte的Packet Number（内容为：0），总长度为5 byte。Druid、DRDS默认都会testOnBorrow，所以每个连接使用前都会先做ping。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">public class MySQLPingPacket implements CommandPacket &#123;</div><div class="line">    private final WriteBuffer buffer = new WriteBuffer();</div><div class="line">    public MySQLPingPacket() &#123;</div><div class="line">        buffer.writeByte((byte) 0x0e);</div><div class="line">    &#125;</div><div class="line">    public int send(final OutputStream os) throws IOException &#123;</div><div class="line">        os.write(buffer.getLengthWithPacketSeq((byte) 0)); // Packet Number</div><div class="line">        os.write(buffer.getBuffer(),0,buffer.getLength()); // Packet Length 固定为1</div><div class="line">        os.flush();</div><div class="line">        return 0;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="/images/oss/7cf291546a167b0ca6a017e98db5a821.png" alt="image.png"></p>
<p>也就是一个TCP包中的Payload为 MySQL协议中的内容长度 + 4（Packet Length+Packet Number）。</p>
<h2 id="线程池卡死案例：show-stats导致集群3406监控卡死"><a href="#线程池卡死案例：show-stats导致集群3406监控卡死" class="headerlink" title="线程池卡死案例：show stats导致集群3406监控卡死"></a>线程池卡死案例：show stats导致集群3406监控卡死</h2><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>应用用于获取监控信息的端口 3406卡死，监控脚本无法连接上3406，监控没有数据（需要从3406采集）、DDL操作、show processlist、show stats操作卡死（需要跟整个集群的3406端口同步）。</p>
<p>通过jstack看到应用进程的3406端口线程池都是这样: </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">"ManagerExecutor-1-thread-1" #47 daemon prio=5 os_prio=0 tid=0x00007fe924004000 nid=0x15c runnable [0x00007fe9034f4000]</div><div class="line">   java.lang.Thread.State: RUNNABLE</div><div class="line">    at java.net.SocketInputStream.socketRead0(Native Method)</div><div class="line">    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</div><div class="line">    at java.net.SocketInputStream.read(SocketInputStream.java:171)</div><div class="line">    at java.net.SocketInputStream.read(SocketInputStream.java:141)</div><div class="line">    at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">    at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">    at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">    - locked &lt;0x0000000722538b60&gt; (a com.mysql.jdbc.util.ReadAheadInputStream)</div><div class="line">    at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3005)</div><div class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3466)</div><div class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3456)</div><div class="line">    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3897)</div><div class="line">    at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2524)</div><div class="line">    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2677)</div><div class="line">    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2545)</div><div class="line">    - locked &lt;0x00000007432e19c8&gt; (a com.mysql.jdbc.JDBC4Connection)</div><div class="line">    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2503)</div><div class="line">    at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1369)</div><div class="line">    - locked &lt;0x00000007432e19c8&gt; (a com.mysql.jdbc.JDBC4Connection)</div><div class="line">    at com.alibaba.druid.pool.ValidConnectionCheckerAdapter.isValidConnection(ValidConnectionCheckerAdapter.java:44)</div><div class="line">    at com.alibaba.druid.pool.DruidAbstractDataSource.testConnectionInternal(DruidAbstractDataSource.java:1298)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1057)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:997)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:987)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:103)</div><div class="line">    at com.taobao.tddl.atom.AbstractTAtomDataSource.getConnection(AbstractTAtomDataSource.java:32)</div><div class="line">    at com.alibaba.cobar.ClusterSyncManager$1.run(ClusterSyncManager.java:60)</div><div class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">    at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">    at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)</div><div class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)</div><div class="line">    at java.lang.Thread.run(Thread.java:882)</div></pre></td></tr></table></figure>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ol>
<li>用户监控采集数据通过访问3306端口上的show stats，这个show stats命令要访问集群下所有节点的3406端口来执行show stats，3406端口上是一个大小为8个的Manager 线程池在执行这些show stats命令，导致占满了manager线程池的8个线程，每个3306的show stats线程都在wait 所有节点3406上的子任务的返回</li>
<li>每个子任务的线程，都在等待向集群所有节点3406端口的manager建立连接，建连接后会先执行testValidatation操作验证连接的有效性，这个验证操作会执行SQL Query：select 1，这个query请求又要申请一个manager线程才能执行成功</li>
<li>默认isValidConnection操作没有超时时间，如果Manager线程池已满后需要等待至socketTimeout后才会返回，导致这里出现卡死，还不如快速返回错误，可以增加超时来改进</li>
</ol>
<p>从线程栈来说，就是出现了活锁</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul>
<li>增加manager线程池大小</li>
<li>代码逻辑上优化3406 jdbc连接池参数，修改jdbc默认的socketTimeout超时时间以及替换默认checker（一般增加一个1秒超时的checker）</li>
</ul>
<p>对于checker，参考druid的实现，com/alibaba/druid/pool/vendor/MySqlValidConnectionChecker.java：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//druid的MySqlValidConnectionChecker设定了valid超时时间为1秒</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValidConnection</span><span class="params">(Connection conn, String validateQuery, <span class="keyword">int</span> validationQueryTimeout)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (conn.isClosed()) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (usePingMethod) &#123;</div><div class="line">            <span class="keyword">if</span> (conn <span class="keyword">instanceof</span> DruidPooledConnection) &#123;</div><div class="line">                conn = ((DruidPooledConnection) conn).getConnection();</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (conn <span class="keyword">instanceof</span> ConnectionProxy) &#123;</div><div class="line">                conn = ((ConnectionProxy) conn).getRawObject();</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (clazz.isAssignableFrom(conn.getClass())) &#123;</div><div class="line">                <span class="keyword">if</span> (validationQueryTimeout &lt;= <span class="number">0</span>) &#123;</div><div class="line">                    validationQueryTimeout = DEFAULT_VALIDATION_QUERY_TIMEOUT;<span class="comment">// 默认值1ms</span></div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    ping.invoke(conn, <span class="keyword">true</span>, validationQueryTimeout * <span class="number">1000</span>); <span class="comment">//1秒</span></div><div class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</div><div class="line">                    Throwable cause = e.getCause();</div><div class="line">                    <span class="keyword">if</span> (cause <span class="keyword">instanceof</span> SQLException) &#123;</div><div class="line">                        <span class="keyword">throw</span> (SQLException) cause;</div><div class="line">                    &#125;</div><div class="line">                    <span class="keyword">throw</span> e;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String query = validateQuery;</div><div class="line">        <span class="keyword">if</span> (validateQuery == <span class="keyword">null</span> || validateQuery.isEmpty()) &#123;</div><div class="line">            query = DEFAULT_VALIDATION_QUERY;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Statement stmt = <span class="keyword">null</span>;</div><div class="line">        ResultSet rs = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            stmt = conn.createStatement();</div><div class="line">            <span class="keyword">if</span> (validationQueryTimeout &gt; <span class="number">0</span>) &#123;</div><div class="line">                stmt.setQueryTimeout(validationQueryTimeout);</div><div class="line">            &#125;</div><div class="line">            rs = stmt.executeQuery(query);</div><div class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">            JdbcUtils.close(rs);</div><div class="line">            JdbcUtils.close(stmt);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="comment">//使用如上validation</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String DEFAULT_DRUID_MYSQL_VALID_CONNECTION_CHECKERCLASS =</div><div class="line">        <span class="string">"com.alibaba.druid.pool.vendor.MySqlValidConnectionChecker"</span>;</div><div class="line"></div><div class="line"> String validConnnectionCheckerClassName =</div><div class="line">                    TAtomConstants.DEFAULT_DRUID_MYSQL_VALID_CONNECTION_CHECKERCLASS;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    Class.forName(validConnnectionCheckerClassName);</div><div class="line">                    localDruidDataSource.setValidConnectionCheckerClassName(validConnnectionCheckerClassName);</div></pre></td></tr></table></figure>
<p>这种线程池打满特别容易在分布式环境下出现，除了以上案例比如还有:</p>
<blockquote>
<p>drds-server线程池，接收一个逻辑SQL，如果需要查询1024分片的sort merge join，相当于派生了一批子任务，每个子任务占用一个线程，父任务等待子任务执行后返回数据。如果这样的逻辑SQL同时来一批并发，就会出现父任务都在等子任务，子任务又因为父任务占用了线程，导致子任务也在等着从线程池中取线程，这样父子任务就进入了死锁</p>
<p>比如并行执行的SQL MPP线程池也有这个问题，多个查询节点收到SQL，拆分出子任务做并行，互相等待资源</p>
</blockquote>
<h2 id="DRDS对分布式任务打挂线程池的优化"><a href="#DRDS对分布式任务打挂线程池的优化" class="headerlink" title="DRDS对分布式任务打挂线程池的优化"></a>DRDS对分布式任务打挂线程池的优化</h2><p>对如下这种案例：</p>
<blockquote>
<p>drds-server线程池，接收一个逻辑SQL，如果需要查询1024分片的sort merge join，相当于派生了1024个子任务，每个子任务占用一个线程，父任务等待子任务执行后返回数据。如果这样的逻辑SQL同时来一批并发，就会出现父任务都在等子任务，子任务又因为父任务占用了线程，导致子任务也在等着从线程池中取线程，这样父子任务就进入了死锁</p>
</blockquote>
<p>首先DRDS对执行SQL 的线程池分成了多个bucket，每个SQL只跑在一个bucket里面的线程上，同时通过滑动窗口向线程池提交任务数，来控制并发量，进而避免线程池的死锁、活锁问题。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ServerThreadPool <span class="title">create</span><span class="params">(String name, <span class="keyword">int</span> poolSize, <span class="keyword">int</span> deadLockCheckPeriod, <span class="keyword">int</span> bucketSize)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ServerThreadPool(name, poolSize, deadLockCheckPeriod, bucketSize); <span class="comment">//bucketSize可以设置</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ServerThreadPool</span><span class="params">(String poolName, <span class="keyword">int</span> poolSize, <span class="keyword">int</span> deadLockCheckPeriod, <span class="keyword">int</span> bucketSize)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.poolName = poolName;</div><div class="line">    <span class="keyword">this</span>.deadLockCheckPeriod = deadLockCheckPeriod;</div><div class="line"></div><div class="line">    <span class="keyword">this</span>.numBuckets = bucketSize;</div><div class="line">    <span class="keyword">this</span>.executorBuckets = <span class="keyword">new</span> ThreadPoolExecutor[bucketSize];</div><div class="line">    <span class="keyword">int</span> bucketPoolSize = poolSize / bucketSize; <span class="comment">//将整个pool分成多个bucket</span></div><div class="line">    <span class="keyword">this</span>.poolSize = bucketPoolSize;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bucketIndex = <span class="number">0</span>; bucketIndex &lt; bucketSize; bucketIndex++) &#123;</div><div class="line">        ThreadPoolExecutor executor = <span class="keyword">new</span> ThreadPoolExecutor(bucketPoolSize,</div><div class="line">            bucketPoolSize,</div><div class="line">            <span class="number">0L</span>,</div><div class="line">            TimeUnit.MILLISECONDS,</div><div class="line">            <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;(),</div><div class="line">            <span class="keyword">new</span> NamedThreadFactory(poolName + <span class="string">"-bucket-"</span> + bucketIndex, <span class="keyword">true</span>));</div><div class="line"></div><div class="line">        executorBuckets[bucketIndex] = executor;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">this</span>.lastCompletedTaskCountBuckets = <span class="keyword">new</span> <span class="keyword">long</span>[bucketSize];</div><div class="line">    <span class="comment">// for check thread</span></div><div class="line">    <span class="keyword">if</span> (deadLockCheckPeriod &gt; <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">this</span>.timer = <span class="keyword">new</span> Timer(SERVER_THREAD_POOL_TIME_CHECK, <span class="keyword">true</span>);</div><div class="line">        buildCheckTask();</div><div class="line">        <span class="keyword">this</span>.timer.scheduleAtFixedRate(checkTask, deadLockCheckPeriod, deadLockCheckPeriod);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>通过bucketSize将一个大的线程池分成多个小的线程池，每个SQL 控制跑在一个小的线程池中，这里和MySQL的thread_pool是同样的设计思路，当然MySQL 的thread_pool主要是为了改进大锁的问题。</p>
<p>另外DRDS上线程池拆分后性能也有提升：</p>
<p><img src="/images/951413iMgBlog/image-20211104163732499.png" alt="image-20211104163732499"></p>
<p>测试结果说明：(以全局线程池为基准，分别关注：关日志、分桶线程池、协程)</p>
<blockquote>
<ol>
<li>关日志，整体性能提升在20%左右 (8core最好成绩在6.4w qps)</li>
<li>协程，整体性能15%左右</li>
<li>关日志+协程，整体提升在35%左右 (8core最好成绩在7w qps)</li>
<li>分桶，整体性能提升在18%左右 </li>
<li>分桶+关日志，整体提升在39%左右 (8core最好成绩在7.4w qps)</li>
<li>分桶+协程，整体提升在36%左右</li>
<li>分桶+关日志+协程，整体提升在60%左右 (8core最好成绩在8.3w qps)</li>
</ol>
</blockquote>
<h3 id="线程池拆成多个bucket优化分析"><a href="#线程池拆成多个bucket优化分析" class="headerlink" title="线程池拆成多个bucket优化分析"></a>线程池拆成多个bucket优化分析</h3><p>拆分前锁主要是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">Started [lock] profiling</div><div class="line">--- Execution profile ---</div><div class="line">Total samples:         <span class="number">496</span></div><div class="line"></div><div class="line">Frame buffer usage:    <span class="number">0.0052</span>%</div><div class="line"></div><div class="line">--- <span class="number">352227700</span> ns (<span class="number">53.09</span>%), <span class="number">248</span> samples</div><div class="line">  [ <span class="number">0</span>] java.util.Properties</div><div class="line">  [ <span class="number">1</span>] java.util.Hashtable.get</div><div class="line">  [ <span class="number">2</span>] java.util.Properties.getProperty</div><div class="line">  [ <span class="number">3</span>] com.taobao.tddl.common.properties.SystemPropertiesHelper.getPropertyValue</div><div class="line">  [ <span class="number">4</span>] com.taobao.tddl.executor.MatrixExecutor.configMppExecutionContext</div><div class="line">  [ <span class="number">5</span>] com.taobao.tddl.executor.MatrixExecutor.optimize</div><div class="line">  [ <span class="number">6</span>] com.taobao.tddl.matrix.jdbc.TConnection.optimizeThenExecute</div><div class="line">  [ <span class="number">7</span>] com.taobao.tddl.matrix.jdbc.TConnection.executeSQL</div><div class="line">  [ <span class="number">8</span>] com.taobao.tddl.matrix.jdbc.TPreparedStatement.executeSQL</div><div class="line">  [ <span class="number">9</span>] com.taobao.tddl.matrix.jdbc.TStatement.executeInternal</div><div class="line">  [<span class="number">10</span>] com.taobao.tddl.matrix.jdbc.TPreparedStatement.execute</div><div class="line">  [<span class="number">11</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [<span class="number">12</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [<span class="number">13</span>] com.alibaba.cobar.server.ServerConnection$<span class="number">1</span>.run</div><div class="line">  [<span class="number">14</span>] com.taobao.tddl.common.utils.thread.FlowControlThreadPool$RunnableAdapter.run</div><div class="line">  [<span class="number">15</span>] java.util.concurrent.Executors$RunnableAdapter.call</div><div class="line">  [<span class="number">16</span>] java.util.concurrent.FutureTask.run</div><div class="line">  [<span class="number">17</span>] java.util.concurrent.ThreadPoolExecutor.runWorker</div><div class="line">  [<span class="number">18</span>] java.util.concurrent.ThreadPoolExecutor$Worker.run</div><div class="line">  [<span class="number">19</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">--- <span class="number">307781689</span> ns (<span class="number">46.39</span>%), <span class="number">243</span> samples</div><div class="line">  [ <span class="number">0</span>] java.util.Properties</div><div class="line">  [ <span class="number">1</span>] java.util.Hashtable.get</div><div class="line">  [ <span class="number">2</span>] java.util.Properties.getProperty</div><div class="line">  [ <span class="number">3</span>] com.taobao.tddl.common.properties.SystemPropertiesHelper.getPropertyValue</div><div class="line">  [ <span class="number">4</span>] com.taobao.tddl.config.ConfigDataMode.isDrdsMasterMode</div><div class="line">  [ <span class="number">5</span>] com.taobao.tddl.matrix.jdbc.TConnection.updatePlanManagementInfo</div><div class="line">  [ <span class="number">6</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [ <span class="number">7</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [ <span class="number">8</span>] com.alibaba.cobar.server.ServerConnection$<span class="number">1</span>.run</div><div class="line">  [ <span class="number">9</span>] com.taobao.tddl.common.utils.thread.FlowControlThreadPool$RunnableAdapter.run</div><div class="line">  [<span class="number">10</span>] java.util.concurrent.Executors$RunnableAdapter.call</div><div class="line">  [<span class="number">11</span>] java.util.concurrent.FutureTask.run</div><div class="line">  [<span class="number">12</span>] java.util.concurrent.ThreadPoolExecutor.runWorker</div><div class="line">  [<span class="number">13</span>] java.util.concurrent.ThreadPoolExecutor$Worker.run</div><div class="line">  [<span class="number">14</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">--- <span class="number">3451038</span> ns (<span class="number">0.52</span>%), <span class="number">4</span> samples</div><div class="line">  [ <span class="number">0</span>] java.lang.Object</div><div class="line">  [ <span class="number">1</span>] sun.nio.ch.SocketChannelImpl.ensureReadOpen</div><div class="line">  [ <span class="number">2</span>] sun.nio.ch.SocketChannelImpl.read</div><div class="line">  [ <span class="number">3</span>] com.alibaba.cobar.net.AbstractConnection.read</div><div class="line">  [ <span class="number">4</span>] com.alibaba.cobar.net.NIOReactor$R.read</div><div class="line">  [ <span class="number">5</span>] com.alibaba.cobar.net.NIOReactor$R.run</div><div class="line">  [ <span class="number">6</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">--- <span class="number">4143</span> ns (<span class="number">0.00</span>%), <span class="number">1</span> sample</div><div class="line">  [ <span class="number">0</span>] com.taobao.tddl.common.IdGenerator</div><div class="line">  [ <span class="number">1</span>] com.taobao.tddl.common.IdGenerator.nextId</div><div class="line">  [ <span class="number">2</span>] com.alibaba.cobar.server.ServerConnection.genTraceId</div><div class="line">  [ <span class="number">3</span>] com.alibaba.cobar.server.ServerQueryHandler.query</div><div class="line">  [ <span class="number">4</span>] com.alibaba.cobar.net.FrontendConnection.query</div><div class="line">  [ <span class="number">5</span>] com.alibaba.cobar.net.handler.FrontendCommandHandler.handle</div><div class="line">  [ <span class="number">6</span>] com.alibaba.cobar.net.FrontendConnection$<span class="number">1</span>.run</div><div class="line">  [ <span class="number">7</span>] java.util.concurrent.ThreadPoolExecutor.runWorker</div><div class="line">  [ <span class="number">8</span>] java.util.concurrent.ThreadPoolExecutor$Worker.run</div><div class="line">  [ <span class="number">9</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">          ns  percent  samples  top</div><div class="line">  ----------  -------  -------  ---</div><div class="line">   <span class="number">660009389</span>   <span class="number">99.48</span>%      <span class="number">491</span>  java.util.Properties</div><div class="line">     <span class="number">3451038</span>    <span class="number">0.52</span>%        <span class="number">4</span>  java.lang.Object</div><div class="line">        <span class="number">4143</span>    <span class="number">0.00</span>%        <span class="number">1</span>  com.taobao.tddl.common.IdGenerator</div></pre></td></tr></table></figure>
<p>com.taobao.tddl.matrix.jdbc.TConnection.optimizeThenExecute调用对应代码逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (InsertSplitter.needSplit(sql, policy, extraCmd)) &#123;</div><div class="line">             executionContext.setDoingBatchInsertBySpliter(<span class="keyword">true</span>);</div><div class="line">             InsertSplitter insertSplitter = <span class="keyword">new</span> InsertSplitter(executor);</div><div class="line">                        <span class="comment">// In batch insert, update transaction policy in writing</span></div><div class="line">                        <span class="comment">// broadcast table is also needed.</span></div><div class="line">     resultCursor = insertSplitter.execute(sql,executionContext,policy,</div><div class="line">         (String insertSql) -&gt; optimizeThenExecute(insertSql, executionContext,trxPolicyModified));</div><div class="line">                    &#125; <span class="keyword">else</span> &#123;</div><div class="line">       resultCursor = optimizeThenExecute(sql, executionContext,trxPolicyModified);</div><div class="line">                    &#125;</div><div class="line">                    </div><div class="line"> 最终会访问到：</div><div class="line"> <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configMppExecutionContext</span><span class="params">(ExecutionContext executionContext)</span> </span>&#123;</div><div class="line"></div><div class="line">        String instRole = (String) SystemPropertiesHelper.getPropertyValue(SystemPropertiesHelper.INST_ROLE);</div><div class="line">        SqlType sqlType = executionContext.getSqlType();</div><div class="line">        </div><div class="line"> 相当于执行每个SQL都要加锁访问HashMap(SystemPropertiesHelper.getPropertyValue)，这里排队比较厉害</div></pre></td></tr></table></figure>
<p>实际以上测试结果显示bucket对性能有提升这么大是不对的，刚好这个版本把对HashMap的访问去掉了，这才是提升的主要原因，当然如果线程池入队出队有等锁的话改成多个肯定是有帮助的，但是从等锁观察是没有这个问题的。</p>
<p>在这个代码基础上将bucket改成1，在4core机器下经过反复对比测试性能基本没有明显的差异，可能core越多这个问题会更明显些。总结</p>
<p>回到最开始部分查询卡顿这个问题，本质在于 MySQL线程池开启后，因为会将多个连接分配在一个池子中共享这个池子中的几个线程。导致一个池子中的线程特别慢的时候会影响这个池子中所有的查询都会卡顿。即使别的池子很空闲也不会将任务调度过去。</p>
<p>MySQL线程池设计成多个池子（Group）的原因是为了将任务队列拆成多个，这样每个池子中的线程只是内部竞争锁，跟其他池子不冲突，类似ConcurrentHashmap的实现，当然这个设计带来的问题就是多个池子中的任务不能均衡了。</p>
<p>同时从案例我们也可以清楚地看到这个池子太小会造成锁冲突严重的卡顿，池子太大（每个池子中的线程数量就少）容易造成等线程的卡顿。</p>
<p><strong>类似地这个问题也会出现在Nginx的多worker中，一旦一个连接分发到了某个worker，就会一直在这个worker上处理，如果这个worker上的某个连接有一些慢操作，会导致这个worker上的其它连接的所有操作都受到影响，特别是会影响一些探活任务的误判。</strong>Nginx的worker这么设计也是为了将单worker绑定到固定的cpu，然后避免多核之间的上下文切换。</p>
<p>如果池子卡顿后，调用方有快速fail，比如druid的MySqlValidConnectionChecker，那么调用方从堆栈很快能发现这个问题，如果没有异常一直死等的话对问题的排查不是很友好。</p>
<p>另外可以看到分布式环境下死锁、活锁还是很容易产生的，想要一次性提前设计好比较难，需要不断踩坑爬坑。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.atatech.org/articles/36343" target="_blank" rel="external">记一次诡异的数据库故障的排查过程</a></p>
<p><a href="http://mysql.taobao.org/monthly/2016/02/09/" target="_blank" rel="external">http://mysql.taobao.org/monthly/2016/02/09/</a></p>
<p><a href="https://dbaplus.cn/news-11-1989-1.html" target="_blank" rel="external">https://dbaplus.cn/news-11-1989-1.html</a></p>
<p><a href="https://kb.aliyun-inc.com/repo/921/article?id=G71264" target="_blank" rel="external">慢查询触发kill后导致集群卡死</a></p>
<p><a href="https://kb.aliyun-inc.com/repo/921/article?id=G56753" target="_blank" rel="external">青海湖、天津医保 RDS线程池过小导致DRDS查询卡顿问题排查 </a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--pagecache/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--pagecache/" itemprop="url">Linux内存--PageCache</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–PageCache"><a href="#Linux内存–PageCache" class="headerlink" title="Linux内存–PageCache"></a>Linux内存–PageCache</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="read-write"><a href="#read-write" class="headerlink" title="read/write"></a>read/write</h2><p><code>read(2)/write(2)</code> 是 Linux 系统中最基本的 I/O 读写系统调用，我们开发操作 I/O 的程序时必定会接触到它们，而在这两个系统调用和真实的磁盘读写之间存在一层称为 <code>Kernel buffer cache</code> 的缓冲区缓存。在 Linux 中 I/O 缓存其实可以细分为两个：<code>Page Cache</code> 和 <code>Buffer Cache</code>，这两个其实是一体两面，共同组成了 Linux 的内核缓冲区（Kernel Buffer Cache），Page Cache 是在应用程序读写文件的过程中产生的：</p>
<ul>
<li><strong>读磁盘</strong>：内核会先检查 <code>Page Cache</code> 里是不是已经缓存了这个数据，若是，直接从这个内存缓冲区里读取返回，若否，则穿透到磁盘去读取，然后再缓存在 <code>Page Cache</code> 里，以备下次缓存命中；</li>
<li><strong>写磁盘</strong>：内核直接把数据写入 <code>Page Cache</code>，并把对应的页标记为 dirty，添加到 dirty list 里，然后就直接返回，内核会定期把 dirty list 的页缓存 flush 到磁盘，保证页缓存和磁盘的最终一致性。</li>
</ul>
<p>在 Linux 还不支持虚拟内存技术之前，还没有页的概念，因此 <code>Buffer Cache</code> 是基于操作系统读写磁盘的最小单位 – 块（block）来进行的，所有的磁盘块操作都是通过 <code>Buffer Cache</code> 来加速，<strong>Linux 引入虚拟内存的机制来管理内存后，页成为虚拟内存管理的最小单位</strong>，因此也引入了 <code>Page Cache</code> 来缓存 Linux 文件内容，主要用来作为文件系统上的文件数据的缓存，提升读写性能，常见的是针对文件的 <code>read()/write()</code> 操作，另外也包括了通过 <code>mmap()</code> 映射之后的块设备，也就是说，事实上 Page Cache 负责了大部分的块设备文件的缓存工作。而 <code>Buffer Cache</code> 用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。</p>
<p>在 Linux 2.4 版本之后，kernel 就将两者进行了统一，<code>Buffer Cache</code> 不再以独立的形式存在，而是以融合的方式存在于 <code>Page Cache</code> 中</p>
<p><img src="/images/oss/cd1b3a9bebaf1e7219904fd537191cde.png" alt=""></p>
<p>融合之后就可以统一操作 <code>Page Cache</code> 和 <code>Buffer Cache</code>：处理文件 I/O 缓存交给 <code>Page Cache</code>，而当底层 RAW device 刷新数据时以 <code>Buffer Cache</code> 的块单位来实际处理。</p>
<h2 id="pagecache-的产生和释放"><a href="#pagecache-的产生和释放" class="headerlink" title="pagecache 的产生和释放"></a>pagecache 的产生和释放</h2><ul>
<li>标准 I/O 是写的 (write(2)) 用户缓冲区 (Userpace Page 对应的内存)，<strong>然后再将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)</strong>；如果是读的 (read(2)) 话则是先从内核缓冲区拷贝到用户缓冲区，再从用户缓冲区读数据，也就是 buffer 和文件内容不存在任何映射关系。</li>
<li>对于存储映射 I/O（Memory-Mapped I/O） 而言，则是直接将 Pagecache Page 给映射到用户地址空间，用户直接读写 Pagecache Page 中内容，效率相对标准IO更高一些</li>
</ul>
<p><img src="/images/oss/51bf36aa14dc01e7ad309c1bb9d252e9.png" alt="image.png" style="zoom: 20%;"></p>
<p>当 <strong>将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)</strong> 最容易发生缺页中断，OS需要先分配Page（应用感知到的就是卡顿了）</p>
<p><img src="/images/oss/d62ea00662f8342b7df3aab6b28e4cbb.png" alt="img.png" style="zoom: 25%;">  </p>
<ul>
<li>Page Cache 是在应用程序读写文件的过程中产生的，所以在读写文件之前你需要留意是否还有足够的内存来分配 Page Cache；</li>
<li>Page Cache 中的脏页很容易引起问题，你要重点注意这一块；</li>
<li>在系统可用内存不足的时候就会回收 Page Cache 来释放出来内存，可以通过 sar 或者 /proc/vmstat 来观察这个行为从而更好的判断问题是否跟回收有关</li>
</ul>
<p>缺页后kswapd在短时间内回收不了足够多的 free 内存，或kswapd 还没有触发执行，操作系统就会进行内存页直接回收。这个过程中，应用会进行自旋等待直到回收的完成，从而产生巨大的延迟。</p>
<p><img src="/images/oss/0a5cdeb75b7dee2068254cd4b7fe254d.png" style="zoom:50%;"></p>
<p>如果page被swapped，那么恢复进内存的过程也对延迟有影响，当匿名内存页被回收后，如果下次再访问就会产生IO的延迟。</p>
<p><img src="/images/oss/740b95056dace8ae6fb3b8f58d91572e.png" style="zoom:50%;"></p>
<h3 id="min-和-low的区别"><a href="#min-和-low的区别" class="headerlink" title="min 和 low的区别"></a>min 和 low的区别</h3><ol>
<li>min下的内存是保留给内核使用的；当到达min，会触发内存的direct reclaim （vm.min_free_kbytes）</li>
<li>low水位比min高一些，当内存可用量小于low的时候，会触发 kswapd回收内存，当kswapd慢慢的将内存 回收到high水位，就开始继续睡眠 </li>
</ol>
<h3 id="内存回收方式"><a href="#内存回收方式" class="headerlink" title="内存回收方式"></a>内存回收方式</h3><p>内存回收方式有两种，主要对应low ，min</p>
<ol>
<li>kswapd reclaim : 达到low水位线时执行 – 异步（实际还有，只是比较危险了，后台kswapd会回收，不会卡顿应用）</li>
<li>direct reclaim : 达到min水位线时执行 – 同步</li>
</ol>
<p>为了减少缺页中断，首先就要保证我们有足够的内存可以使用。由于Linux会尽可能多的使用free的内存，运行很久的应用free的内存是很少的。下面的图中，紫色表示已经使用的内存，白色表示尚未分配的内存。当我们的内存使用达到水位的low值的时候，kswapd就会开始回收工作，而一旦内存分配超过了min，就会进行内存的直接回收。</p>
<p><img src="/images/oss/5933cc4c28f86aa08410a8af4ff4410d.png" style="zoom:50%;"></p>
<p>针对这种情况，需要采用预留内存的手段，系统参数vm.extra_free_kbytes就是用来做这个事情的。这个参数设置了系统预留给应用的内存，可以避免紧急需要内存时发生内存回收不及时导致的高延迟。从下面图中可以看到，通过vm.extra_free_kbytes的设置，预留内存可以让内存的申请处在一个安全的水位。<strong>需要注意的是，因为内核的优化，在3.10以上的内核版本这个参数已经被取消。</strong></p>
<p><img src="/images/oss/f55022d4eb181b92ba5d2e142ec940c8.png" style="zoom: 50%;"></p>
<p>三个watermark的计算方法：</p>
<p>watermark[min] = vm.min_free_kbytes换算为page单位即可，假设为vm.min_free_kbytes。</p>
<p>watermark[low] = watermark[min] * 5 / 4</p>
<p>watermark[high] = watermark[min] * 3 / 2</p>
<p>比如默认 vm.min_free_kbytes = 65536是64K，很容易导致应用的毛刺，可以适当改大</p>
<p>或者禁止： vm.swappiness  来避免swapped来减少延迟</p>
<h3 id="direct-IO"><a href="#direct-IO" class="headerlink" title="direct IO"></a>direct IO</h3><p>绕过page cache，直接读写硬盘</p>
<h2 id="cache回收"><a href="#cache回收" class="headerlink" title="cache回收"></a>cache回收</h2><p>系统内存大体可分为三块，应用程序使用内存、系统Cache 使用内存（包括page cache、buffer，内核slab 等）和Free 内存。</p>
<ul>
<li>应用程序使用内存：应用使用都是虚拟内存，应用申请内存时只是分配了地址空间，并未真正分配出物理内存，等到应用真正访问内存时会触发内核的缺页中断，这时候才真正的分配出物理内存，映射到用户的地址空间，因此应用使用内存是不需要连续的，内核有机制将非连续的物理映射到连续的进程地址空间中（mmu），缺页中断申请的物理内存，内核优先给低阶碎内存。</li>
<li><p>系统Cache 使用内存：使用的也是虚拟内存，申请机制与应用程序相同。</p>
</li>
<li><p>Free 内存，未被使用的物理内存，这部分内存以4k 页的形式被管理在内核伙伴算法结构中，相邻的2^n 个物理页会被伙伴算法组织到一起，形成一块连续物理内存，所谓的阶内存就是这里的n (0&lt;= n &lt;=10)，高阶内存指的就是一块连续的物理内存，在OSS 的场景中，如果3阶内存个数比较小的情况下，如果系统有吞吐burst 就会触发Drop cache 情况。</p>
</li>
</ul>
<blockquote>
<p>echo 1/2/3 &gt;/proc/sys/vm/drop_caches</p>
</blockquote>
<p>查看回收后：</p>
<pre><code>cat /proc/meminfo
</code></pre><p><img src="/images/oss/7cedcb6daa53cbcfc9c68568086500b7.png" alt="image.png" style="zoom:20%;"></p>
<p>当我们执行 echo 2 来 drop slab 的时候，它也会把 Page Cache(inode可能会有对应的pagecache，inode释放后对应的pagecache也释放了)给 drop 掉</p>
<p>在系统内存紧张的时候，运维人员或者开发人员会想要通过 drop_caches 的方式来释放一些内存，但是由于他们清楚 Page Cache 被释放掉会影响业务性能，所以就期望只去 drop slab 而不去 drop pagecache。于是很多人这个时候就运行 echo 2 &gt; /proc/sys/vm/drop_caches，但是结果却出乎了他们的意料：Page Cache 也被释放掉了，业务性能产生了明显的下降。</p>
<p>查看 drop_caches 是否执行过释放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ grep drop /proc/vmstat</div><div class="line">drop_pagecache 1</div><div class="line">drop_slab 0</div><div class="line"></div><div class="line">$ grep inodesteal /proc/vmstat </div><div class="line">pginodesteal 114341</div><div class="line">kswapd_inodesteal 1291853</div></pre></td></tr></table></figure>
<p>在内存紧张的时候会触发内存回收，内存回收会尝试去回收 reclaimable（可以被回收的）内存，这部分内存既包含 Page Cache 又包含 reclaimable kernel memory(比如 slab)。inode被回收后可以通过  grep inodesteal /proc/vmstat 观察到</p>
<blockquote>
<p>kswapd_inodesteal 是指在 kswapd 回收的过程中，因为回收 inode 而释放的 pagecache page 个数；</p>
<p>pginodesteal 是指 kswapd 之外其他线程在回收过程中，因为回收 inode 而释放的 pagecache page 个数;</p>
</blockquote>
<h2 id="Page回收–缺页中断"><a href="#Page回收–缺页中断" class="headerlink" title="Page回收–缺页中断"></a>Page回收–缺页中断</h2><p><img src="/images/oss/3fdffacd66c0981956b15be348fff46a.png" alt="image.png" style="zoom:20%;"></p>
<p>从图里你可以看到，在开始内存回收后，首先进行后台异步回收（上图中蓝色标记的地方），这不会引起进程的延迟；如果后台异步回收跟不上进程内存申请的速度，就会开始同步阻塞回收，导致延迟（上图中红色和粉色标记的地方，这就是引起 load 高的地址 – Sys CPU 使用率飙升/Sys load 飙升）。</p>
<p>那么，针对直接内存回收引起 load 飙高或者业务 RT 抖动的问题，一个解决方案就是及早地触发后台回收来避免应用程序进行直接内存回收，那具体要怎么做呢？</p>
<p><img src="/images/oss/4b341ba757d27e3a81145a55f54363e1.png" alt="image.png" style="zoom:25%;"></p>
<p>它的意思是：当内存水位低于 watermark low 时，就会唤醒 kswapd 进行后台回收，然后 kswapd 会一直回收到 watermark high。</p>
<p>那么，我们可以增大 min_free_kbytes 这个配置选项来及早地触发后台回收，该选项最终控制的是内存回收水位，不过，内存回收水位是内核里面非常细节性的知识点，我们可以先不去讨论。</p>
<p>对于大于等于 128G 的系统而言，将 min_free_kbytes 设置为 4G 比较合理，这是我们在处理很多这种问题时总结出来的一个经验值，既不造成较多的内存浪费，又能避免掉绝大多数的直接内存回收。</p>
<p>该值的设置和总的物理内存并没有一个严格对应的关系，我们在前面也说过，如果配置不当会引起一些副作用，所以在调整该值之前，我的建议是：你可以渐进式地增大该值，比如先调整为 1G，观察 sar -B 中 pgscand 是否还有不为 0 的情况；如果存在不为 0 的情况，继续增加到 2G，再次观察是否还有不为 0 的情况来决定是否增大，以此类推。</p>
<blockquote>
<p>sar -B :  Report paging statistics.</p>
<p>pgscand/s  Number of pages scanned directly per second.</p>
</blockquote>
<h3 id="系统中脏页过多引起-load-飙高"><a href="#系统中脏页过多引起-load-飙高" class="headerlink" title="系统中脏页过多引起 load 飙高"></a>系统中脏页过多引起 load 飙高</h3><p>直接回收过程中，如果存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟，而且因为这个过程本身是阻塞式的，所以又可能进一步导致系统中处于 D 状态的进程数增多，最终的表现就是系统的 load 值很高。</p>
<p><img src="/images/oss/f16438b744a248d7671d5ac7317b0a98.png" alt="image.png" style="zoom: 25%;"></p>
<p>可以通过 sar -r 来观察系统中的脏页个数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ sar -r 1</div><div class="line">07:30:01 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</div><div class="line">09:20:01 PM   5681588   2137312     27.34         0   1807432    193016      2.47    534416   1310876         4</div><div class="line">09:30:01 PM   5677564   2141336     27.39         0   1807500    204084      2.61    539192   1310884        20</div><div class="line">09:40:01 PM   5679516   2139384     27.36         0   1807508    196696      2.52    536528   1310888        20</div><div class="line">09:50:01 PM   5679548   2139352     27.36         0   1807516    196624      2.51    536152   1310892        24</div></pre></td></tr></table></figure>
<p>kbdirty 就是系统中的脏页大小，它同样也是对 /proc/vmstat 中 nr_dirty 的解析。你可以通过调小如下设置来将系统脏页个数控制在一个合理范围:</p>
<blockquote>
<p>vm.dirty_background_bytes = 0</p>
<p>vm.dirty_background_ratio = 10</p>
<p>vm.dirty_bytes = 0</p>
<p>vm.dirty_expire_centisecs = 3000</p>
<p>vm.dirty_ratio = 20</p>
</blockquote>
<p>至于这些值调整大多少比较合适，也是因系统和业务的不同而异，我的建议也是一边调整一边观察，将这些值调整到业务可以容忍的程度就可以了，即在调整后需要观察业务的服务质量 (SLA)，要确保 SLA 在可接受范围内。调整的效果可以通过 /proc/vmstat 来查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#grep &quot;nr_dirty_&quot; /proc/vmstat</div><div class="line">nr_dirty_threshold 3071708</div><div class="line">nr_dirty_background_threshold 1023902</div></pre></td></tr></table></figure>
<p>在4.20的内核并且sar 的版本为12.3.3可以看到PSI（Pressure-Stall Information）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">some avg10=45.49 avg60=10.23 avg300=5.41 total=76464318</div><div class="line">full avg10=40.87 avg60=9.05 avg300=4.29 total=58141082</div></pre></td></tr></table></figure>
<p>重点关注 avg10 这一列，它表示最近 10s 内存的平均压力情况，如果它很大（比如大于 40）那 load 飙高大概率是由于内存压力，尤其是 Page Cache 的压力引起的。</p>
<p><img src="/images/oss/cf58f10a523e1e4f0db443be3f54fc04.png" alt="image.png" style="zoom: 25%;"></p>
<h2 id="通过tracepoint分析内存卡顿问题"><a href="#通过tracepoint分析内存卡顿问题" class="headerlink" title="通过tracepoint分析内存卡顿问题"></a>通过tracepoint分析内存卡顿问题</h2><p><img src="/images/oss/d5446b656e8d91a9fb72200a7b97e723.png" alt="image.png" style="zoom:25%;"></p>
<p>我们继续以内存规整 (memory compaction) 为例，来看下如何利用 tracepoint 来对它进行观察：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#首先来使能compcation相关的一些tracepoing</div><div class="line">$ echo 1 &gt;</div><div class="line">/sys/kernel/debug/tracing/events/compaction/mm_compaction_begin/enable</div><div class="line">$ echo 1 &gt;</div><div class="line">/sys/kernel/debug/tracing/events/compaction/mm_compaction_end/enable </div><div class="line"></div><div class="line">#然后来读取信息，当compaction事件触发后就会有信息输出</div><div class="line">$ cat /sys/kernel/debug/tracing/trace_pipe</div><div class="line">           &lt;...&gt;-49355 [037] .... 1578020.975159: mm_compaction_begin: </div><div class="line">zone_start=0x2080000 migrate_pfn=0x2080000 free_pfn=0x3fe5800 </div><div class="line">zone_end=0x4080000, mode=async</div><div class="line">           &lt;...&gt;-49355 [037] .N.. 1578020.992136: mm_compaction_end: </div><div class="line">zone_start=0x2080000 migrate_pfn=0x208f420 free_pfn=0x3f4b720 </div><div class="line">zone_end=0x4080000, mode=async status=contended</div></pre></td></tr></table></figure>
<p>从这个例子中的信息里，我们可以看到是 49355 这个进程触发了 compaction，begin 和 end 这两个 tracepoint 触发的时间戳相减，就可以得到 compaction 给业务带来的延迟，我们可以计算出这一次的延迟为 17ms。</p>
<p>或者用 <a href="https://lore.kernel.org/linux-mm/20191001144524.GB3321@techsingularity.net/T/" target="_blank" rel="external">perf script</a> 脚本来分析, <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop.py" target="_blank" rel="external">基于 bcc(eBPF) 写的direct reclaim snoop</a>来观察进程因为 direct reclaim 而导致的延迟。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--HugePage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--HugePage/" itemprop="url">Linux内存--HugePage</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–HugePage"><a href="#Linux内存–HugePage" class="headerlink" title="Linux内存–HugePage"></a>Linux内存–HugePage</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="proc-buddyinfo"><a href="#proc-buddyinfo" class="headerlink" title="/proc/buddyinfo"></a>/proc/buddyinfo</h2><p>/proc/buddyinfo记录了内存的详细碎片情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#cat /proc/buddyinfo </div><div class="line">Node 0, zone      DMA      1      1      1      0      2      1      1      0      1      1      3 </div><div class="line">Node 0, zone    DMA32      2      5      3      6      2      0      4      4      2      2    404 </div><div class="line">Node 0, zone   Normal 243430 643847 357451  32531   9508   6159   3917   2960  17172   2633  22854</div></pre></td></tr></table></figure>
<p>Normal行的第二列表示：  643847*2^1*Page_Size(4K) ;  第三列表示：  357451*2^2*Page_Size(4K)  ，高阶内存指的是2^3及更大的内存块。</p>
<p>应用申请大块连续内存（高阶内存，一般之4阶及以上, 也就是64K以上–2^4*4K）时，容易导致卡顿。这是因为大块连续内存确实系统需要触发回收或者碎片整理，需要一定的时间。</p>
<h2 id="slabtop和-proc-slabinfo"><a href="#slabtop和-proc-slabinfo" class="headerlink" title="slabtop和/proc/slabinfo"></a>slabtop和/proc/slabinfo</h2><p>slabtop和/proc/slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<h2 id="关于hugetlb"><a href="#关于hugetlb" class="headerlink" title="关于hugetlb"></a>关于hugetlb</h2><p>This is an entry in the TLB that points to a HugePage (a large/big page larger than regular 4K and predefined in size). HugePages are implemented via hugetlb entries, i.e. we can say that a HugePage is handled by a “hugetlb page entry”. The ‘hugetlb” term is also (and mostly) used synonymously with a HugePage.</p>
<p> hugetlb 是TLB中指向HugePage的一个entry(通常大于4k或预定义页面大小)。 HugePage 通过hugetlb entries来实现，也可以理解为HugePage 是hugetlb page entry的一个句柄。</p>
<p><strong>Linux下的大页分为两种类型：标准大页（Huge Pages）和透明大页（Transparent Huge Pages）</strong></p>
<p>标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式</p>
<p>目前透明大页与传统HugePages联用会出现一些问题，导致性能问题和系统重启。Oracle 建议禁用透明大页（Transparent Huge Pages）</p>
<p>hugetlbfs比THP要好，开thp的机器碎片化严重（不开THP会有更严重的碎片化问题），最后和没开THP一样 <a href="https://www.atatech.org/articles/152660" target="_blank" rel="external">https://www.atatech.org/articles/152660</a></p>
<p>Linux 中的 HugePages 都被锁定在内存中，所以哪怕是在系统内存不足时，它们也不会被 Swap 到磁盘上，这也就能从根源上杜绝了重要内存被频繁换入和换出的可能。</p>
<blockquote>
<p><strong>Transparent Hugepages</strong> are similar to standard <strong>HugePages</strong>. However, while standard <strong>HugePages</strong> allocate memory at startup, <strong>Transparent Hugepages</strong> memory uses the khugepaged thread in the kernel to allocate memory dynamically during runtime, using swappable <strong>HugePages</strong>.</p>
</blockquote>
<p>HugePage要求OS启动的时候提前分配出来，管理难度比较大，所以Enterprise Linux 6增加了一层抽象层来动态创建管理HugePage，这就是THP，而这个THP对应用透明，由khugepaged thread在后台动态将小页组成大页给应用使用，这里会遇上碎片问题导致需要compact才能得到大页，应用感知到的就是SYS CPU飙高，应用卡顿了。</p>
<p>虽然 HugePages 的开启大都需要开发或者运维工程师的额外配置，但是在应用程序中启用 HugePages 却可以在以下几个方面降低内存页面的管理开销：</p>
<ul>
<li>更大的内存页能够减少内存中的页表层级，这不仅可以降低页表的内存占用，也能降低从虚拟内存到物理内存转换的性能损耗；</li>
<li>更大的内存页意味着更高的缓存命中率，CPU 有更高的几率可以直接在 TLB（Translation lookaside buffer）中获取对应的物理地址；</li>
<li>更大的内存页可以减少获取大内存的次数，使用 HugePages 每次可以获取 2MB 的内存，是 4KB 的默认页效率的 512 倍；</li>
</ul>
<h2 id="HugePage"><a href="#HugePage" class="headerlink" title="HugePage"></a>HugePage</h2><p><strong>为什么需要Huge Page</strong> 了解CPU Cache大致架构的话，一定听过TLB Cache。<code>Linux</code>系统中，对程序可见的，可使用的内存地址是<code>Virtual Address</code>。每个程序的内存地址都是从0开始的。而实际的数据访问是要通过<code>Physical Address</code>进行的。因此，每次内存操作，CPU都需要从<code>page table</code>中把<code>Virtual Address</code>翻译成对应的<code>Physical Address</code>，那么对于大量内存密集型程序来说<code>page table</code>的查找就会成为程序的瓶颈。</p>
<p>所以现代CPU中就出现了TLB(Translation Lookaside Buffer) Cache用于缓存少量热点内存地址的mapping关系。然而由于制造成本和工艺的限制，响应时间需要控制在CPU Cycle级别的Cache容量只能存储几十个对象。那么TLB Cache在应对大量热点数据<code>Virual Address</code>转换的时候就显得捉襟见肘了。我们来算下按照标准的Linux页大小(page size) 4K，一个能缓存64元素的TLB Cache只能涵盖<code>4K*64 = 256K</code>的热点数据的内存地址，显然离理想非常遥远的。于是Huge Page就产生了。</p>
<p>Huge pages require contiguous areas of memory, so allocating them at boot is the most reliable method since memory has not yet become fragmented. To do so, add the following parameters to the kernel boot command line:</p>
<p><strong>Huge pages kernel options</strong></p>
<ul>
<li><p>hugepages</p>
<p>Defines the number of persistent huge pages configured in the kernel at boot time. The default value is <code>0</code>. It is only possible to allocate (or deallocate) huge pages if there are sufficient physically contiguous free pages in the system. Pages reserved by this parameter cannot be used for other purposes.</p>
<p>Default size huge pages can be dynamically allocated or deallocated by changing the value of the <code>/proc/sys/vm/nr_hugepages</code> file.</p>
<p>In a NUMA system, huge pages assigned with this parameter are divided equally between nodes. You can assign huge pages to specific nodes at runtime by changing the value of the node’s <code>/sys/devices/system/node/node_id/hugepages/hugepages-1048576kB/nr_hugepages</code> file.</p>
<p>For more information, read the relevant kernel documentation, which is installed in <code>/usr/share/doc/kernel-doc-kernel_version/Documentation/vm/hugetlbpage.txt</code> by default. This documentation is available only if the <em>kernel-doc</em> package is installed.</p>
</li>
<li><p>hugepagesz</p>
<p>Defines the size of persistent huge pages configured in the kernel at boot time. Valid values are 2 MB and 1 GB. The default value is 2 MB.</p>
</li>
<li><p>default_hugepagesz</p>
<p>Defines the default size of persistent huge pages configured in the kernel at boot time. Valid values are 2 MB and 1 GB. The default value is 2 MB.</p>
</li>
</ul>
<h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a><a href="https://www.golinuxcloud.com/configure-hugepages-vm-nr-hugepages-red-hat-7/" target="_blank" rel="external">工具</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">yum install libhugetlbfs-utils -y</div><div class="line"></div><div class="line">//列出</div><div class="line">hugeadm --pool-list</div><div class="line">      Size  Minimum  Current  Maximum  Default</div><div class="line">   2097152    12850    12850    12850        *</div><div class="line">1073741824        0        0        0</div><div class="line"></div><div class="line">hugeadm --list-all-mounts</div><div class="line">Mount Point          Options</div><div class="line">/dev/hugepages       rw,relatime,pagesize=2M</div></pre></td></tr></table></figure>
<h3 id="大页和-MySQL-性能-case"><a href="#大页和-MySQL-性能-case" class="headerlink" title="大页和 MySQL 性能 case"></a>大页和 MySQL 性能 case</h3><p>MySQL的页都是16K, 当查询的行不在内存中时需要按照16K为单位从磁盘读取页,而文件系统中的页是4k，也就是一次数据库请求需要有4次磁盘IO，如过查询比较随机，每次只需要一个页中的几行数据，存在很大的读放大。</p>
<p>那么我们是否可以把MySQL的页设置为4K来减少读放大呢？</p>
<p>在5.7里收益不大，因为每次IO存在 fil_system 的锁，导致IO的并发上不去</p>
<p>8.0中总算优化了这个场景，测试细节可以参考<a href="http://dimitrik.free.fr/blog/archives/2018/05/mysql-performance-1m-iobound-qps-with-80-ga-on-intel-optane-ssd.html" target="_blank" rel="external">这篇</a></p>
<p>16K VS 4K 性能对比（4K接近翻倍）</p>
<p><img src="/images/951413iMgBlog/1547605552845-d406952d-9857-462d-a666-1694b19fbedb.png" alt="img"></p>
<p>4K会带来的问题：顺序insert慢了10%（因为fsync更多了）；DDL更慢；二级索引更多的场景下4K性能较差；大BP下，刷脏代价大。</p>
<h3 id="HugePage-带来的问题"><a href="#HugePage-带来的问题" class="headerlink" title="HugePage 带来的问题"></a><a href="http://cenalulu.github.io/linux/huge-page-on-numa/" target="_blank" rel="external">HugePage 带来的问题</a></h3><h4 id="CPU对同一个Page抢占增多"><a href="#CPU对同一个Page抢占增多" class="headerlink" title="CPU对同一个Page抢占增多"></a>CPU对同一个Page抢占增多</h4><p>对于写操作密集型的应用，Huge Page会大大增加Cache写冲突的发生概率。由于CPU独立Cache部分的写一致性用的是<code>MESI协议</code>，写冲突就意味：</p>
<ul>
<li>通过CPU间的总线进行通讯，造成总线繁忙</li>
<li>同时也降低了CPU执行效率。</li>
<li>CPU本地Cache频繁失效</li>
</ul>
<p>类比到数据库就相当于，原来一把用来保护10行数据的锁，现在用来锁1000行数据了。必然这把锁在线程之间的争抢概率要大大增加。</p>
<h4 id="连续数据需要跨CPU读取"><a href="#连续数据需要跨CPU读取" class="headerlink" title="连续数据需要跨CPU读取"></a>连续数据需要跨CPU读取</h4><p>Page太大，更容易造成Page跨Numa/CPU 分布。</p>
<p>从下图我们可以看到，原本在4K小页上可以连续分配，并因为较高命中率而在同一个CPU上实现locality的数据。到了Huge Page的情况下，就有一部分数据为了填充统一程序中上次内存分配留下的空间，而被迫分布在了两个页上。而在所在Huge Page中占比较小的那部分数据，由于在计算CPU亲和力的时候权重小，自然就被附着到了其他CPU上。那么就会造成：本该以热点形式存在于CPU2 L1或者L2 Cache上的数据，不得不通过CPU inter-connect去remote CPU获取数据。 假设我们连续申明两个数组，<code>Array A</code>和<code>Array B</code>大小都是1536K。内存分配时由于第一个Page的2M没有用满，因此<code>Array B</code>就被拆成了两份，分割在了两个Page里。而由于内存的亲和配置，一个分配在Zone 0，而另一个在Zone 1。那么当某个线程需要访问Array B时就不得不通过代价较大的Inter-Connect去获取另外一部分数据。</p>
<p><img src="/images/951413iMgBlog/false_sharing.png" alt="img"></p>
<h3 id="Java进程开启HugePage"><a href="#Java进程开启HugePage" class="headerlink" title="Java进程开启HugePage"></a>Java进程开启HugePage</h3><p>从perf数据来看压满后tlab miss比较高，得想办法降低这个值</p>
<h4 id="修改JVM启动参数"><a href="#修改JVM启动参数" class="headerlink" title="修改JVM启动参数"></a>修改JVM启动参数</h4><p>JVM启动参数增加如下三个(-XX:LargePageSizeInBytes=2m, 这个一定要，有些资料没提这个，在我的JDK8.0环境必须要)：</p>
<blockquote>
<p>-XX:+UseLargePages -XX:LargePageSizeInBytes=2m -XX:+UseHugeTLBFS</p>
</blockquote>
<h4 id="修改机器系统配置"><a href="#修改机器系统配置" class="headerlink" title="修改机器系统配置"></a>修改机器系统配置</h4><p>设置HugePage的大小</p>
<blockquote>
<p>cat /proc/sys/vm/nr_hugepages</p>
</blockquote>
<p>nr_hugepages设置多大参考如下计算方法：</p>
<blockquote>
<p>If you are using the option <code>-XX:+UseSHM</code> or <code>-XX:+UseHugeTLBFS</code>, then specify the number of large pages. In the following example, 3 GB of a 4 GB system are reserved for large pages (assuming a large page size of 2048kB, then 3 GB = 3 <em> 1024 MB = 3072 MB = 3072 </em> 1024 kB = 3145728 kB and 3145728 kB / 2048 kB = 1536):</p>
<p>echo 1536 &gt; /proc/sys/vm/nr_hugepages </p>
</blockquote>
<p>透明大页是没有办法减少系统tlab，tlab是对应于进程的，系统分给进程的透明大页还是由物理上的4K page组成。</p>
<p>对于c++来说，他malloc经常会散落得全地址都是，因为会触发各种mmap，冷热区域。所以THP和hugepage都可能导致大量内存被浪费了，进而导致内存紧张，性能下滑。jvm的连续内存布局，加上gc会使得内存密度很紧凑。THP的问题是，他是逻辑页，不是物理页，tlb依旧要N份，所以他的收益来自page fault减少，是一次性的收益。</p>
<p>hugepage的在减少page_fault上和thp效果一样第二个作用是，他只需要一份TLB了，hugepage是真正的大页内存，thp是逻辑上的，物理上还是需要很多小的page。</p>
<p><strong>如果TLB miss，则可能需要额外三次内存读取操作才能将线性地址翻译为物理地址。</strong></p>
<h2 id="THP"><a href="#THP" class="headerlink" title="THP"></a>THP</h2><p>Linux kernel在2.6.38内核增加了Transparent Huge Pages (THP)特性 ，支持大内存页(2MB)分配，默认开启。当开启时可以降低fork子进程的速度，但fork之后，每个内存页从原来4KB变为2MB，会大幅增加重写期间父进程内存消耗。同时<strong>每次写命令引起的复制内存页单位放大了512倍</strong>，会拖慢写操作的执行时间，导致大量写操作慢查询。例如简单的incr命令也会出现在慢查询中。因此Redis日志中建议将此特性进行禁用。  </p>
<p>THP 的目的是用一个页表项来映射更大的内存（大页），这样可以减少 Page Fault，因为需要的页数少了。当然，这也会提升 TLB（Translation Lookaside Buffer，由存储器管理单元用于改进虚拟地址到物理地址的转译速度） 命中率，因为需要的页表项也少了。如果进程要访问的数据都在这个大页中，那么这个大页就会很热，会被缓存在 Cache 中。而大页对应的页表项也会出现在 TLB 中，从上一讲的存储层次我们可以知道，这有助于性能提升。但是反过来，假设应用程序的数据局部性比较差，它在短时间内要访问的数据很随机地位于不同的大页上，那么大页的优势就会消失。</p>
<p>THP 对redis、mongodb 这种cache类推荐关闭，对drds这种java应用最好打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">#cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">[always] madvise never</div><div class="line"></div><div class="line">grep &quot;Huge&quot; /proc/meminfo</div><div class="line">AnonHugePages:   1286144 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">HugePages_Total:       0</div><div class="line">HugePages_Free:        0</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:               0 kB</div><div class="line"></div><div class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &apos;&#123; if($2&gt;4) print $0&#125; &apos; |  awk -F &quot;/&quot;  &apos;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &apos;</div><div class="line"></div><div class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &apos;&#123; if($2&gt;4) print $0&#125; &apos; |  awk -F &quot;/&quot;  &apos;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &apos;</div><div class="line"></div><div class="line">//查看pagesize（默认4K） </div><div class="line">$getconf PAGESIZE</div></pre></td></tr></table></figure>
<p>在透明大页功能打开时，造成系统性能下降的主要原因可能是 <code>khugepaged</code> 守护进程。该进程会在（它认为）系统空闲时启动，扫描系统中剩余的空闲内存，并将普通 4k 页转换为大页。该操作会在内存路径中加锁，而该守护进程可能会在错误的时间启动扫描和转换大页的操作，从而影响应用性能。</p>
<p>此外，当缺页异常(page faults)增多时，透明大页会和普通 4k 页一样，产生同步内存压缩(direct compaction)操作，以节省内存。该操作是一个同步的内存整理操作，如果应用程序会短时间分配大量内存，内存压缩操作很可能会被触发，从而会对系统性能造成风险。<a href="https://yq.aliyun.com/articles/712830" target="_blank" rel="external">https://yq.aliyun.com/articles/712830</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#查看系统级别的 THP 使用情况，执行下列命令：</div><div class="line">cat /proc/meminfo  | grep AnonHugePages</div><div class="line">#类似地，查看进程级别的 THP 使用情况，执行下列命令：</div><div class="line">cat /proc/1730/smaps | grep AnonHugePages |grep -v &quot;0 kB&quot;</div><div class="line">#是否开启了hugepage</div><div class="line">$cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">always [madvise] never</div></pre></td></tr></table></figure>
<p><code>/proc/sys/vm/nr_hugepages</code> 中存储的数据就是大页面的数量，虽然在默认情况下它的值都是 0，不过我们可以通过更改该文件的内容申请或者释放操作系统中的大页：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ echo 1 &gt; /proc/sys/vm/nr_hugepages</div><div class="line">$ cat /proc/meminfo | grep HugePages_</div><div class="line">HugePages_Total:       1</div><div class="line">HugePages_Free:        1</div></pre></td></tr></table></figure>
<h2 id="MySQL-场景下代码大页对性能的影响"><a href="#MySQL-场景下代码大页对性能的影响" class="headerlink" title="MySQL 场景下代码大页对性能的影响"></a><a href="https://ata.alibaba-inc.com/articles/217859" target="_blank" rel="external">MySQL 场景下代码大页对性能的影响</a></h2><p>不只是数据可以用HugePage，代码段也可以开启HugePage, 无论在x86还是arm（arm下提升更明显）下，都可以得到大页优于透明大页，透明大页优于正常的4K page</p>
<blockquote>
<p>收益：代码大页 &gt; anon THP &gt; 4k</p>
</blockquote>
<p>arm下，对32core机器用32并发的sysbench来对比，代码大页带来的性能提升大概有11%，iTLB miss下降了10倍左右。</p>
<p>x86下，性能提升只有大概3-5%之间，iTLB miss下降了1.5-3倍左右。</p>
<h2 id="TLAB-miss高的案例"><a href="#TLAB-miss高的案例" class="headerlink" title="TLAB miss高的案例"></a><a href="https://ata.alibaba-inc.com/articles/152660" target="_blank" rel="external">TLAB miss高的案例</a></h2><p>程序运行久了之后会变慢大概10%</p>
<p>刚开始运行的时候perf各项数据:</p>
<p><img src="/images/951413iMgBlog/7a26deaf96bdcc07db4db34ae1178641.png" alt="img"></p>
<p>长时间运行后：</p>
<p><img src="/images/951413iMgBlog/3385ae6ffbd5b48b80efa759f42b8174.png" alt="img"></p>
<p>内存的利用以页为单位，当时分析认为，在此4k连续的基础上，页的碎片不应该对64 byte align的cache有什么影响。当时guest和host都没有开THP。</p>
<p>既然无法理解这个结果，那就只有按部就班的查看内核执行路径上各个函数的差别了，祭出ftrace:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">echo kerel_func_name1 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</div><div class="line"></div><div class="line">echo kerel_func_name2 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</div><div class="line"></div><div class="line">echo kerel_func_name3 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</div><div class="line">echo 1 &gt; /sys/kernel/debug/tracing/function_profile_enabled</div></pre></td></tr></table></figure>
<p>在CPU#20上执行代码:</p>
<p>taskset -c 20 ./b</p>
<p>代码执行完后:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo 0 &gt; /sys/kernel/debug/tracing/function_profile_enabled</div><div class="line">cat /sys/kernel/debug/tracing/trace_stat/function20</div></pre></td></tr></table></figure>
<p>这个时候就会打印出在各个函数上花费的时间，比如:</p>
<p><img src="/images/951413iMgBlog/329769dd1da2ed324ac11b8b922382cd.png" alt="img"></p>
<p>经过调试后，逐步定位到主要时间差距在  __mem_cgroup_commit_charge() (58%).</p>
<p>在阅读代码的过程中，注意到当前内核使能了CONFIG_SPARSEMEM_VMEMMAP=y</p>
<p>原因就是机器运行久了之后内存碎片化严重，导致TLAB Miss严重。</p>
<p>解决：开启THP后，性能稳定</p>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 4G/8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a href="https://www.atatech.org/articles/163233" target="_blank" rel="external">https://www.atatech.org/articles/163233</a> <a href="https://www.atatech.org/articles/97130" target="_blank" rel="external">https://www.atatech.org/articles/97130</a></p>
<p>compact: 在进行 compcation 时，线程会从前往后扫描已使用的 movable page，然后从后往前扫描 free page，扫描结束后会把这些 movable page 给迁移到 free page 里，最终规整出一个 2M 的连续物理内存，这样 THP 就可以成功申请内存了。</p>
<p><img src="/images/951413iMgBlog/image-20210628144121108.png" alt="image-20210628144121108"></p>
<p>一次THP compact堆栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">java          R  running task        0 144305 144271 0x00000080</div><div class="line"> ffff88096393d788 0000000000000086 ffff88096393d7b8 ffffffff81060b13</div><div class="line"> ffff88096393d738 ffffea003968ce50 000000000000000e ffff880caa713040</div><div class="line"> ffff8801688b0638 ffff88096393dfd8 000000000000fbc8 ffff8801688b0640</div><div class="line"></div><div class="line">Call Trace:</div><div class="line"> [&lt;ffffffff81060b13&gt;] ? perf_event_task_sched_out+0x33/0x70</div><div class="line"> [&lt;ffffffff8100bb8e&gt;] ? apic_timer_interrupt+0xe/0x20</div><div class="line"> [&lt;ffffffff810686da&gt;] __cond_resched+0x2a/0x40</div><div class="line"> [&lt;ffffffff81528300&gt;] _cond_resched+0x30/0x40</div><div class="line"> [&lt;ffffffff81169505&gt;] compact_checklock_irqsave+0x65/0xd0</div><div class="line"> [&lt;ffffffff81169862&gt;] compaction_alloc+0x202/0x460</div><div class="line"> [&lt;ffffffff811748d8&gt;] ? buffer_migrate_page+0xe8/0x130</div><div class="line"> [&lt;ffffffff81174b4a&gt;] migrate_pages+0xaa/0x480</div><div class="line"> [&lt;ffffffff81169660&gt;] ? compaction_alloc+0x0/0x460                 //compact and migrate</div><div class="line"> [&lt;ffffffff8116a1a1&gt;] compact_zone+0x581/0x950</div><div class="line"> [&lt;ffffffff8116a81c&gt;] compact_zone_order+0xac/0x100</div><div class="line"> [&lt;ffffffff8116a951&gt;] try_to_compact_pages+0xe1/0x120</div><div class="line"> [&lt;ffffffff8112f1ba&gt;] __alloc_pages_direct_compact+0xda/0x1b0</div><div class="line"> [&lt;ffffffff8112f80b&gt;] __alloc_pages_nodemask+0x57b/0x8d0</div><div class="line"> [&lt;ffffffff81167b9a&gt;] alloc_pages_vma+0x9a/0x150</div><div class="line"> [&lt;ffffffff8118337d&gt;] do_huge_pmd_anonymous_page+0x14d/0x3b0        //huge page</div><div class="line"> [&lt;ffffffff8152a116&gt;] ? rwsem_down_read_failed+0x26/0x30</div><div class="line"> [&lt;ffffffff8114b350&gt;] handle_mm_fault+0x2f0/0x300</div><div class="line"> [&lt;ffffffff810ae950&gt;] ? wake_futex+0x40/0x60</div><div class="line"> [&lt;ffffffff8104a8d8&gt;] __do_page_fault+0x138/0x480</div><div class="line"> [&lt;ffffffff810097cc&gt;] ? __switch_to+0x1ac/0x320</div><div class="line"> [&lt;ffffffff81527910&gt;] ? thread_return+0x4e/0x76e</div><div class="line"> [&lt;ffffffff8152d45e&gt;] do_page_fault+0x3e/0xa0                       //page fault</div><div class="line"> [&lt;ffffffff8152a815&gt;] page_fault+0x25/0x30</div></pre></td></tr></table></figure>
<h3 id="查看pagetypeinfo"><a href="#查看pagetypeinfo" class="headerlink" title="查看pagetypeinfo"></a>查看pagetypeinfo</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">#cat /proc/pagetypeinfo</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    0, zone      DMA, type    Unmovable      1      1      1      0      2      1      1      0      1      0      0</div><div class="line">Node    0, zone      DMA, type  Reclaimable      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      1      3</div><div class="line">Node    0, zone      DMA, type      Reserve      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type    Unmovable     89    144     98     42     21     14      5      2      1      0      1</div><div class="line">Node    0, zone    DMA32, type  Reclaimable     28     22      9      8      0      0      0      0      0      1      7</div><div class="line">Node    0, zone    DMA32, type      Movable    402     50     21      8    880    924    321     51      4      1    227</div><div class="line">Node    0, zone    DMA32, type      Reserve      0      0      0      0      0      0      0      0      0      0      1</div><div class="line">Node    0, zone    DMA32, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type    Unmovable  13709  15231   6637   2646    816    181     46      4      4      1      0</div><div class="line">Node    0, zone   Normal, type  Reclaimable      1      5      6   3293   1295    128     29      7      5      0      0</div><div class="line">Node    0, zone   Normal, type      Movable   6396 1383350 1301956 1007627 670102 366248 160232  54894  13126   1482     37</div><div class="line">Node    0, zone   Normal, type      Reserve      0      0      0      2      1      1      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</div><div class="line">Node 0, zone      DMA            1            0            7            0            0            0</div><div class="line">Node 0, zone    DMA32           24           38          889            1            0            0</div><div class="line">Node 0, zone   Normal         1568          795       127683            2            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    1, zone   Normal, type    Unmovable   3938   8735   5469   3221   2097    989    202      6      0      0      0</div><div class="line">Node    1, zone   Normal, type  Reclaimable      1      7      7      8      7      2      2      2      1      0      0</div><div class="line">Node    1, zone   Normal, type      Movable  18623 1001037 2084894 1261484 631159 276096  87272  17169   1389    797      0</div><div class="line">Node    1, zone   Normal, type      Reserve      0      0      0      8      0      0      0      0      0      0      0</div><div class="line">Node    1, zone   Normal, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    1, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</div><div class="line">Node 1, zone   Normal         1530          637       128903            2            0            0</div></pre></td></tr></table></figure>
<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</div><div class="line">Node 0, zone      DMA</div><div class="line">Node 0, zone    DMA32</div><div class="line">Node 0, zone   Normal</div><div class="line">Node 1, zone   Normal</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3975</div><div class="line">        min      20</div><div class="line">        low      25</div><div class="line">        high     30</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3996</div><div class="line">        managed  3975</div><div class="line">    nr_free_pages 3975</div><div class="line">    nr_alloc_batch 5</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     382873</div><div class="line">        min      2335</div><div class="line">        low      2918</div><div class="line">        high     3502</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  513024</div><div class="line">        managed  450639</div><div class="line">    nr_free_pages 382873</div><div class="line">    nr_alloc_batch 584</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     11105097</div><div class="line">        min      61463</div><div class="line">        low      76828</div><div class="line">        high     92194</div><div class="line">        scanned  0</div><div class="line">        spanned  12058624</div><div class="line">        present  12058624</div><div class="line">        managed  11859912</div><div class="line">    nr_free_pages 11105097</div><div class="line">    nr_alloc_batch 12344</div><div class="line">    </div><div class="line">    low = 5/4 * min</div><div class="line">high = 3/2 * min</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=499 MB</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=624 MB</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=802 MB</div></pre></td></tr></table></figure>
<h2 id="内存碎片化导致rt升高的诊断"><a href="#内存碎片化导致rt升高的诊断" class="headerlink" title="内存碎片化导致rt升高的诊断"></a>内存碎片化导致rt升高的诊断</h2><p>判定方法如下：</p>
<ol>
<li>运行 sar -B 观察 pgscand/s，其含义为每秒发生的直接内存回收次数，当在一段时间内持续大于 0 时，则应继续执行后续步骤进行排查；</li>
<li>运行 <code>cat /sys/kernel/debug/extfrag/extfrag_index</code> 观察内存碎片指数，重点关注 order &gt;= 3 的碎片指数，当接近 1.000 时，表示碎片化严重，当接近 0 时表示内存不足；</li>
<li>运行 <code>cat /proc/buddyinfo, cat /proc/pagetypeinfo</code> 查看内存碎片情况， 指标含义参考 （<a href="https://man7.org/linux/man-pages/man5/proc.5.html），同样关注" target="_blank" rel="external">https://man7.org/linux/man-pages/man5/proc.5.html），同样关注</a> order &gt;= 3 的剩余页面数量，pagetypeinfo 相比 buddyinfo 展示的信息更详细一些，根据迁移类型 （伙伴系统通过迁移类型实现反碎片化）进行分组，需要注意的是，当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重，我们需要结合其他工具来排查具体原因，在本文就不做过多介绍了；</li>
<li>对于 CentOS 7.6 等支持 BPF 的 kernel 也可以运行我们研发的 <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt" target="_blank" rel="external">drsnoop</a>，<a href="https://github.com/iovisor/bcc/blob/master/tools/compactsnoop_example.txt" target="_blank" rel="external">compactsnoop</a> 工具对延迟进行定量分析，使用方法和解读方式请参考对应文档；</li>
<li>(Opt) 使用 ftrace 抓取 mm_page_alloc_extfrag 事件，观察因内存碎片从备用迁移类型“盗取”页面的信息。</li>
</ol>
<p>​    </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--零拷贝/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--零拷贝/" itemprop="url">Linux内存--零拷贝</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–零拷贝"><a href="#Linux内存–零拷贝" class="headerlink" title="Linux内存–零拷贝"></a>Linux内存–零拷贝</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h2><blockquote>
<p>“<strong>Zero-copy</strong>“ describes computer operations in which the <a href="https://en.wikipedia.org/wiki/Central_processing_unit" target="_blank" rel="external">CPU</a> does not perform the task of copying data from one <a href="https://en.wikipedia.org/wiki/RAM" target="_blank" rel="external">memory</a> area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network.</p>
</blockquote>
<p>零拷贝技术是指计算机执行操作时，<a href="https://zh.wikipedia.org/wiki/中央处理器" target="_blank" rel="external">CPU</a>不需要先将数据从某处<a href="https://zh.wikipedia.org/wiki/随机存取存储器" target="_blank" rel="external">内存</a>复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省 CPU 和<a href="https://zh.wikipedia.org/wiki/内存带宽" target="_blank" rel="external">内存带宽</a>。</p>
<p>零拷贝可以做到用户空间和内核空间共用同一块内存（Java中的DirectBuffer），这样少做一次拷贝。普通Buffer是在JVM堆上分配的内存，而DirectBuffer是堆外分配的（内核和JVM可以同时读写），这样不需要再多一次内核到用户Buffer的拷贝 </p>
<p><img src="/images/951413iMgBlog/83e2dfbd25d703c58877b2faf71c4944.jpg" alt=""></p>
<p>比如通过网络下载文件，普通拷贝的流程会复制4次并有4次上下文切换，上下文切换是因为读写慢导致了IO的阻塞，进而线程被内核挂起，所以发生了上下文切换。在极端情况下如果read/write没有导致阻塞是不会发生上下文切换的：</p>
<p><img src="/images/951413iMgBlog/NoOptimization.jpg" alt="NoOptimization"></p>
<p>改成零拷贝后，也就是将read和write合并成一次，直接在内核中完成磁盘到网卡的数据复制</p>
<p><img src="/images/951413iMgBlog/ccdc10037d35349293cba8a63ad72af5.png" alt="image.png"></p>
<p>零拷贝就是操作系统提供的新函数(sendfile)，同时接收文件描述符和 TCP socket 作为输入参数，这样执行时就可以完全在内核态完成内存拷贝，既减少了内存拷贝次数，也降低了上下文切换次数。</p>
<p>而且，零拷贝取消了用户缓冲区后，不只降低了用户内存的消耗，还通过最大化利用 socket 缓冲区中的内存，间接地再一次减少了系统调用的次数，从而带来了大幅减少上下文切换次数的机会！</p>
<p>应用读取磁盘写入网络的时候还得考虑缓存的大小，一般会设置的比较小，这样一个大文件导致多次小批量的读取，每次读取伴随着多次上下文切换。</p>
<p>零拷贝使我们不必关心 socket 缓冲区的大小（socket缓冲区大小本身默认就是动态调整、或者应用代码指定大小）。比如，调用零拷贝发送方法时，尽可以把发送字节数设为文件的所有未发送字节数，例如 320MB，也许此时 socket 缓冲区大小为 1.4MB，那么一次性就会发送 1.4MB 到客户端，而不是只有 32KB。这意味着对于 1.4MB 的 1 次零拷贝，仅带来 2 次上下文切换，而不使用零拷贝且用户缓冲区为 32KB 时，经历了 176 次（4 * 1.4MB/32KB）上下文切换。</p>
<h2 id="read-write-和零拷贝优化"><a href="#read-write-和零拷贝优化" class="headerlink" title="read+write 和零拷贝优化"></a>read+write 和零拷贝优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">read(file, tmp_buf, len);</div><div class="line">write(socket, tmp_buf, len);</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20201104175056589.png" alt="image-20201104175056589"></p>
<h3 id="通过mmap替换read优化一下"><a href="#通过mmap替换read优化一下" class="headerlink" title="通过mmap替换read优化一下"></a>通过mmap替换read优化一下</h3><p>用 <code>mmap()</code> 替换原先的 <code>read()</code>，<code>mmap()</code> 也即是内存映射（memory map）：把用户进程空间的一段内存缓冲区（user buffer）映射到文件所在的内核缓冲区（kernel buffer）上。</p>
<p><img src="/images/951413iMgBlog/516c11b9b9d3f6092f00645c1742c111.png" alt="image.png"></p>
<p>通过使用 <code>mmap()</code> 来代替 <code>read()</code>， 可以减少一次数据拷贝的过程。</p>
<p>但这还不是最理想的零拷贝，因为首先仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次；另外内存映射技术是一个开销很大的虚拟存储操作：这种操作需要修改页表以及用内核缓冲区里的文件数据汰换掉当前 TLB 里的缓存以维持虚拟内存映射的一致性。</p>
<h3 id="sendfile"><a href="#sendfile" class="headerlink" title="sendfile"></a>sendfile</h3><p>在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 <code>sendfile()</code>，函数形式如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></div><div class="line"><span class="keyword">ssize_t</span> sendfile(<span class="keyword">int</span> out_fd, <span class="keyword">int</span> in_fd, <span class="keyword">off_t</span> *offset, <span class="keyword">size_t</span> count);</div></pre></td></tr></table></figure>
<p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。</p>
<p>首先，它可以替代前面的 <code>read()</code> 和 <code>write()</code> 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。</p>
<p>其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：</p>
<p><img src="/images/951413iMgBlog/bd72f4a031bcd88db0ca233e59234832.png" alt="image.png"></p>
<p>当然这里还是有一次CPU来拷贝内存的过程，仍然有文件截断的问题。<code>sendfile()</code> 依然是一个适用性很窄的技术，最适合的场景基本也就是一个静态文件服务器了。</p>
<p>然而 <code>sendfile()</code> 本身是有很大问题的，从不同的角度来看的话主要是：</p>
<ol>
<li>首先一个是这个接口并没有进行标准化，导致 <code>sendfile()</code> 在 Linux 上的接口实现和其他类 Unix 系统的实现并不相同；</li>
<li>其次由于网络传输的异步性，很难在接收端实现和 <code>sendfile()</code> 对接的技术，因此接收端一直没有实现对应的这种技术；</li>
<li>最后从性能方面考量，因为 <code>sendfile()</code> 在把磁盘文件从内核缓冲区（page cache）传输到到套接字缓冲区的过程中依然需要 CPU 参与，这就很难避免 CPU 的高速缓存被传输的数据所污染。</li>
</ol>
<h3 id="SG-DMA（The-Scatter-Gather-Direct-Memory-Access）技术"><a href="#SG-DMA（The-Scatter-Gather-Direct-Memory-Access）技术" class="headerlink" title="SG-DMA（The Scatter-Gather Direct Memory Access）技术"></a>SG-DMA（<em>The Scatter-Gather Direct Memory Access</em>）技术</h3><p>上一小节介绍的 <code>sendfile()</code> 技术已经把一次数据读写过程中的 CPU 拷贝的降低至只有 1 次了，但是人永远是贪心和不知足的，现在如果想要把这仅有的一次 CPU 拷贝也去除掉，有没有办法呢？</p>
<p>当然有！通过引入一个新硬件上的支持，我们可以把这个仅剩的一次 CPU 拷贝也给抹掉：Linux 在内核 2.4 版本里引入了 DMA 的 scatter/gather – 分散/收集功能，并修改了 <code>sendfile()</code> 的代码使之和 DMA 适配。scatter 使得 DMA 拷贝可以不再需要把数据存储在一片连续的内存空间上，而是允许离散存储，gather 则能够让 DMA 控制器根据少量的元信息：一个包含了内存地址和数据大小的缓冲区描述符，收集存储在各处的数据，最终还原成一个完整的网络包，直接拷贝到网卡而非套接字缓冲区，避免了最后一次的 CPU 拷贝：</p>
<p><img src="/images/951413iMgBlog/2361e8c6dcfd20a67f404b684196c160.png" alt="image.png"></p>
<p>如果网卡支持 SG-DMA（<em>The Scatter-Gather Direct Memory Access</em>）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。</p>
<p>这就是所谓的<strong>零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。</strong> 数据传输过程就再也没有 CPU 的参与了，也因此 CPU 的高速缓存再不会被污染了，也不再需要 CPU 来计算数据校验和了，CPU 可以去执行其他的业务计算任务，同时和 DMA 的 I/O 任务并行，此举能极大地提升系统性能。</p>
<p>零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，<strong>只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。</strong></p>
<p>所以，总体来看，<strong>零拷贝技术可以把文件传输的性能提高至少一倍以上</strong>。</p>
<h3 id="splice"><a href="#splice" class="headerlink" title="splice()"></a>splice()</h3><p><code>sendfile()</code> + DMA Scatter/Gather 的零拷贝方案虽然高效，但是也有两个缺点：</p>
<ol>
<li>这种方案需要引入新的硬件支持；</li>
<li>虽然 <code>sendfile()</code> 的输出文件描述符在 Linux kernel 2.6.33 版本之后已经可以支持任意类型的文件描述符，但是输入文件描述符依然只能指向文件。</li>
</ol>
<p>这两个缺点限制了 <code>sendfile()</code> + DMA Scatter/Gather 方案的适用场景。为此，Linux 在 2.6.17 版本引入了一个新的系统调用 <code>splice()</code>，它在功能上和 <code>sendfile()</code> 非常相似，但是能够实现在任意类型的两个文件描述符时之间传输数据；而在底层实现上，<code>splice()</code>又比 <code>sendfile()</code> 少了一次 CPU 拷贝，也就是等同于 <code>sendfile()</code> + DMA Scatter/Gather，完全去除了数据传输过程中的 CPU 拷贝。</p>
<p><code>splice()</code> 所谓的写入数据到管道其实并没有真正地拷贝数据，而是玩了个 tricky 的操作：只进行内存地址指针的拷贝而不真正去拷贝数据。所以，数据 <code>splice()</code> 在内核中并没有进行真正的数据拷贝，因此 <code>splice()</code> 系统调用也是零拷贝。</p>
<p>还有一点需要注意，前面说过管道的容量是 16 个内存页，也就是 16 * 4KB = 64 KB，也就是说一次往管道里写数据的时候最好不要超过 64 KB，否则的话会 <code>splice()</code> 会阻塞住，除非在创建管道的时候使用的是 <code>pipe2()</code> 并通过传入 <code>O_NONBLOCK</code> 属性将管道设置为非阻塞。</p>
<h3 id="send-with-MSG-ZEROCOPY"><a href="#send-with-MSG-ZEROCOPY" class="headerlink" title="send() with MSG_ZEROCOPY"></a>send() with MSG_ZEROCOPY</h3><p>Linux 内核在 2017 年的 v4.14 版本接受了来自 Google 工程师 Willem de Bruijn 在 TCP 网络报文的通用发送接口 <code>send()</code> 中实现的 zero-copy 功能 (MSG_ZEROCOPY) 的 patch，通过这个新功能，用户进程就能够把用户缓冲区的数据通过零拷贝的方式经过内核空间发送到网络套接字中去，这个新技术和前文介绍的几种零拷贝方式相比更加先进，因为前面几种零拷贝技术都是要求用户进程不能处理加工数据而是直接转发到目标文件描述符中去的。Willem de Bruijn 在他的论文里给出的压测数据是：采用 netperf 大包发送测试，性能提升 39%，而线上环境的数据发送性能则提升了 5%~8%，官方文档陈述说这个特性通常只在发送 10KB 左右大包的场景下才会有显著的性能提升。一开始这个特性只支持 TCP，到内核 v5.0 版本之后才支持 UDP。</p>
<p>这个技术是基于 redhat 红帽在 2010 年给 Linux 内核提交的 virtio-net zero-copy 技术之上实现的，至于底层原理，简单来说就是通过 <code>send()</code> 把数据在用户缓冲区中的分段指针发送到 socket 中去，利用 page pinning 页锁定机制锁住用户缓冲区的内存页，然后利用 DMA 直接在用户缓冲区通过内存地址指针进行数据读取，实现零拷贝</p>
<p>目前来说，这种技术的主要缺陷有：</p>
<ol>
<li>只适用于大文件 (10KB 左右) 的场景，小文件场景因为 page pinning 页锁定和等待缓冲区释放的通知消息这些机制，甚至可能比直接 CPU 拷贝更耗时；</li>
<li>因为可能异步发送数据，需要额外调用 <code>poll()</code> 和 <code>recvmsg()</code> 系统调用等待 buffer 被释放的通知消息，增加代码复杂度，以及会导致多次用户态和内核态的上下文切换；</li>
<li>MSG_ZEROCOPY 目前只支持发送端，接收端暂不支持。</li>
</ol>
<h3 id="零拷贝应用"><a href="#零拷贝应用" class="headerlink" title="零拷贝应用"></a>零拷贝应用</h3><p>kafka就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一(利用磁盘顺序写；PageCache)。</p>
<p>非零拷贝代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">File.read(fileDesc, buf, len);</div><div class="line">Socket.send(socket, buf, len);</div></pre></td></tr></table></figure>
<p>Traditional data copying approach：</p>
<p><img src="/images/951413iMgBlog/gif/figure1.gif" alt="Traditional data copying approach"></p>
<p>Traditional context switches：</p>
<p><img src="/images/951413iMgBlog/gif/figure2.gif" alt="Traditional context switches"></p>
<p>如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 <code>transferTo</code> 方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Overridepublic</span> </div><div class="line"><span class="function"><span class="keyword">long</span> <span class="title">transferFrom</span><span class="params">(FileChannel fileChannel, <span class="keyword">long</span> position, <span class="keyword">long</span> count)</span> <span class="keyword">throws</span> IOException </span>&#123; </div><div class="line">    <span class="keyword">return</span> fileChannel.transferTo(position, count, socketChannel);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Data copy with transferTo()</p>
<p><img src="/images/951413iMgBlog/gif/figure3.gif" alt="Data copy with transferTo()"></p>
<p>Context switching with transferTo()：</p>
<p><img src="/images/951413iMgBlog/gif/figure4.gif" alt="Context switching when using transferTo()"></p>
<p>Data copies when transferTo() and gather operations are used</p>
<p><img src="/images/951413iMgBlog/gif/figure5.gif" alt="Data copies when transferTo() and gather operations are used"></p>
<p>如果 Linux 系统支持 <code>sendfile()</code> 系统调用，那么 <code>transferTo()</code> 实际上最后就会使用到 <code>sendfile()</code> 系统调用函数。</p>
<p>Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">http &#123;</div><div class="line">...</div><div class="line">    sendfile on</div><div class="line">...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>sendfile 配置的具体意思:</p>
<ul>
<li>设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。</li>
<li>设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。</li>
</ul>
<p>如果是大文件很容易消耗非常多的PageCache，不推荐使用PageCache（或者说零拷贝），建议使用异步IO+直接IO。</p>
<p>在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">location /video/ &#123; </div><div class="line">    sendfile on; </div><div class="line">    aio on; </div><div class="line">    directio 1024m; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当文件大小大于 <code>directio</code> 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。</p>
<h4 id="零拷贝性能"><a href="#零拷贝性能" class="headerlink" title="零拷贝性能"></a>零拷贝性能</h4><p>如果用零拷贝和不用零拷贝来做一个文件服务器，来对比下他们的性能</p>
<p><a href="https://developer.ibm.com/articles/j-zerocopy/" target="_blank" rel="external">Performance comparison: Traditional approach vs. zero copy</a></p>
<table>
<thead>
<tr>
<th style="text-align:left">File size</th>
<th style="text-align:left">Normal file transfer (ms)</th>
<th style="text-align:left">transferTo (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">7MB</td>
<td style="text-align:left">156</td>
<td style="text-align:left">45</td>
</tr>
<tr>
<td style="text-align:left">21MB</td>
<td style="text-align:left">337</td>
<td style="text-align:left">128</td>
</tr>
<tr>
<td style="text-align:left">63MB</td>
<td style="text-align:left">843</td>
<td style="text-align:left">387</td>
</tr>
<tr>
<td style="text-align:left">98MB</td>
<td style="text-align:left">1320</td>
<td style="text-align:left">617</td>
</tr>
<tr>
<td style="text-align:left">200MB</td>
<td style="text-align:left">2124</td>
<td style="text-align:left">1150</td>
</tr>
<tr>
<td style="text-align:left">350MB</td>
<td style="text-align:left">3631</td>
<td style="text-align:left">1762</td>
</tr>
<tr>
<td style="text-align:left">700MB</td>
<td style="text-align:left">13498</td>
<td style="text-align:left">4422</td>
</tr>
<tr>
<td style="text-align:left">1GB</td>
<td style="text-align:left">18399</td>
<td style="text-align:left">8537</td>
</tr>
</tbody>
</table>
<h2 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h2><p>什么是 DMA 技术？简单理解就是，<strong>在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务</strong></p>
<h2 id="RDMA"><a href="#RDMA" class="headerlink" title="RDMA"></a>RDMA</h2><p>remote DMA</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>
<p><a href="https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA" target="_blank" rel="external">https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA</a> 从Linux内存管理到零拷贝</p>
<p><a href="https://developer.ibm.com/articles/j-zerocopy/" target="_blank" rel="external">Efficient data transfer through zero copy</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/272200286" target="_blank" rel="external">CPU：一个故事看懂DMA</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--管理和碎片/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--管理和碎片/" itemprop="url">Linux内存--管理和碎片</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–管理和碎片"><a href="#Linux内存–管理和碎片" class="headerlink" title="Linux内存–管理和碎片"></a>Linux内存–管理和碎片</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="物理结构分析"><a href="#物理结构分析" class="headerlink" title="物理结构分析"></a>物理结构分析</h2><p>内存从物理结构上面分为：<strong>Channel &gt; DIMM（对应物理上售卖的内存条） &gt;Rank &gt; Chip &gt; Bank &gt; Row/Column。</strong></p>
<p>Chip就是DRAM芯片，一个chip里面会有很多bank。每个bank就是数据存储的实体，相当于一个二维矩阵，只要声明了column和row就可以从每个bank中取出8bit的数据。</p>
<p>具体可以看如下图，一个通道Channel可以是一个DIMM也可以是两个DIMM，甚至3个DIMM，图中是2个DIMM。</p>
<p><img src="/images/951413iMgBlog/image-20211222135852796.png" alt="image-20211222135852796"></p>
<h2 id="虚拟内存和物理内存"><a href="#虚拟内存和物理内存" class="headerlink" title="虚拟内存和物理内存"></a>虚拟内存和物理内存</h2><p>进程所操作的内存是一个虚拟内存，由OS来将这块虚拟内存映射到实际的物理内存上，这样做的好处是每个进程可以独占 128T 内存，任意地使用，系统上还运行了哪些进程已经与我们完全没有关系了（不需要考虑和其它进程之间的地址会冲突）。为变量和函数分配地址的活，我们交给链接器去自动安排就可以了。这一切都是因为虚拟内存能够提供内存地址空间的隔离，极大地扩展了可用空间。</p>
<p>操作系统管理着这种映射关系，所以你在写代码的时候，就不用再操心物理内存的使用情况了，你看到的内存就是虚拟内存。无论一个进程占用的内存资源有多大，在任一时刻，它需要的物理内存都是很少的。在这个推论的基础上，CPU 为每个进程只需要保留很少的物理内存就可以保证进程的正常执行了。</p>
<p>当程序中使用 malloc 等分配内存的接口时会将内存从待分配状态变成已分配状态，此时这块分配好的内存还没有真正映射到对应的物理内存上，这块内存就是未映射状态，因为它并没有被映射到相应的物理内存，直到对该块内存进行读写时，操作系统才会真正地为它分配物理内存。然后这个页面才能成为正常页面。</p>
<p>i7 处理器的页表也是存储在内存页里的，每个页表项都是 4 字节。所以，人们就将 1024 个页表项组成一张页表。这样一张页表的大小就刚好是 4K，占据一个内存页，这样就更加方便管理。而且，当前市场上主流的处理器也都选择将页大小定为 4K。</p>
<blockquote>
<p>虚拟地址在计算机体系结构里可以评为特优的一项技术；超线程、流水线、多发射只是优；cache则只是良好（成本高）</p>
</blockquote>
<h3 id="CPU-如何找到真实地址"><a href="#CPU-如何找到真实地址" class="headerlink" title="CPU 如何找到真实地址"></a>CPU 如何找到真实地址</h3><p><img src="/images/951413iMgBlog/image-20211107175201297.png" alt="image-20211107175201297"></p>
<ul>
<li>第一步是确定页目录基址。每个 CPU 都有一个页目录基址寄存器，最高级页表的基地址就存在这个寄存器里。在 X86 上，这个寄存器是 CR3。每一次计算物理地址时，MMU 都会从 CR3 寄存器中取出页目录所在的物理地址。</li>
<li>第二步是定位页目录项（PDE）。一个 32 位的虚拟地址可以拆成 10 位，10 位和 12 位三段，上一步找到的页目录表基址加上高 10 位的值乘以 4，就是页目录项的位置。这是因为，一个页目录项正好是 4 字节，所以 1024 个页目录项共占据 4096 字节，刚好组成一页，而 1024 个页目录项需要 10 位进行编码。这样，我们就可以通过最高 10 位找到该地址所对应的 PDE 了。</li>
<li>第三步是定位页表项（PTE）。页目录项里记录着页表的位置，CPU 通过页目录项找到页表的位置以后，再用中间 10 位计算页表中的偏移，可以找到该虚拟地址所对应的页表项了。页表项也是 4 字节的，所以一页之内刚好也是 1024 项，用 10 位进行编码。所以计算公式与上一步相似，用页表基址加上中间 10 位乘以 4，可以得到页表项的地址。</li>
<li>最后一步是确定真实的物理地址。上一步 CPU 已经找到页表项了，这里存储着物理地址，这才真正找到该虚拟地址所对应的物理页。虚拟地址的低 12 位，刚好可以对一页内的所有字节进行编码，所以我们用低 12 位来代表页内偏移。计算的公式是物理页的地址直接加上低 12 位。</li>
</ul>
<p>前面我们分析的是 32 位操作系统，那对于 64 位机器是不是有点不同呢？在 64 位的机器上，使用了 48 位的虚拟地址，所以它需要使用 4 级页表。它的结构与 32 位的 3 级页表是相似的，<strong>只是多了一级页目录，定位的过程也从 32 位的 4 步变成了 5 步。</strong></p>
<p><img src="/images/951413iMgBlog/image-20211107182305732.png" alt="image-20211107182305732"></p>
<p>8086最开始是按不同的作用将内存分为代码段、数据段等，386开始按页开始管理内存（混合有按段管理）。 现代的操作系统都是采用段式管理来做基本的权限管理，而对于内存的分配、回收、调度都是依赖页式管理。</p>
<h3 id="tlab-miss"><a href="#tlab-miss" class="headerlink" title="tlab miss"></a><a href="https://lwn.net/Articles/379748" target="_blank" rel="external">tlab miss</a></h3><p>tlb：从各级cache里分配的一块专用空间，用来存放页表(虚拟地址和物理地址的对应关系)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">#x86info -c</div><div class="line">Monitor/Mwait: min/max line size 64/64, ecx bit 0 support, enumeration extension</div><div class="line">SVM: revision 1, 32768 ASIDs</div><div class="line">Address Size: 48 bits virtual, 48 bits physical</div><div class="line">The physical package has 96 of 32768 possible cores implemented.</div><div class="line">L1 Data TLB (1G):           Fully associative. 64 entries.</div><div class="line">L1 Instruction TLB (1G):    Fully associative. 64 entries.</div><div class="line">L1 Data TLB (2M/4M):        Fully associative. 64 entries.</div><div class="line">L1 Instruction TLB (2M/4M): Fully associative. 64 entries.</div><div class="line">L1 Data TLB (4K):           Fully associative. 64 entries.</div><div class="line">L1 Instruction TLB (4K):    Fully associative. 64 entries.</div><div class="line">L1 Data cache:</div><div class="line">	Size: 32Kb	8-way associative.</div><div class="line">	lines per tag=1	line size=64 bytes.</div><div class="line">L1 Instruction cache:</div><div class="line">	Size: 32Kb	8-way associative.</div><div class="line">	lines per tag=1	line size=64 bytes.</div><div class="line">L2 Data TLB (1G):           Fully associative. 64 entries.</div><div class="line">L2 Instruction TLB (1G):    Disabled. 0 entries.</div><div class="line">L2 Data TLB (2M/4M):        4-way associative. 2048 entries.</div><div class="line">L2 Instruction TLB (2M/4M): 2-way associative. 512 entries.</div><div class="line">L2 Data TLB (4K):           8-way associative. 2048 entries.</div><div class="line">L2 Instruction TLB (4K):    4-way associative. 512 entries.</div><div class="line">L2 cache:</div><div class="line">	Size: 512Kb	8-way associative.</div><div class="line">	lines per tag=1	line size=64 bytes.</div><div class="line"></div><div class="line"> running at an estimated 2.55GHz</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20220928160318893.png" alt="image-20220928160318893"></p>
<p>TLB(Translation Lookaside Buffer) Cache用于缓存少量热点内存地址的mapping关系。TLB和L1一样每个core独享，由于制造成本和工艺的限制，响应时间需要控制在CPU Cycle级别的Cache容量只能存储几十个对象。那么TLB Cache在应对大量热点数据<code>Virual Address</code>转换的时候就显得捉襟见肘了。我们来算下按照标准的Linux页大小(page size) 4K，一个能缓存64元素的TLB Cache只能涵盖<code>4K*64 = 256K</code>的热点数据的内存地址，显然离理想非常遥远的。于是Huge Page就产生了。</p>
<p><a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer" target="_blank" rel="external">These are typical performance levels of a TLB</a>:</p>
<ul>
<li>Size: 12 bits – 4,096 entries</li>
<li>Hit time: 0.5 – 1 clock cycle</li>
<li>Miss penalty: 10 – 100 clock cycles</li>
<li>Miss rate: 0.01 – 1% (20–40% for sparse/graph applications)</li>
</ul>
<p>TLB也分为iTLB和dTLB, 分别顶在L1i和L1d前面（比L1更小更快，每个core独享tlb）</p>
<p><img src="/images/951413iMgBlog/tlb_lookup.png" alt="img"></p>
<p>以intel x86为例，一个cpu也就32到64个tlb, 超出这个范畴，就得去查页表。 每个型号的cpu都不一样，需要查看<a href="https://en.wikichip.org/wiki/WikiChip" target="_blank" rel="external">spec</a></p>
<p>进程分配到的不是内存的实际物理地址，而是一个经过映射后的虚拟地址，这么做的原因是为了让每个应用可以独享完整的虚拟地址，而不需要每个进程互相考虑使用内存的协调。</p>
<p>但是虚拟地址到物理地址的映射需要巨大的映射空间，如何用更少的内存消耗来管理庞大的内存（如果没有分级，4G内存对应着4MB的索引空间，一级比如使用4K就够了，多个二级使总共用4M，但是这4M大部分时候不用提前分配），Linux通过四级表项来做虚拟地址到物理地址的映射，这样4Kb就能管理256T内存，4级映射是时间换空间的典型案例。不过一般而言一个进程是远远用不了256T内存的，那么这四级映射大部分时候都是没必要的，所以实际用不了那么大的PageTable。</p>
<p><a href="https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA" target="_blank" rel="external">虚拟内存的核心原理</a>是：为每个程序设置一段”连续”的虚拟地址空间，把这个地址空间分割成多个具有连续地址范围的页 (page)，并把这些页和物理内存做映射，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令：</p>
<p><img src="/images/951413iMgBlog/1610941678194-bb42451f-b59a-475d-9b8d-b1085c18766d.png" alt="img"></p>
<p>在 <strong>内存管理单元（Memory Management Unit，MMU）</strong>进行地址转换时，如果页表项的 “在/不在” 位是 0，则表示该页面并没有映射到真实的物理页框，则会引发一个<strong>缺页中断</strong>，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出 (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个脏页 (dirty page)，需要写回磁盘更新改页面在磁盘上的副本，如果该页面是”干净”的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。</p>
<p>还需要了解的一个概念是<strong>转换检测缓冲器（Translation Lookaside Buffer，TLB，每个core一个TLB，类似L1 cache）</strong>，也叫快表，是用来加速虚拟地址映射的，因为虚拟内存的分页机制，页表一般是保存内存中的一块固定的存储区，导致进程通过 MMU 访问内存比直接访问内存多了一次内存访问，性能至少下降一半，因此需要引入加速机制，即 TLB 快表，TLB 可以简单地理解成页表的高速缓存，保存了最高频被访问的页表项，由于一般是硬件实现的，因此速度极快，MMU 收到虚拟地址时一般会先通过硬件 TLB 查询对应的页表号，若命中且该页表项的访问操作合法，则直接从 TLB 取出对应的物理页框号返回，若不命中则穿透到内存页表里查询，并且会用这个从内存页表里查询到最新页表项替换到现有 TLB 里的其中一个，以备下次缓存命中。</p>
<p>如果没有TLB那么每一次内存映射都需要查表四次然后才是一次真正的内存访问，代价比较高。</p>
<p>有了TLB之后，CPU访问某个虚拟内存地址的过程如下</p>
<ol>
<li>CPU产生一个虚拟地址</li>
<li>MMU从TLB中获取页表，翻译成物理地址</li>
<li>MMU把物理地址发送给L1/L2/L3/内存</li>
<li>L1/L2/L3/内存将地址对应数据返回给CPU</li>
</ol>
<p>由于第2步是类似于寄存器的访问速度，所以<strong>如果TLB能命中，则虚拟地址到物理地址的时间开销几乎可以忽</strong>略。tlab miss比较高的话开启内存大页对性能是有提升的，但是会有一定的内存浪费。</p>
<h2 id="内存布局"><a href="#内存布局" class="headerlink" title="内存布局"></a>内存布局</h2><ul>
<li>代码段：CPU 运行一个程序，实质就是在顺序执行该程序的机器码。一个程序的机器码会被组织到同一个地方。</li>
<li>数据段：程序在运行过程中必然要操作数据。这其中，对于有初值的变量，它的初始值会存放在程序的二进制文件中，而且，这些数据部分也会被装载到内存中，即程序的数据段。数据段存放的是程序中已经初始化且不为 0 的全局变量和静态变量。</li>
<li>BSS 段: 对于未初始化的全局变量和静态变量，因为编译器知道它们的初始值都是 0，因此便不需要再在程序的二进制映像中存放这么多 0 了，只需要记录他们的大小即可，这便是BSS段 。BSS 段这个缩写名字是 Block Started by Symbol，但很多人可能更喜欢把它记作 Better Save Space 的缩写。</li>
<li>堆是程序员可以自由申请的空间，当我们在写程序时要保存数据，优先会选择堆；</li>
<li>栈是函数执行时的活跃记录，这将是我们下一节课要重点分析的内容。</li>
</ul>
<p>数据段和 BSS 段里存放的数据也只能是部分数据，主要是全局变量和静态变量，但程序在运行过程中，仍然需要记录大量的临时变量，以及运行时生成的变量，这里就需要新的内存区域了，即程序的堆空间跟栈空间。与代码段以及数据段不同的是，堆和栈并不是从磁盘中加载，它们都是由程序在运行的过程中申请，在程序运行结束后释放。</p>
<p>总的来说，一个程序想要运行起来所需要的几块基本内存区域：代码段、数据段、BSS 段、堆空间和栈空间。下面就是内存布局的示意图：</p>
<p><img src="/images/951413iMgBlog/image-20211108115717113.png" alt="image-20211108115717113" style="zoom:35%;"></p>
<p>其它内存形态：</p>
<ul>
<li>存放加载的共享库的内存空间：如果一个进程依赖共享库，那对应的，该共享库的代码段、数据段、BSS 段也需要被加载到这个进程的地址空间中。</li>
<li>共享内存段：我们可以通过系统调用映射一块匿名区域作为共享内存，用来进行进程间通信。</li>
<li>内存映射文件：我们也可以将磁盘的文件映射到内存中，用来进行文件编辑或者是类似共享内存的方式进行进程通信。</li>
</ul>
<p>32位 x86机器下，通过 cat /proc/pid/maps 看到的进程所使用的内存分配空间：</p>
<p><img src="/images/951413iMgBlog/image-20211108120532370.png" alt="image-20211108120532370" style="zoom:33%;"></p>
<p>64位 x86机器下，通过 cat /proc/pid/maps 看到的进程所使用的内存分配空间：</p>
<p><img src="/images/951413iMgBlog/image-20211108120718732.png" alt="image-20211108120718732" style="zoom:40%;"></p>
<p>目前的 64 系统下的寻址空间是 2^48(太多用不完，高16位为Canonical空间），即 256TB。而且根据 canonical address 的划分，地址空间天然地被分割成两个区间，分别是 0x0 - 0x00007fffffffffff 和 0xffff800000000000 - 0xffffffffffffffff。这样就直接将低 128T 的空间划分为用户空间，高 128T 划分为内核空间。</p>
<p>brk:内核维护指向堆的顶部</p>
<p>Java程序对应的maps：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">#cat /proc/14011/maps</div><div class="line">00400000-00401000 r-xp 00000000 08:03 3935494                            /opt/taobao/install/ajdk-8.3.6_fp9-b30/bin/java</div><div class="line">00600000-00601000 rw-p 00000000 08:03 3935494                            /opt/taobao/install/ajdk-8.3.6_fp9-b30/bin/java</div><div class="line">ed400000-1001e0000 rw-p 00000000 00:00 0</div><div class="line">1001e0000-140000000 ---p 00000000 00:00 0</div><div class="line">7f86e8000000-7f8a7fc00000 rw-p 00000000 00:00 0</div><div class="line">..</div><div class="line">7f8aaecfa000-7f8aaeff8000 rw-p 00000000 00:00 0</div><div class="line">7f8aaeff8000-7f8aaf000000 r-xp 00000000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</div><div class="line">7f8aaf000000-7f8aaf1ff000 ---p 00008000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</div><div class="line">7f8aaf1ff000-7f8aaf200000 rw-p 00007000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</div><div class="line">..</div><div class="line">7f8ad8cea000-7f8ad8cec000 r--s 00004000 08:05 7078938                    /home/admin/drds-worker/lib/netty-handler-proxy-4.1.17.Final.jar</div><div class="line">7f8ad8cec000-7f8ad8cf5000 r--s 0006f000 08:05 7078952                    /home/admin/drds-worker/lib/log4j-1.2.17.jar</div><div class="line">7f8ad8cf5000-7f8ad8cf7000 r--s 00005000 08:05 7078960                    /home/admin/drds-worker/lib/objenesis-1.0.jar</div><div class="line">7f8ad8cf7000-7f8ad8cff000 r--s 0004b000 08:05 7078929                    /home/admin/drds-worker/lib/spring-aop-3.2.18.RELEASE.jar</div><div class="line">7f8ad8cff000-7f8ad8d00000 ---p 00000000 00:00 0</div><div class="line">7f8ad8d00000-7f8ad9000000 rw-p 00000000 00:00 0</div><div class="line">7f8ad90e3000-7f8ad90ef000 r--s 000b6000 08:05 7079066                    /home/admin/drds-worker/lib/transmittable-thread-local-2.5.1.jar</div><div class="line">7f8ad9dd8000-7f8ad9dfe000 r--s 0026f000 08:05 7078997                    /home/admin/drds-worker/lib/druid-1.1.7-preview_12.jar</div><div class="line">7f8ad9dfe000-7f8ad9dff000 ---p 00000000 00:00 0</div><div class="line">7f8ad9dff000-7f8ad9eff000 rw-p 00000000 00:00 0</div><div class="line">7f8ad9eff000-7f8ad9f00000 ---p 00000000 00:00 0</div><div class="line">7f8ad9f00000-7f8ada200000 rw-p 00000000 00:00 0</div><div class="line">7f8ada200000-7f8ada202000 r--s 00003000 08:05 7078944                    /home/admin/drds-worker/lib/liberate-rest-1.0.2.jar</div><div class="line">7f8ada202000-7f8ada206000 r--s 00036000 08:05 7078912                    /home/admin/drds-worker/lib/jackson-core-lgpl-1.9.6.jar</div><div class="line">7f8ada289000-7f8ada28b000 r--s 00001000 08:05 7078998                    /home/admin/drds-worker/lib/opencensus-contrib-grpc-metrics-0.10.0.jar</div><div class="line">7f8ada2b5000-7f8ada2b9000 r--s 0003a000 08:05 7079099                    /home/admin/drds-worker/lib/tddl-repo-mysql-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8ada2b9000-7f8ada2c6000 r--s 0007d000 08:05 7078982                    /home/admin/drds-worker/lib/grpc-core-1.9.0.jar</div><div class="line">7f8ada2c6000-7f8ada2d6000 r--s 00149000 08:05 7079000                    /home/admin/drds-worker/lib/protobuf-java-3.5.1.jar</div><div class="line">7f8ada2d6000-7f8ada2db000 r--s 0002b000 08:05 7078927                    /home/admin/drds-worker/lib/tddl-net-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8ada2db000-7f8ada2e0000 r--s 0002a000 08:05 7078939                    /home/admin/drds-worker/lib/grpc-netty-1.9.0.jar</div><div class="line">7f8ada2e0000-7f8ada2ff000 r--s 00150000 08:05 7078965                    /home/admin/drds-worker/lib/mockito-core-1.9.5.jar</div><div class="line">7f8ada2ff000-7f8ada300000 ---p 00000000 00:00 0</div><div class="line">7f8ada300000-7f8ada600000 rw-p 00000000 00:00 0</div><div class="line">7f8ada600000-7f8ada601000 r--s 00003000 08:05 7079089                    /home/admin/drds-worker/lib/ushura-1.0.jar</div><div class="line">7f8ae9ba2000-7f8ae9baa000 r-xp 00000000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</div><div class="line">7f8ae9baa000-7f8ae9da9000 ---p 00008000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</div><div class="line">7f8ae9da9000-7f8ae9daa000 rw-p 00007000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</div><div class="line">7f8ae9daa000-7f8ae9db6000 r-xp 00000000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9db6000-7f8ae9fb5000 ---p 0000c000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9fb5000-7f8ae9fb6000 r--p 0000b000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9fb6000-7f8ae9fb7000 rw-p 0000c000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9fb7000-7f8ae9fbd000 rw-p 00000000 00:00 0</div><div class="line">7f8ae9fbd000-7f8ae9fe7000 r-xp 00000000 08:03 3935961                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libjava.so</div><div class="line">7f8aebc03000-7f8aebc05000 r--s 00008000 08:05 7079098                    /home/admin/drds-worker/lib/grpc-stub-1.9.0.jar</div><div class="line">7f8aebc05000-7f8aebc07000 r--s 00020000 08:05 7078930                    /home/admin/drds-worker/lib/tddl-group-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8aebc07000-7f8aebc1f000 r--s 001af000 08:05 7079085                    /home/admin/drds-worker/lib/aspectjweaver-1.8.5.jar</div><div class="line">7f8aebc1f000-7f8aebc20000 ---p 00000000 00:00 0</div><div class="line">7f8aebc20000-7f8aebd20000 rw-p 00000000 00:00 0</div><div class="line">7f8aebd20000-7f8aebd35000 r-xp 00000000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebd35000-7f8aebf34000 ---p 00015000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebf34000-7f8aebf35000 r--p 00014000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebf35000-7f8aebf36000 rw-p 00015000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebf36000-7f8aec037000 r-xp 00000000 08:03 1837579                    /usr/lib64/libm-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aec037000-7f8aec236000 ---p 00101000 08:03 1837579                    /usr/lib64/libm-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aeca17000-7f8aeca18000 rw-p 0000d000 08:03 3936057                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/jli/libjli.so</div><div class="line">7f8aeca18000-7f8aeca2d000 r-xp 00000000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aeca2d000-7f8aecc2c000 ---p 00015000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aecc2c000-7f8aecc2d000 r--p 00014000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aecc2d000-7f8aecc2e000 rw-p 00015000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aecc2e000-7f8aecc45000 r-xp 00000000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aecc45000-7f8aece44000 ---p 00017000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aece44000-7f8aece45000 r--p 00016000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aece45000-7f8aece46000 rw-p 00017000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aece46000-7f8aece4a000 rw-p 00000000 00:00 0</div><div class="line">7f8aece4a000-7f8aecea1000 r-xp 00000000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</div><div class="line">7f8aecea1000-7f8aed0a0000 ---p 00057000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</div><div class="line">7f8aed0a0000-7f8aed0a3000 rw-p 00056000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</div><div class="line">7f8aed0a3000-7f8aed0b5000 rw-p 00000000 00:00 0</div><div class="line">7f8aed0b5000-7f8aed0d7000 r-xp 00000000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aed0d7000-7f8aed0dc000 r--s 00038000 08:05 7079012                    /home/admin/drds-worker/lib/org.osgi.core-4.2.0.jar</div><div class="line">7f8aed0dc000-7f8aed0e1000 r--s 00038000 08:05 7079018                    /home/admin/drds-worker/lib/commons-beanutils-1.9.3.jar</div><div class="line">7f8aed0e1000-7f8aed0e3000 r--s 00001000 08:05 7079033                    /home/admin/drds-worker/lib/j2objc-annotations-1.1.jar</div><div class="line">7f8aed0e3000-7f8aed0e8000 r--s 00017000 08:05 7079056                    /home/admin/drds-worker/lib/hibernate-jpa-2.1-api-1.0.0.Final.jar</div><div class="line">7f8aed1be000-7f8aed1c6000 rw-s 00000000 08:04 393222                     /tmp/hsperfdata_admin/14011</div><div class="line">7f8aed1c6000-7f8aed1ca000 ---p 00000000 00:00 0</div><div class="line">7f8aed1ca000-7f8aed2cd000 rw-p 00000000 00:00 0</div><div class="line">7f8aed2cd000-7f8aed2ce000 r--s 00005000 08:05 7079029                    /home/admin/drds-worker/lib/jersey-apache-connector-2.26.jar</div><div class="line">7f8aed2ce000-7f8aed2d1000 r--s 0000a000 08:05 7079027                    /home/admin/drds-worker/lib/metrics-jvm-1.7.4.jar</div><div class="line">7f8aed2d1000-7f8aed2d3000 r--s 00006000 08:05 7078961                    /home/admin/drds-worker/lib/tddl-client-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8aed2d3000-7f8aed2d4000 rw-p 00000000 00:00 0</div><div class="line">7f8aed2d4000-7f8aed2d5000 r--p 00000000 00:00 0</div><div class="line">7f8aed2d5000-7f8aed2d6000 rw-p 00000000 00:00 0</div><div class="line">7f8aed2d6000-7f8aed2d7000 r--p 00021000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aed2d7000-7f8aed2d8000 rw-p 00022000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aed2d8000-7f8aed2d9000 rw-p 00000000 00:00 0</div><div class="line">7fff087e0000-7fff08801000 rw-p 00000000 00:00 0                          [stack]</div><div class="line">7fff089c2000-7fff089c4000 r-xp 00000000 00:00 0                          [vdso]</div><div class="line">ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]</div></pre></td></tr></table></figure>
<h2 id="内存管理和使用"><a href="#内存管理和使用" class="headerlink" title="内存管理和使用"></a>内存管理和使用</h2><h3 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a>malloc</h3><p>malloc()分配内存时：</p>
<ul>
<li>如果用户分配的内存小于 128 KB，则通过 brk() 申请内存–在堆顶分配；</li>
<li>如果用户分配的内存大于 128 KB，则通过 mmap()  申请内存–从文件映射区域分配；</li>
</ul>
<p><img src="/images/951413iMgBlog/640-20220323120420806.png" alt="图片"></p>
<p><img src="/images/951413iMgBlog/640-20220323120443853.png" alt="图片"></p>
<p>对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」：</p>
<ul>
<li>malloc 通过 <strong>brk()</strong> 方式申请的内存，free 释放内存的时候，<strong>并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用</strong>–小内存分配避免反复调用系统操作导致上下文切换，缺点是没回收容易导致内存碎片进而浪费内存。brk分配出来的内存在maps中显示有heap字样；</li>
<li>malloc 通过 <strong>mmap()</strong> 方式申请的内存，free 释放内存的时候，<strong>会把内存归还给操作系统，内存得到真正的释放</strong>。并且mmap分配的虚拟内存都是缺页状态的。</li>
</ul>
<h3 id="malloc和mmap"><a href="#malloc和mmap" class="headerlink" title="malloc和mmap"></a>malloc和mmap</h3><p>glibc中的malloc/free 负责向内核批发内存（不需要每次分配都真正地去调用内核态来分配），分配好的内存按大小分成不同的桶，每次malloc的时候实际到对应的桶上摘取对应的块(slab)就好，用完free的时候挂回去。</p>
<p><img src="/images/951413iMgBlog/image-20211118121500859.png" alt="image-20211118121500859"></p>
<p>mmap映射内存</p>
<p><img src="/images/951413iMgBlog/image-20211108122432263.png" alt="image-20211108122432263" style="zoom:20%;"></p>
<p>私有匿名映射常用于分配内存，也就是申请堆内存</p>
<p>分桶式内存管理比简单算法无论是在算法效率方面，还是在碎片控制方面都有很大的提升。但它的缺陷也很明显：区域内部的使用率不够高和动态扩展能力不够好。例如，4 字节的区域提前消耗完了，但 8 字节的空闲区域还有很多，此时就会面临两难选择，如果直接分配 8 字节的区域，则区域内部浪费就比较多，如果不分配，则明明还有空闲区域，却无法成功分配。</p>
<p>为了解决以上问题所以搞了buddy</p>
<h3 id="node-gt-zone-gt-buddy-gt-slab"><a href="#node-gt-zone-gt-buddy-gt-slab" class="headerlink" title="node-&gt;zone-&gt;buddy-&gt;slab"></a>node-&gt;zone-&gt;buddy-&gt;slab</h3><p><img src="/images/oss/debfe12e-d1b9-49cd-988d-3f7fcba6ecd2.png" alt="img"></p>
<p>假如需要分配一块 4 字节大小的空间，但是在 4 字节的 free list 上找不到空闲区域，系统就会往上找，假如 8 字节和 16 字节的 free list 中也没有空闲区域，就会一直向上找到 32 字节的 free list。</p>
<p><img src="/images/951413iMgBlog/image-20211118120823595.png" alt="image-20211118120823595"></p>
<p>伙伴系统不会直接把 32 的空闲区域分配出去，因为这样做的话，会带来巨大的浪费。它会先把 32 字节分成两个 16 字节，把后边一个挂入到 16 字节的 free list 中。然后继续拆分前一半。前一半继续拆成两个 8 字节，再把后一半挂入到 8 字节的 free list，最后，把前一半 8 字节拿去分配，当然这里也要继续拆分成两个 4 字节的空闲区域，其中一个用于本次 malloc 分配，另一个则挂入到 4 字节的 free list。分配后的内存的状态如下所示：</p>
<p><img src="/images/951413iMgBlog/image-20211118120851731.png" alt="image-20211118120851731"></p>
<h3 id="查看zone"><a href="#查看zone" class="headerlink" title="查看zone"></a>查看zone</h3><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/KernelMemoryZones" target="_blank" rel="external">The zones are</a>:</p>
<ul>
<li><code>DMA</code> is the low 16 MBytes of memory. At this point it exists for historical reasons; once upon what is now a long time ago, there was hardware that could only do DMA into this area of physical memory.</li>
<li><code>DMA32</code> exists only in 64-bit Linux; it is the low 4 GBytes of memory, more or less. It exists because the transition to large memory 64-bit machines has created a class of hardware that can only do DMA to the low 4 GBytes of memory.(This is where people mutter about everything old being new again.)</li>
<li><strong><code>Normal</code></strong> is different on 32-bit and 64-bit machines. On 64-bit machines, it is all RAM from 4GB or so on upwards. On 32-bit machines it is all RAM from 16 MB to 896 MB for complex and somewhat historical reasons. Note that this implies that machines with a 64-bit kernel can have very small amounts of Normal memory unless they have significantly more than 4GB of RAM. For example, a 2 GB machine running a 64-bit kernel will have no Normal memory at all while a 4 GB machine will have only a tiny amount of it.</li>
<li><code>HighMem</code> exists only on 32-bit Linux; it is all RAM above 896 MB, including RAM above 4 GB on sufficiently large machines.</li>
</ul>
<p>每个zone下很多pages(大小为4K)，buddy就是这些Pages的组织管理者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"># cat /proc/zoneinfo |grep Node -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3972</div><div class="line">        min      0</div><div class="line">        low      0</div><div class="line">        high     0</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3993</div><div class="line">        managed  3972</div><div class="line">    nr_free_pages 3972</div><div class="line">    nr_alloc_batch 0</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     361132</div><div class="line">        min      30</div><div class="line">        low      37</div><div class="line">        high     45</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  430773</div><div class="line">        managed  361133</div><div class="line">    nr_free_pages 361132</div><div class="line">    nr_alloc_batch 8</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     96017308</div><div class="line">        min      16864</div><div class="line">        low      21080</div><div class="line">        high     25296</div><div class="line">        scanned  0</div><div class="line">        spanned  200736768</div><div class="line">        present  200736768</div><div class="line">        managed  197571780</div><div class="line">    nr_free_pages 96017308</div><div class="line">    nr_alloc_batch 3807</div><div class="line">    </div><div class="line"># free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            755         150         367           3         236         589</div><div class="line">Swap:             0           0           0</div></pre></td></tr></table></figure>
<p>每个页面大小是4K，很容易可以计算出每个 zone 的大小。比如对于上面 Node0 的 Normal， 197571780 <em> 4K/(1024</em>1024) = 753 GB。</p>
<p>dmidecode 可以查看到服务器上插着的所有内存条，也可以看到它是和哪个CPU直接连接的。每一个CPU以及和他直连的内存条组成了一个 <strong>node（节点）</strong></p>
<h3 id="proc-buddyinfo"><a href="#proc-buddyinfo" class="headerlink" title="/proc/buddyinfo"></a>/proc/buddyinfo</h3><p>/proc/buddyinfo记录了<strong>可用内存</strong>的情况。</p>
<p>Normal那行之后的第二列表示：  643847*2^1*Page_Size(4K) ;  第三列表示：  357451*2^2*Page_Size(4K)  ，高阶内存指的是2^3及更大的内存块。</p>
<p>应用申请大块连续内存（高阶内存，一般之4阶及以上, 也就是64K以上–2^4*4K）时，容易导致卡顿。这是因为大块连续内存确实系统需要触发回收或者碎片整理，需要一定的时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div></pre></td><td class="code"><pre><div class="line">#cat /proc/buddyinfo </div><div class="line">Node 0, zone      DMA      1      1      1      0      2      1      1      0      1      1      3 </div><div class="line">Node 0, zone    DMA32      2      5      3      6      2      0      4      4      2      2    404 </div><div class="line">Node 0, zone   Normal 243430 643847 357451  32531   9508   6159   3917   2960  17172   2633  22854</div><div class="line"></div><div class="line">如果是多node机器：</div><div class="line">#cat /proc/buddyinfo</div><div class="line">Node 0, zone      DMA      4      6      3      2      3      3      1      1      2      3      1</div><div class="line">Node 0, zone    DMA32   1607   1619   1552   1520   1370   1065    827    576    284    105     13</div><div class="line">Node 0, zone   Normal  38337 145731 222145 199776 151452  91969  38086  10037   1762    104      1</div><div class="line">Node 1, zone   Normal  21521 147637 299185 245533 172451  81459  19451   7198    579      3      0</div><div class="line">Node 2, zone   Normal  68427 538670 446906 229138 123555  62539  21161   4407   1122    166    274</div><div class="line">Node 3, zone   Normal  27353  54601 114355 123568 101892  79098  48610  21036   5021    475      6</div><div class="line">Node 4, zone   Normal  45802  42758   8573 184548 148397  70540  20772   4147    381    148    109</div><div class="line">Node 5, zone   Normal  19514  39583 140493 167901 134774  61888  22998   6326    457     32      0</div><div class="line">Node 6, zone   Normal 104493 378362 355158  93138  12928   2248   1019    663    172     40    121</div><div class="line">Node 7, zone   Normal  34185 256886 249560  95547  54526  51022  28180   9757   2038   1351    280</div><div class="line"></div><div class="line">[root@hygon8 15:50 /root]</div><div class="line">#numactl -H</div><div class="line">available: 8 nodes (0-7)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 64 65 66 67 68 69 70 71</div><div class="line">node 0 size: 64083 MB</div><div class="line">node 0 free: 49838 MB</div><div class="line">node 1 cpus: 8 9 10 11 12 13 14 15 72 73 74 75 76 77 78 79</div><div class="line">node 1 size: 64480 MB</div><div class="line">node 1 free: 43596 MB</div><div class="line">node 2 cpus: 16 17 18 19 20 21 22 23 80 81 82 83 84 85 86 87</div><div class="line">node 2 size: 64507 MB</div><div class="line">node 2 free: 44216 MB</div><div class="line">node 3 cpus: 24 25 26 27 28 29 30 31 88 89 90 91 92 93 94 95</div><div class="line">node 3 size: 64507 MB</div><div class="line">node 3 free: 51095 MB</div><div class="line">node 4 cpus: 32 33 34 35 36 37 38 39 96 97 98 99 100 101 102 103</div><div class="line">node 4 size: 64507 MB</div><div class="line">node 4 free: 32877 MB</div><div class="line">node 5 cpus: 40 41 42 43 44 45 46 47 104 105 106 107 108 109 110 111</div><div class="line">node 5 size: 64507 MB</div><div class="line">node 5 free: 33430 MB</div><div class="line">node 6 cpus: 48 49 50 51 52 53 54 55 112 113 114 115 116 117 118 119</div><div class="line">node 6 size: 64507 MB</div><div class="line">node 6 free: 14233 MB</div><div class="line">node 7 cpus: 56 57 58 59 60 61 62 63 120 121 122 123 124 125 126 127</div><div class="line">node 7 size: 63483 MB</div><div class="line">node 7 free: 36577 MB</div><div class="line">node distances:</div><div class="line">node   0   1   2   3   4   5   6   7</div><div class="line">  0:  10  16  16  16  28  28  22  28</div><div class="line">  1:  16  10  16  16  28  28  28  22</div><div class="line">  2:  16  16  10  16  22  28  28  28</div><div class="line">  3:  16  16  16  10  28  22  28  28</div><div class="line">  4:  28  28  22  28  10  16  16  16</div><div class="line">  5:  28  28  28  22  16  10  16  16</div><div class="line">  6:  22  28  28  28  16  16  10  16</div><div class="line">  7:  28  22  28  28  16  16  16  10</div><div class="line">  </div><div class="line">[root@hygon8 15:51 /root]</div><div class="line">#cat /proc/pagetypeinfo</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    0, zone      DMA, type    Unmovable      1      2      1      1      3      2      0      0      1      0      0</div><div class="line">Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      3      1</div><div class="line">Node    0, zone      DMA, type  Reclaimable      3      4      2      1      0      1      1      1      1      0      0</div><div class="line">Node    0, zone      DMA, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type    Unmovable    151    164    162    165    140     78     19      8      0      0      0</div><div class="line">Node    0, zone    DMA32, type      Movable   1435   1430   1374   1335   1214    974    798    563    281     98     12</div><div class="line">Node    0, zone    DMA32, type  Reclaimable     21     25     16     20     16     13     10      5      3      7      1</div><div class="line">Node    0, zone    DMA32, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type    Unmovable   4849   6607   4133   1629    654    121     15      3      0      0      0</div><div class="line">Node    0, zone   Normal, type      Movable  21088 &gt;100000 &gt;100000 &gt;100000 &gt;100000  90231  37197   9379   1552     83      1</div><div class="line">Node    0, zone   Normal, type  Reclaimable    153    139   3012   3113   2437   1617    874    655    210     21      0</div><div class="line">Node    0, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 0, zone      DMA            1            6            1            0            0</div><div class="line">Node 0, zone    DMA32           27          974           15            0            0</div><div class="line">Node 0, zone   Normal          856        30173          709            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    1, zone   Normal, type    Unmovable    842   2898   2495   1316    490    102     23      1      2      0      0</div><div class="line">Node    1, zone   Normal, type      Movable  22484 &gt;100000 &gt;100000 &gt;100000 &gt;100000  80084  18922   6889     48      4      0</div><div class="line">Node    1, zone   Normal, type  Reclaimable      1   2022   3850   3534   2582   1273    506    308    529      0      0</div><div class="line">Node    1, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    1, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 1, zone   Normal          810        31221          737            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    2, zone   Normal, type    Unmovable   2833   6802   3888   1636    329      3      1      2      0      0      0</div><div class="line">Node    2, zone   Normal, type      Movable  72017 &gt;100000 &gt;100000 &gt;100000 &gt;100000  61710  20764   4242    841     55    239</div><div class="line">Node    2, zone   Normal, type  Reclaimable    114      8   2056   2221   1544    826    396    163    281    111     35</div><div class="line">Node    2, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    2, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 2, zone   Normal         1066        31063          639            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    3, zone   Normal, type    Unmovable   2508   6171   3802   1502    365     93     30      1      2      0      0</div><div class="line">Node    3, zone   Normal, type      Movable  23396  48450 &gt;100000 &gt;100000  99802  77850  47910  20587   4796    428      5</div><div class="line">Node    3, zone   Normal, type  Reclaimable     10      0    609   2111   1726   1155    670    448    223     46      1</div><div class="line">Node    3, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    3, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 3, zone   Normal          768        31425          575            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    4, zone   Normal, type    Unmovable   3817   3739   1716    992    261     39      4      1      0      0      1</div><div class="line">Node    4, zone   Normal, type      Movable  27857  39138   6875 &gt;100000 &gt;100000  70501  20752   4115    362     49    104</div><div class="line">Node    4, zone   Normal, type  Reclaimable      1      8      3      5      0      0     16     31     19     97      4</div><div class="line">Node    4, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    4, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 4, zone   Normal          712        31706          350            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    5, zone   Normal, type    Unmovable   4875   4728   3165   1202    464     67      3      0      0      0      0</div><div class="line">Node    5, zone   Normal, type      Movable  18382  34874 &gt;100000 &gt;100000 &gt;100000  61296  22711   6235    348     32      0</div><div class="line">Node    5, zone   Normal, type  Reclaimable     16      0      1      7      2    525    284     91    109      0      0</div><div class="line">Node    5, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    5, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 5, zone   Normal          736        31716          316            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    6, zone   Normal, type    Unmovable  10489   6842   2821    434    257     22      1      1      1      3      0</div><div class="line">Node    6, zone   Normal, type      Movable  90841 &gt;100000 &gt;100000  92129  11336   1526    704    552    141     34    118</div><div class="line">Node    6, zone   Normal, type  Reclaimable    434     41      0    576   1338    700    314    110     30      5      3</div><div class="line">Node    6, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    6, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 6, zone   Normal          807        31686          275            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    7, zone   Normal, type    Unmovable   1516   1894   2285    908    633    121     16      7      4      2      0</div><div class="line">Node    7, zone   Normal, type      Movable  18209 &gt;100000 &gt;100000  93283  52811  50349  27973   9703   2026   1341    248</div><div class="line">Node    7, zone   Normal, type  Reclaimable      0      1      0   1341   1082    552    191     47      8      8     32</div><div class="line">Node    7, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    7, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 7, zone   Normal         1262        31265          241            0            0</div></pre></td></tr></table></figure>
<h3 id="proc-pagetypeinfo"><a href="#proc-pagetypeinfo" class="headerlink" title="/proc/pagetypeinfo"></a>/proc/pagetypeinfo</h3><p><code>cat /proc/pagetypeinfo</code>, 你可以看到当前系统里伙伴系统里各个尺寸的可用连续内存块数量。unmovable pages是不可以被迁移的，比如slab等kmem都不可以被迁移，因为内核里面对这些内存很多情况下是通过指针来访问的，而不是通过页表，如果迁移的话，就会导致原来的指针访问出错。</p>
<p><img src="/images/oss/2e73247c-8a10-43e6-bb0e-49ecfff14268.png" alt="img"></p>
<p><strong>当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重</strong></p>
<p>alloc_pages分配内存的时候就到上面对应大小的free_area的链表上寻找可用连续页面。<code>alloc_pages</code>是怎么工作的呢？我们举个简单的小例子。假如要申请8K-连续两个页框的内存。为了描述方便，我们先暂时忽略UNMOVEABLE、RELCLAIMABLE等不同类型</p>
<p><img src="/images/oss/16ebe996-0e3a-4d67-810f-3121b457271e.png" alt="img"></p>
<p>基于伙伴系统的内存分配中，有可能需要将大块内存拆分成两个小伙伴。在释放中，可能会将两个小伙伴合并再次组成更大块的连续内存。</p>
<blockquote>
<p>伙伴系统中的伙伴指的是两个内存块，大小相同，地址连续，同属于一个大块区域。</p>
</blockquote>
<p>对于应用来说基本分配单位是4K(开启大页后一般是2M)，对于内核来说4K有点浪费。所以内核又专门给自己定制了一个更精细的内存管理系统slab。</p>
<h3 id="slab"><a href="#slab" class="headerlink" title="slab"></a>slab</h3><p>对于内核运行中实际使用的对象来说，多大的对象都有。有的对象有1K多，但有的对象只有几百、甚至几十个字节。如果都直接分配一个 4K的页面 来存储的话也太浪费了，所以伙伴系统并不能直接使用。</p>
<p>在伙伴系统之上，<strong>内核又给自己搞了一个专用的内存分配器， 叫slab</strong>。</p>
<p>这个分配器最大的特点就是，一个slab内只分配特定大小、甚至是特定的对象。这样当一个对象释放内存后，另一个同类对象可以直接使用这块内存。通过这种办法极大地降低了碎片发生的几率。</p>
<h4 id="kmem-cache"><a href="#kmem-cache" class="headerlink" title="kmem cache"></a>kmem cache</h4><p>slabtop和/proc/slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<p>通过查看 <code>/proc/slabinfo</code> 我们可以查看到所有的 kmem cache。</p>
<p><img src="/images/oss/5135d81f-6985-4d6a-8896-e451c0ba20f5.png" alt="img"></p>
<p>slabtop 按占用内存从大往小进行排列。用来分析 slab 内存开销。</p>
<p><img src="/images/oss/0d8a26db-3663-40af-b215-f8601ef23676.png" alt="0d8a26db-3663-40af-b215-f8601ef23676.png (1388×1506)"></p>
<p>无论是 <code>/proc/slabinfo</code>，还是 slabtop 命令的输出。里面都包含了每个 cache 中 slab的如下几个关键属性。</p>
<ul>
<li>objsize：每个对象的大小</li>
<li><p>objperslab：一个 slab 里存放的对象的数量</p>
</li>
<li><p>pagesperslab：一个slab 占用的页面的数量，每个页面4K，这样也就能算出每个 slab 占用的内存大小。</p>
</li>
</ul>
<p>比如如下TCP slabinfo中可以看到每个slab占用8(pagesperslab)个Page(8<em>4096=32768)，每个对象的大小是1984(objsize)，每个slab存放了16(objperslab)个对象. 那么1984 </em>16=31744，现在的空间基本用完，剩下接近1K，又放不下一个1984大小的对象，算是额外开销了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#cat /proc/slabinfo |grep -E &quot;active_objs|TCP&quot;</div><div class="line"># name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</div><div class="line">tw_sock_TCP         5372   5728    256   32    2 : tunables    0    0    0 : slabdata    179    179      0</div><div class="line">TCP                 6090   6144   1984   16    8 : tunables    0    0    0 : slabdata    384    384      0</div></pre></td></tr></table></figure>
<h2 id="内存分配和延迟"><a href="#内存分配和延迟" class="headerlink" title="内存分配和延迟"></a>内存分配和延迟</h2><p>内存不够、脏页太多、碎片太多，都会导致分配失败，从而触发回收，导致卡顿。</p>
<h3 id="系统中脏页过多引起-load-飙高"><a href="#系统中脏页过多引起-load-飙高" class="headerlink" title="系统中脏页过多引起 load 飙高"></a>系统中脏页过多引起 load 飙高</h3><p>直接回收过程中，如果存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟，而且因为这个过程本身是阻塞式的，所以又可能进一步导致系统中处于 D 状态的进程数增多，最终的表现就是系统的 load 值很高。</p>
<p><img src="/images/oss/f16438b744a248d7671d5ac7317b0a98.png" alt="image.png" style="zoom: 50%;"></p>
<p>可以通过 sar -r 来观察系统中的脏页个数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ sar -r 1</div><div class="line">07:30:01 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</div><div class="line">09:20:01 PM   5681588   2137312     27.34         0   1807432    193016      2.47    534416   1310876         4</div><div class="line">09:30:01 PM   5677564   2141336     27.39         0   1807500    204084      2.61    539192   1310884        20</div><div class="line">09:40:01 PM   5679516   2139384     27.36         0   1807508    196696      2.52    536528   1310888        20</div><div class="line">09:50:01 PM   5679548   2139352     27.36         0   1807516    196624      2.51    536152   1310892        24</div></pre></td></tr></table></figure>
<p>kbdirty 就是系统中的脏页大小，它同样也是对 /proc/vmstat 中 nr_dirty 的解析。你可以通过调小如下设置来将系统脏页个数控制在一个合理范围:</p>
<blockquote>
<p>vm.dirty_background_bytes = 0</p>
<p>vm.dirty_background_ratio = 10</p>
<p>vm.dirty_bytes = 0</p>
<p>vm.dirty_expire_centisecs = 3000</p>
<p>vm.dirty_ratio = 20</p>
</blockquote>
<p>至于这些值调整大多少比较合适，也是因系统和业务的不同而异，我的建议也是一边调整一边观察，将这些值调整到业务可以容忍的程度就可以了，即在调整后需要观察业务的服务质量 (SLA)，要确保 SLA 在可接受范围内。调整的效果你可以通过 /proc/vmstat 来查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#grep &quot;nr_dirty_&quot; /proc/vmstat</div><div class="line">nr_dirty_threshold 3071708</div><div class="line">nr_dirty_background_threshold 1023902</div></pre></td></tr></table></figure>
<p>在4.20的内核并且sar 的版本为12.3.3可以看到PSI（Pressure-Stall Information）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">some avg10=45.49 avg60=10.23 avg300=5.41 total=76464318</div><div class="line">full avg10=40.87 avg60=9.05 avg300=4.29 total=58141082</div></pre></td></tr></table></figure>
<p>你需要重点关注 avg10 这一列，它表示最近 10s 内存的平均压力情况，如果它很大（比如大于 40）那 load 飙高大概率是由于内存压力，尤其是 Page Cache 的压力引起的。</p>
<p><img src="/images/oss/cf58f10a523e1e4f0db443be3f54fc04.png" alt="image.png"></p>
<h3 id="容器中的内存回收"><a href="#容器中的内存回收" class="headerlink" title="容器中的内存回收"></a>容器中的内存回收</h3><blockquote>
<p>kswapd线程(每个node一个kswapd进程，负责本node）回收内存时，可以先对脏页进行回写（writeback）再进行回收，而直接内存回收只回收干净页。也叫同步回收.</p>
<p>直接内存回收是在当前进程的上下文中进行的，要等内存回收完成才能继续尝试进行分配，所以是阻塞了当前进程的执行，会导致响应延迟增加</p>
</blockquote>
<p>如果是在容器里，也就是在某个子memory cgroup 中，那么在分配内存后，还有一个记账（charge）的步骤，就是要把这次分配的内存页记在某个memory cgroup的账上，这样才能控制这个容器里的进程所能使用的内存数量。</p>
<p>在开源社区的linux代码中，如果charge 失败，也就是说，当新分配的内存加上原先的usage超过了limit，就会触发内存回收，try_to_free_mem_cgroup_pages，这个也是同步回收，等同于直接内存回收（发生在当前进程的上下文忠），所以会对应用的响应造成影响（表现为卡顿）。</p>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 4G/8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a href="https://www.atatech.org/articles/163233" target="_blank" rel="external">https://www.atatech.org/articles/163233</a> <a href="https://www.atatech.org/articles/97130" target="_blank" rel="external">https://www.atatech.org/articles/97130</a></p>
<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</div><div class="line">Node 0, zone      DMA</div><div class="line">Node 0, zone    DMA32</div><div class="line">Node 0, zone   Normal</div><div class="line">Node 1, zone   Normal</div><div class="line"></div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3975</div><div class="line">        min      20</div><div class="line">        low      25</div><div class="line">        high     30</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3996</div><div class="line">        managed  3975</div><div class="line">    nr_free_pages 3975</div><div class="line">    nr_alloc_batch 5</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     382873</div><div class="line">        min      2335</div><div class="line">        low      2918</div><div class="line">        high     3502</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  513024</div><div class="line">        managed  450639</div><div class="line">    nr_free_pages 382873</div><div class="line">    nr_alloc_batch 584</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     11105097</div><div class="line">        min      61463</div><div class="line">        low      76828</div><div class="line">        high     92194</div><div class="line">        scanned  0</div><div class="line">        spanned  12058624</div><div class="line">        present  12058624</div><div class="line">        managed  11859912</div><div class="line">    nr_free_pages 11105097</div><div class="line">    nr_alloc_batch 12344</div><div class="line">    </div><div class="line">    low = 5/4 * min</div><div class="line">high = 3/2 * min</div><div class="line"></div><div class="line"></div><div class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=499 MB</div><div class="line"></div><div class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=624 MB</div><div class="line"></div><div class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=802 MB</div></pre></td></tr></table></figure>
<h2 id="内存碎片化导致rt升高的诊断"><a href="#内存碎片化导致rt升高的诊断" class="headerlink" title="内存碎片化导致rt升高的诊断"></a>内存碎片化导致rt升高的诊断</h2><p>判定方法如下：</p>
<ol>
<li>运行 sar -B 观察 pgscand/s，其含义为每秒发生的直接内存回收次数，当在一段时间内持续大于 0 时，则应继续执行后续步骤进行排查；</li>
<li>运行 <code>cat /sys/kernel/debug/extfrag/extfrag_index</code> 观察内存碎片指数，重点关注 order &gt;= 3 的碎片指数，当接近 1.000 时，表示碎片化严重，当接近 0 时表示内存不足；</li>
<li>运行 <code>cat /proc/buddyinfo, cat /proc/pagetypeinfo</code> 查看内存碎片情况， 指标含义<a href="https://man7.org/linux/man-pages/man5/proc.5.html" target="_blank" rel="external">参考</a> ，同样关注 order &gt;= 3 的剩余页面数量，pagetypeinfo 相比 buddyinfo 展示的信息更详细一些，根据迁移类型 （伙伴系统通过迁移类型实现反碎片化）进行分组，需要注意的是，<strong>当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重</strong>，我们需要结合其他工具来排查具体原因，在本文就不做过多介绍了；</li>
<li>对于 CentOS 7.6 等支持 BPF 的 kernel 也可以运行我们研发的 <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt" target="_blank" rel="external">drsnoop</a>，<a href="https://github.com/iovisor/bcc/blob/master/tools/compactsnoop_example.txt" target="_blank" rel="external">compactsnoop</a> 工具对延迟进行定量分析，使用方法和解读方式请参考对应文档；</li>
<li>(Opt) 使用 ftrace 抓取 mm_page_alloc_extfrag 事件，观察因内存碎片从备用迁移类型“盗取”页面的信息。</li>
</ol>
<h2 id="一个阿里云ECS-因为宿主机碎片导致性能衰退的案例"><a href="#一个阿里云ECS-因为宿主机碎片导致性能衰退的案例" class="headerlink" title="一个阿里云ECS 因为宿主机碎片导致性能衰退的案例"></a>一个阿里云ECS 因为宿主机碎片导致性能衰退的案例</h2><p>LVS后面三个RS在同样压力流量下，其中一个节点CPU非常高，通过top看起来是所有操作都很慢，像是CPU被降频了一样，但是直接跑CPU Prime性能又没有问题</p>
<p><img src="/images/oss/8bbb5c886dc06196546daec46712ff71.png" alt="image.png"></p>
<p>原因：ECS所在的宿主机内存碎片比较严重，导致分配到的内存主要是4K Page，在ECS中大页场景下会慢很多</p>
<p>通过 <strong>openssl speed aes-256-ige 能稳定重现</strong> 在大块的加密上慢很多</p>
<p><img src="/images/oss/8e15e91d4dcc61bbd329e7283c7c7500.png" alt="image.png"></p>
<p>小块上性能一致，这也就是为什么算Prime性能没问题。导致慢只涉及到大块内存分配的场景，这里需要映射到宿主机，但是碎片多分配慢导致了问题。</p>
<p>如果reboot ECS的话实际只是就地重启ECS，仍然使用的reboot前分配好的宿主机内存，不会解决问题。重启ECS中的进程也不会解决问题，只有将ECS迁移到别的物理机（也就是通过控制台重启，会重新选择物理机）才有可能解决这个问题。</p>
<p>或者购买新的ECS机型（比如第6代之后ECS）能避免这个问题。</p>
<p>ECS内部没法查看到这个碎片，只能在宿主机上通过命令查看大页情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">有问题NC上buddyinfo信息</div><div class="line">$cat /proc/buddyinfo</div><div class="line">Node 0, zone      DMA      1      1      0      0      2      1      1      0      1      1      3</div><div class="line">Node 0, zone    DMA32     23     23     17     15     13      9      8      8      4      3    367</div><div class="line">Node 0, zone   Normal 295291 298652 286048 266597 218191 156837  93156  45930  25856      0      0</div><div class="line"></div><div class="line">最新建的vm，大页不多</div><div class="line">$sudo cat /proc/9550/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">210944</div><div class="line">------------------------</div><div class="line">第一台正常ECS所在的NC</div><div class="line">$cat /proc/buddyinfo</div><div class="line">Node 0, zone      DMA      1      1      0      0      2      1      1      0      1      1      3</div><div class="line">Node 0, zone    DMA32      7      5      5      9      8      4      6     10      5      5    366</div><div class="line">Node 0, zone   Normal 203242 217888 184465 176280 148612 102122  55787  26642  24824      0      0</div><div class="line"></div><div class="line">早期的vm，大页充足</div><div class="line">$sudo cat /proc/87369/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">8275968</div><div class="line"></div><div class="line">近期的vm，大页不够</div><div class="line">$sudo cat /proc/22081/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">251904</div><div class="line"></div><div class="line">$sudo cat /proc/44073/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">10240</div></pre></td></tr></table></figure>
<h2 id="内存使用分析"><a href="#内存使用分析" class="headerlink" title="内存使用分析"></a>内存使用分析</h2><h3 id="pmap"><a href="#pmap" class="headerlink" title="pmap"></a>pmap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">pmap -x 24282 | less</div><div class="line">24282:   /usr/sbin/rsyslogd -n</div><div class="line">Address           Kbytes     RSS   Dirty Mode  Mapping</div><div class="line">000055ce1a99f000     596     580       0 r-x-- rsyslogd</div><div class="line">000055ce1ac34000      12      12      12 r---- rsyslogd</div><div class="line">000055ce1ac37000      28      28      28 rw--- rsyslogd</div><div class="line">000055ce1ac3e000       4       4       4 rw---   [ anon ]</div><div class="line">000055ce1c1f1000     364     204     204 rw---   [ anon ]</div><div class="line">00007fff8b5a4000     132      20      20 rw---   [ stack ]</div><div class="line">00007fff8b5e6000      12       0       0 r----   [ anon ]</div><div class="line">00007fff8b5e9000       8       4       0 r-x--   [ anon ]</div><div class="line">---------------- ------- ------- -------</div><div class="line">total kB          620060   17252    3304</div></pre></td></tr></table></figure>
<ul>
<li>Address：占用内存的文件的内存起始地址。</li>
<li>Kbytes：占用内存的字节数。</li>
<li>RSS：实际占用内存大小。</li>
<li>Dirty：脏页大小。</li>
<li>Mapping：占用内存的文件，[anon] 为已分配的内存，[stack] 为程序堆栈</li>
</ul>
<h3 id="proc-pid"><a href="#proc-pid" class="headerlink" title="/proc/pid/"></a>/proc/pid/</h3><p><code>/proc/[pid]/</code> 下面与进程内存相关的文件主要有<code>maps , smaps, status</code>。<br>maps： 文件可以查看某个进程的代码段、栈区、堆区、动态库、内核区对应的虚拟地址<br>smaps: 显示每个分区更详细的内存占用数据，能看到一个动态库被共享了几次<br>status: 包含了所有CPU活跃的信息，该文件中的所有值都是从系统启动开始累计到当前时刻</p>
<h2 id="Java内存使用分析"><a href="#Java内存使用分析" class="headerlink" title="Java内存使用分析"></a><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr007.html" target="_blank" rel="external">Java内存使用分析</a></h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">创建1000个线程，ss为2M</div><div class="line">java -XX:NativeMemoryTracking=detail -Xms10g -Xmx10g -Xmn5g -XX:ReservedCodeCacheSize=512m -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m -XX:MaxDirectMemorySize=1g -Xss2048K ThreadPoolExample</div><div class="line"></div><div class="line">分析结果：</div><div class="line">#jcmd 81849 VM.native_memory summary</div><div class="line">81849:</div><div class="line"></div><div class="line">Native Memory Tracking:</div><div class="line"></div><div class="line">Total: reserved=14737064KB, committed=13157168KB</div><div class="line">-                 Java Heap (reserved=10485760KB, committed=10485760KB)</div><div class="line">                            (mmap: reserved=10485760KB, committed=10485760KB)</div><div class="line"></div><div class="line">-                     Class (reserved=1102016KB, committed=50112KB)</div><div class="line">                            (classes #416)</div><div class="line">                            (malloc=45248KB #1420)</div><div class="line">                            (mmap: reserved=1056768KB, committed=4864KB)</div><div class="line"></div><div class="line">//committed 已向OS提请分配，实际要到使用时page fault才会实际分配物理内存并在RSS中反应出来</div><div class="line">-                    Thread (reserved=2134883KB, committed=2134883KB)//reserved还没分配,不能访问</div><div class="line">                            (thread #1070) //1000个应用线程，加70个JVM native线程</div><div class="line">                            (stack: reserved=2128820KB, committed=2128820KB) //需要2G多点</div><div class="line">                            (malloc=3500KB #5390)</div><div class="line">                            (arena=2563KB #2138)</div><div class="line"></div><div class="line">-                      Code (reserved=532612KB, committed=4620KB)</div><div class="line">                            (malloc=132KB #528)</div><div class="line">                            (mmap: reserved=532480KB, committed=4488KB)</div><div class="line"></div><div class="line">-                        GC (reserved=430421KB, committed=430421KB)</div><div class="line">                            (malloc=50737KB #235)</div><div class="line">                            (mmap: reserved=379684KB, committed=379684KB)</div><div class="line"></div><div class="line">-                  Compiler (reserved=137KB, committed=137KB)</div><div class="line">                            (malloc=6KB #53)</div><div class="line">                            (arena=131KB #3)</div><div class="line"></div><div class="line">-                  Internal (reserved=48901KB, committed=48901KB)</div><div class="line">                            (malloc=48869KB #14030)</div><div class="line">                            (mmap: reserved=32KB, committed=32KB)</div><div class="line"></div><div class="line">-                    Symbol (reserved=1479KB, committed=1479KB)</div><div class="line">                            (malloc=959KB #110)</div><div class="line">                            (arena=520KB #1)</div><div class="line"></div><div class="line">-    Native Memory Tracking (reserved=608KB, committed=608KB)</div><div class="line">                            (malloc=193KB #2556)</div><div class="line">                            (tracking overhead=415KB)</div><div class="line"></div><div class="line">-               Arena Chunk (reserved=248KB, committed=248KB)</div><div class="line">                            (malloc=248KB)</div></pre></td></tr></table></figure>
<p>We can see two types of memory:</p>
<ul>
<li><strong>Reserved</strong> — the size which is guaranteed to be available by a host’s OS (but still not allocated and cannot be accessed by JVM) — it’s just a promise</li>
<li><strong>Committed</strong> — already taken, accessible, and allocated by JVM</li>
</ul>
<h3 id="page-fault"><a href="#page-fault" class="headerlink" title="page fault"></a>page fault</h3><p>内核给用户态申请的内存，默认都只是一段虚拟地址空间而已，并没有分配真正的物理内存。在第一次读写的时候才触发物理内存的分配，这个过程叫做page fault。那么，为了访问到真正的物理内存，page fault的时候，就需要更新对应的page table了。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>
<p><a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="external">rsyslog占用内存高</a></p>
<p><a href="https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html" target="_blank" rel="external">https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html</a></p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html" target="_blank" rel="external">鸟哥 journald 介绍</a></p>
<p><a href="https://mp.weixin.qq.com/s/OR2XB4J76haGc1THeq7WQg" target="_blank" rel="external">说出来你可能不信，内核这家伙在内存的使用上给自己开了个小灶！</a></p>
<p><a href="https://zhengheng.me/2018/08/27/socket-use-slab-dentry/" target="_blank" rel="external">socket 与 slab dentry</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/11/如何在工作中学习--V2.0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/11/如何在工作中学习--V2.0/" itemprop="url">如何在工作中学习--V2.0</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-11T12:30:03+08:00">
                2020-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何在工作中学习–V2-0"><a href="#如何在工作中学习–V2-0" class="headerlink" title="如何在工作中学习–V2.0"></a>如何在工作中学习–V2.0</h1><p>本文被网友翻译的<a href="https://medium.com/@cai.eason/learn-and-improve-the-right-technical-skills-7a0bc5123e1" target="_blank" rel="external">英文版</a> （medium 需要梯子）</p>
<blockquote>
<p>先说一件值得思考的事情：高考的时候大家都是一样的教科书，同一个教室，同样的老师辅导，时间精力基本差不多，可是最后别人考的是清华北大或者一本，而你的实力只能考个三本，为什么？ 当然这里主要是智商的影响，那么其他因素呢？智商解决的问题能不能后天用其他方式来补位一下？</p>
</blockquote>
<p>大家平时都看过很多方法论的文章，看的时候很爽觉得非常有用，但是一两周后基本还是老样子了。其中有很大一部分原因那些方法对脑力有要求、或者方法论比较空缺少落地的步骤。 下文中描述的方式方法是不需要智商也能学会的，非常具体的。</p>
<h2 id="关键问题点"><a href="#关键问题点" class="headerlink" title="关键问题点"></a>关键问题点</h2><h3 id="为什么你的知识积累不了？"><a href="#为什么你的知识积累不了？" class="headerlink" title="为什么你的知识积累不了？"></a>为什么你的知识积累不了？</h3><p>有些知识看过就忘、忘了再看，实际碰到问题还是联系不上这个知识，这其实是知识的积累出了问题，没有深入的理解自然就不能灵活运用，也就谈不上解决问题了。这跟大家一起看相同的高考教科书但是高考结果不一样是一个原因。问题出在了理解上，每个人的理解能力不一样（智商），绝大多数人对知识的理解要靠不断地实践（做题）来巩固。</p>
<h3 id="同样实践效果不一样？"><a href="#同样实践效果不一样？" class="headerlink" title="同样实践效果不一样？"></a>同样实践效果不一样？</h3><p>同样工作一年碰到了10个问题（或者说做了10套高考模拟试卷），但是结果不一样，那是因为在实践过程中方法不够好。或者说你对你为什么做对了、为什么做错了没有去复盘</p>
<p>假如碰到一个问题，身边的同事解决了，而我解决不了。那么我就去想这个问题他是怎么解决的，他看到这个问题后的逻辑和思考是怎么样的，有哪些知识指导了他这么逻辑推理，这些知识哪些我也知道但是我没有想到这么去运用推理（说明我对这个知识理解的不到位导致灵活运用缺乏）；这些知识中又有哪些是我不知道的（知识缺乏，没什么好说的快去Google什么学习下–有场景案例和目的加持，学习理解起来更快）。</p>
<p>等你把这个问题基本按照你同事掌握的知识和逻辑推理想明白后，需要再去琢磨一下他的逻辑推理解题思路中有没有不对的，有没有啰嗦的地方，有没有更直接的方式（对知识更好地运用）。</p>
<p>我相信每个问题都这么去实践的话就不应该再抱怨灵活运用、举一反三，同时知识也积累下来了，这种场景下积累到的知识是不会那么容易忘记的。</p>
<p>这就是向身边的牛人学习，同时很快超过他的办法。这就是为什么高考前你做了10套模拟题还不如其他人做一套的效果好</p>
<p><strong>知识+逻辑 基本等于你的能力</strong>，知识让你知道那个东西，逻辑让你把东西和问题联系起来</p>
<p><strong>这里的问题你可以理解成方案、架构、设计等</strong></p>
<h3 id="系统化的知识哪里来？"><a href="#系统化的知识哪里来？" class="headerlink" title="系统化的知识哪里来？"></a>系统化的知识哪里来？</h3><p>知识之间是可以联系起来的并且像一颗大树一样自我生长，但是当你都没理解透彻，自然没法产生联系，也就不能够自我生长了。</p>
<p>真正掌握好的知识点会慢慢生长连接最终组成一张大网</p>
<p>但是我们最容易陷入的就是掌握的深度、系统化（工作中碎片时间过多，学校里缺少时间）不够，所以一个知识点每次碰到花半个小时学习下来觉得掌握了，但是3个月后就又没印象了。总是感觉自己在懵懵懂懂中，或者一个领域学起来总是不得要领，根本的原因还是在于：宏观整体大图了解不够（缺乏体系，每次都是盲人摸象）；关键知识点深度不够，理解不透彻，这些关键点就是这个领域的骨架、支点、抓手。缺了抓手自然不能生长，缺了宏观大图容易误入歧途。</p>
<p>我们有时候发现自己在某个领域学起来特别快，但是换个领域就总是不得要领，问题出在了上面，即使花再多时间也是徒然。这也就是为什么学霸看两个小时的课本比你看两天效果还好，感受下来还觉得别人好聪明，是不是智商比我高啊。</p>
<p>所以新进入一个领域的时候要去找他的大图和抓手。</p>
<p>好的同事总是能很轻易地把这个大图交给你，再顺便给你几个抓手，你就基本入门了，这就是培训的魅力，这种情况肯定比自学效率高多了。但是目前绝大部分的培训都做不到这点</p>
<h3 id="好的逻辑又怎么来？"><a href="#好的逻辑又怎么来？" class="headerlink" title="好的逻辑又怎么来？"></a>好的逻辑又怎么来？</h3><p>实践、复盘</p>
<p><img src="/images/951413iMgBlog/webp-5540564.jpg" alt="img"></p>
<h2 id="讲个前同事的故事"><a href="#讲个前同事的故事" class="headerlink" title="讲个前同事的故事"></a>讲个前同事的故事</h2><p>有一个前同事是5Q过来的，负责技术（所有解决不了的问题都找他），这位同学从chinaren出道，跟着王兴一块创业5Q，5Q在学校靠鸡腿打下大片市场，最后被陈一舟的校内收购（据说被收购后5Q的好多技术都走了，最后王兴硬是呆在校内网把合约上的所有钱都拿到了）。这位同学让我最佩服的解决问题的能力，好多问题其实他也不一定就擅长，但是他就是有本事通过Help、Google不停地验证尝试就把一个不熟悉的问题给解决了，这是我最羡慕的能力，在后面的职业生涯中一直不停地往这个方面尝试。</p>
<h3 id="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"><a href="#应用刚启动连接到数据库的时候比较慢，但又不是慢查询" class="headerlink" title="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"></a>应用刚启动连接到数据库的时候比较慢，但又不是慢查询</h3><ol>
<li>这位同学的解决办法是通过tcpdump来分析网络包，看网络包的时间戳和网络包的内容，然后找到了具体卡在了哪里。</li>
<li>如果是专业的DBA可能会通过show processlist 看具体连接在做什么，比如看到这些连接状态是 <strong>authentication</strong> 状态，然后再通过Google或者对这个状态的理解知道创建连接的时候MySQL需要反查IP、域名这里比较耗时，通过配置参数 <strong>skip-name-resolve</strong> 跳过去就好了。</li>
<li><p>如果是MySQL的老司机，一上来就知道连接慢的话跟 <strong>skip-name-resolve</strong> 关系最大。</p>
<p> 在我眼里这三种方式都解决了问题，最后一种最快但是纯靠积累和经验，换个问题也许就不灵了；第一种方式是最牛逼和通用的，只需要最少的知识就把问题解决了。</p>
</li>
</ol>
<p>我当时跟着他从sudo、ls等linux命令开始学起。当然我不会轻易去打搅他问他，每次碰到问题我尽量让他在我的电脑上来操作，解决后我再自己复盘，通过history调出他的所有操作记录，看他在我的电脑上用Google搜啥了，然后一个个去学习分析他每个动作，去想他为什么搜这个关键字，复盘完还有不懂的再到他面前跟他面对面的讨论他为什么要这么做，指导他这么做的知识和逻辑又是什么。</p>
<h2 id="有哪些好的行为帮你更好地掌握知识"><a href="#有哪些好的行为帮你更好地掌握知识" class="headerlink" title="有哪些好的行为帮你更好地掌握知识"></a>有哪些好的行为帮你更好地掌握知识</h2><h2 id="笔记-写博客"><a href="#笔记-写博客" class="headerlink" title="笔记+写博客"></a>笔记+写博客</h2><p>看东西的时候要做笔记，要不当时看得再爽也很容易忘记，我们需要反复复习来加深印象和理解，复习的根据就是笔记（不可能再完整又看一次），笔记整理出里面的要点和你的盲点。</p>
<p>一段时间后把相关的笔记整理成一篇体系性的博客文章，这样既加深了理解又系统化了相关知识。以后再看到跟这篇博客相关的案例、知识点时不断地更新博客（完善你的知识点）</p>
<h2 id="场景式学习、体感的来源、面对问题学习"><a href="#场景式学习、体感的来源、面对问题学习" class="headerlink" title="场景式学习、体感的来源、面对问题学习"></a>场景式学习、体感的来源、面对问题学习</h2><p>前面提到的对知识的深入理解这有点空，如何才能做到深入理解？</p>
<h3 id="举个学习TCP三次握手例子"><a href="#举个学习TCP三次握手例子" class="headerlink" title="举个学习TCP三次握手例子"></a>举个学习TCP三次握手例子</h3><p>经历稍微丰富点的工程师都觉得TCP三次握手看过很多次、很多篇文章了，但是文章写得再好似乎当时理解了，但是总是过几个月就忘了或者一看就懂，过一阵子被人一问就模模糊糊了，或者两个为什么就答不上了，自己都觉得自己的回答是在猜或者不确定</p>
<p>为什么会这样呢？而学其它知识就好通畅多了，我觉得这里最主要的是我们对TCP缺乏体感，比如没有几个工程师去看过TCP握手的代码，也没法想象真正的TCP握手是如何在电脑里运作的（打电话能给你一些类似的体感，但是细节覆盖面不够）。</p>
<p>如果这个时候你一边学习的时候一边再用wireshark抓包看看三次握手具体在干什么，比抽象的描述实在多了，你能看到具体握手的一来一回，并且看到一来一回带了哪些内容，这些内容又是用来做什么、为什么要带，这个时候你再去看别人讲解的理论顿时会觉得好理解多了，以后也很难忘记。</p>
<p>但是这里很多人执行能力不强，想去抓包，但是觉得要下载安装wireshark，要学习wireshark就放弃了。只看不动手当然是最舒适的，但是这个最舒适给了你在学习的假象，没有结果。</p>
<p>这是不是跟你要解决一个难题非常像，这个难题需要你去做很多事，比如下载源代码（翻不了墙，放弃）；比如要编译（还要去学习那些编译参数，放弃）；比如要搭建环境（太琐屑，放弃）。你看这中间九九八十一难你放弃了一难都取不了真经。这也是为什么同样学习、同样的问题，他能学会，他能解决，你不可以。</p>
<p><img src="/images/951413iMgBlog/167211888bc4f2a368df3d16c68e6d51.png" alt="167211888bc4f2a368df3d16c68e6d51.png"></p>
<h3 id="再来看一个解决问题的例子"><a href="#再来看一个解决问题的例子" class="headerlink" title="再来看一个解决问题的例子"></a>再来看一个解决问题的例子</h3><p><a href="https://www.atatech.org/articles/73174" target="_blank" rel="external">会员系统双11优化这个问题</a>对我来说，我是个外来者，完全不懂这里面的部署架构、业务逻辑。但是在问题的关键地方（会员认为自己没问题–压力测试正常的；淘宝API更是认为自己没问题，alimonitor监控显示正常），结果就是会员的同学说我们没有问题，淘宝API肯定有问题，然后就不去思考自己这边可能出问题的环节了。思想上已经甩包了，那么即使再去review流程、环节也就不会那么仔细，自然更是发现不了问题了。</p>
<p>但是我的经验告诉我要有证据地甩包，或者说拿着证据优雅地甩包，这迫使我去找更多的细节证据（证据要给力哦，不能让人家拍回来）。如果我是这么说的，这个问题在淘宝API这里，你看理由是…………，我做了这些实验，看到了这些东东。那么淘宝API那边想要证明我的理由错了就会更积极地去找一些数据。</p>
<p>事实上我就是做这些实验找证据过程中发现了会员的问题，这就是态度、执行力、知识、逻辑能力综合下来拿到的一个结果。我最不喜欢的一句话就是我的程序没问题，因为我的逻辑是这样的，不会错的。你当然不会写你知道的错误逻辑，程序之所以有错误都是在你的逻辑、意料之外的东西。有很多次一堆人电话会议中扯皮的时候，我一般把电话静音了，直接上去人肉一个个过对方的逻辑，一般来说电话会议还没有结束我就给出来对方逻辑之外的东西。</p>
<h2 id="钉子式学习方法和系统性学习方法"><a href="#钉子式学习方法和系统性学习方法" class="headerlink" title="钉子式学习方法和系统性学习方法"></a>钉子式学习方法和系统性学习方法</h2><p>系统性学习方法就是想掌握MySQL，那么搞几本MySQL专著和MySQL 官方DOC看下来，一般课程设计的好的话还是比较容易掌握下来，绝大部分时候都是这种学习方法，可是在种学习方法的问题在于学完后当时看着似乎理解了，但是很容易忘记，一片一片地系统性的忘记，并且缺少应用能力（理解不深）。这是因为一般人对知识的理解没那么容易真正<strong>理解（掌握或者说应用）</strong>。</p>
<p>钉子式的学习方式，就是在一大片知识中打入几个桩，反复演练将这个桩不停地夯实，夯稳，做到在这个知识点上用通俗的语言跟小白都能讲明白，然后再这几个桩中间发散像星星之火燎原一样把整个一片知识都掌握下来。这种学习方法的缺点就是很难找到一片知识点的这个点，然后没有很好整合的话知识过于零散。</p>
<p>钉子式学习方法看着慢但是因为这样掌握的更透彻和牢固实际最终反而快。</p>
<p>我们常说的一个人很聪明，就是指系统性的看看书就都理解了，是真的理解那种，还能灵活运用，但是大多数普通人就不是这样的，看完书似乎理解了，实际几周后基本都忘记了，真正实践需要用的时候还是用不好。</p>
<h3 id="举个Open-SSH的例子"><a href="#举个Open-SSH的例子" class="headerlink" title="举个Open-SSH的例子"></a>举个Open-SSH的例子</h3><p>为了做通 SSH 的免密登陆，大家都需要用到 ssh-keygen/ssh-copy-id， 如果我们把这两个命令当一个小的钉子的话，会去了解ssh-keygen做了啥（生成了密钥对），或者ssh-copy-id 的时候报错了（原来是需要秘钥对），然后将 ssh-keygen 生成的pub key复制到server的~/.ssh/authorized_keys 中。</p>
<p>然后你应该会对这个原理要有一些理解（更大的钉子），于是理解了密钥对，和ssh验证的流程，顺便学会怎么看ssh debug信息，那么接下来网络上各种ssh攻略、各种ssh卡顿的解决都是很简单的事情了。</p>
<p>比如你通过SSH可以解决这些问题：</p>
<ul>
<li>免密登陆</li>
<li>ssh卡顿</li>
<li>怎么去掉ssh的时候需要手工多输入yes</li>
<li>我的ssh怎么很快就断掉了</li>
<li>我怎么样才能一次通过跳板机ssh到目标机器</li>
<li>我怎么样通过ssh科学上网</li>
<li>我的ansible（底层批量命令都是基于ssh）怎么这么多问题，到底是为什么</li>
<li>我的git怎么报网络错误了</li>
<li>X11 forward我怎么配置不好</li>
<li>https为什么需要随机数加密，还需要签名</li>
<li>…………</li>
</ul>
<p>这些问题都是一步步在扩大ssh的外延，让这个钉子变成一个巨大的桩。</p>
<p>然后就会学习到一些<a href="/2018/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82SSH--SSH%E8%8A%B1%E5%BC%8F%E7%8E%A9%E6%B3%95/">高级一些的ssh配置</a>，比如干掉经常ssh的时候要yes一下(StrictHostKeyChecking=no), 或者怎么配置一下ssh就不会断线了（ServerAliveInterval=15），或者将 ssh跳板机-&gt;ssh server的过程做成 ssh server一步就可以了(ProxyCommand)，进而发现用 ssh的ProxyCommand很容易科学上网了，或者git有问题的时候轻而易举地把ssh debug打开，对git进行debug了……</p>
<p>这基本都还是ssh的本质范围，像ansible、git在底层都是依赖ssh来通讯的，你会发现学、调试X11、ansible和git简直太容易了。</p>
<p>另外理解了ssh的秘钥对，也就理解了非对称加密，同时也很容易理解https流程（SSL），同时知道对称和非对称加密各自的优缺点，SSL为什么需要用到这两种加密算法了。</p>
<p>你看一个简单日常的知识我们只要沿着它用钉子精神，深挖细挖你就会发现知识之间的连接，这个小小的知识点成为你知识体系的一根结实的柱子。</p>
<p>我见过太多的老的工程师、年轻的工程师，天天在那里ssh 密码，ssh 跳板机，ssh 目标机，一小会ssh断了，重来一遍；或者ssh后卡住了，等吧……</p>
<p>在这个问题上表现得没有求知欲、没有探索精神、没有一次把问题搞定的魄力，所以就习惯了</p>
<h2 id="空洞的口号"><a href="#空洞的口号" class="headerlink" title="空洞的口号"></a>空洞的口号</h2><p>很多老师和文章都会教大家：举一反三、灵活运用、活学活用、多做多练。但是只有这些口号是没法落地的，落地的基本步骤就是前面提到的，却总是被忽视了。</p>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举一反三，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>肯定知识效率最牛逼，但是拥有这种技能的人毕竟非常少（天生的高智商吧）。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快的掌握一个新知识，非常气人。剩下的绝大部分只能拼时间+方法+总结等也能掌握一些知识</p>
<p>非常遗憾我就是工程效率型，只能羡慕那些知识效率型的学霸。但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，两者之间当然没有明显的界限，知识积累多了逻辑训练好了在别人看来你的智商就高了</p>
<h2 id="知识分两种"><a href="#知识分两种" class="headerlink" title="知识分两种"></a>知识分两种</h2><p>一种是通用知识（不是说对所有人通用，而是说在一个专业领域去到哪个公司都能通用）；另外一种是跟业务公司绑定的特定知识</p>
<p>通用知识没有任何疑问碰到后要非常饥渴地扑上去掌握他们（受益终生，这还有什么疑问吗？）。对于特定知识就要看你对业务需要掌握的深度了，肯定也是需要掌握一些的，特定知识掌握好的一般在公司里混的也会比较好。</p>
<p>一个具体知识体系里面又有一些核心知识点（抓手、essential knowledge），也就是掌握可以快速帮你膨胀、延伸到其他相关知识的知识点。</p>
<p>还有一些知识、工具一旦掌握就能帮你贯穿、具象、理解别的知识点，比如网络知识体系中的wireshark；理工科中的数学；知识体系中的学习方法、行为方式。我们要多去发现这些知识、工具（how？）</p>
<h2 id="哪些品质能够决定一个人学习好坏"><a href="#哪些品质能够决定一个人学习好坏" class="headerlink" title="哪些品质能够决定一个人学习好坏"></a>哪些品质能够决定一个人学习好坏</h2><h3 id="排在第一名的品质是复盘、总结能力"><a href="#排在第一名的品质是复盘、总结能力" class="headerlink" title="排在第一名的品质是复盘、总结能力"></a>排在第一名的品质是复盘、总结能力</h3><p>简单的说，这个能力就是这个孩子心里是否有个“小教练”，能够每次跳脱出当前任务，帮助自己分析，失败在哪里，成功在哪里，如何进阶，如何训练等等。</p>
<p>举几个例子：</p>
<ol>
<li><p>如果写不出作文，这个“小教练”能告诉孩子，是没有素材，还是文字能力不强。</p>
<p>如果是文字能力不强，应该如何训练（是造句，还是拆段落）</p>
</li>
<li><p>如果数学题做不出来，这个“小教练”能告诉孩子，我的弱点在哪里，哪个类型题我有重大问题，是因为哪里没有理解和打通。</p>
</li>
</ol>
<p>有内化的“自我教练”，这个能力系数是1.67。也就是其他能力相当，学习效果可以翻1.67倍。</p>
<h3 id="排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等"><a href="#排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等" class="headerlink" title="排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等"></a>排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等</h3><p>在内心“小教练”能把问题进行拆解之后，建构能力能把这些问题进行排序，应该怎么做更合理。</p>
<p>最终怎么把训练步骤整合。遇见一个问题，先干什么，再干什么等等。</p>
<p>有很强的顺序能力，系数为1.44。也就是其他能力相当，学习效果可以翻1.44倍。</p>
<h3 id="排在第三名的能力是智商和过去成绩"><a href="#排在第三名的能力是智商和过去成绩" class="headerlink" title="排在第三名的能力是智商和过去成绩"></a>排在第三名的能力是智商和过去成绩</h3><p>这个毋庸置疑，聪明做事就会简单一些。平均效应系数为0.67。</p>
<p>也就是说，其他能力相当，智商高和过去成绩好，学习效果提升67%</p>
<p>智商重要程度应该比这里更高，但是实际高智商的太少，<strong>大多都是因为基础知识好给人产生了智商高的误解！</strong></p>
<h3 id="排在第四名的能力是自我驱动力"><a href="#排在第四名的能力是自我驱动力" class="headerlink" title="排在第四名的能力是自我驱动力"></a>排在第四名的能力是自我驱动力</h3><p>简单的说，知道自己为什么学习，能够自我鼓励，遇见失败能抗挫，有很强的心理驱动力。</p>
<p>平均效应系数为0.48。也就是说，其他能力相当，有自我驱动力的人，学习效果提升48%。</p>
<h3 id="排在第五名的才是集中注意力"><a href="#排在第五名的才是集中注意力" class="headerlink" title="排在第五名的才是集中注意力"></a>排在第五名的才是集中注意力</h3><p>也就是说，注意力强。注意力对学习影响，并没有很多家长想象的那么大。</p>
<p>注意力的平均效应系数为0.44</p>
<p>也就是说，其他能力相当，注意力好的孩子，效果能提升44%</p>
<p>———总结一下——-</p>
<p>学习提升的个人因素：</p>
<p>自我分析，自我教练的元认知能力 》 逻辑排序与制定计划的建构能力》 智商和过去成绩 》自我驱动力》 集中注意力。</p>
<p>很多家长痴迷于“专注力”。当然专注力是一个效应量很强的学习力，但是从整体数据看，对学习的提升效果，仅仅排到第五名。</p>
<p>帮助孩子建立元认知能力和建构能力的培训，才能给他们对终身学习有帮助的技能包</p>
<p>以上缺少了对方法的落地执行能力的评估，实际这是影响最大的</p>
<h2 id="为什么一直努力却没有结果"><a href="#为什么一直努力却没有结果" class="headerlink" title="为什么一直努力却没有结果"></a><strong>为什么一直努力却没有结果</strong></h2><p>人生不是走斜坡，你持续走就可以走到巅峰；人生像走阶梯（有时候需要跳上台阶），每一阶有每一阶的难点，学物理有物理的难点，学漫画有漫画的难点，你没有克服难点，再怎么努力都是原地跳。所以当你克服难点，你跳上去就不会下来了。</p>
<p>这里的克服难点可以理解成真正掌握知识点，大多时候只是似是而非，所以一直在假忙碌、假学习，只有真正掌握后才像是上了个台阶。</p>
<p>这句话本身就是对知识的具象化理解，本来要描述的是有些人很容易达到目的、有些人则很难，我们很容易归结到天赋等原因，但是还是不能完全理解这个事情，知道天赋差异也不会帮助到我们跳楼梯，而这句话给出了一个形象的描述，一下子就会记住并在日常中尽量尝试跳上去和脱离假忙碌的状态。</p>
<p><img src="/images/951413iMgBlog/6856fb6af3fa3b940c38c71edbc3bb73.png" alt="image.png"></p>
<p>在上述截图中，温伯格拿他自己玩弹子球游戏的经验，来描述”跃迁模式”。<br>　 为啥在跃迁之前会出现一个【低谷】捏？温伯格认为：要想提升到一个新的 level，你需要放弃某些你擅长的技能，然后尝试你所【不】擅长的技能。<br>温伯格以他的弹子球游戏举例说：<br>    为了得到更高的分数，他必须放弃以前熟悉的【低】难度技巧，转而尝试某种【高】难度技巧。在练习高难度技巧的过程中，他的分数会跌得比原先更低（也就是截图中下凹的低谷）。经过了一段时间的练习，当他掌握了高难度技巧，他的游戏得分就突然飞跃到一个新的 level。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/02/举三反一--从理论知识到实际问题的推导/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/02/举三反一--从理论知识到实际问题的推导/" itemprop="url">举三反一--从理论知识到实际问题的推导</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-02T10:30:03+08:00">
                2020-11-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="举三反一–从理论知识到实际问题的推导"><a href="#举三反一–从理论知识到实际问题的推导" class="headerlink" title="举三反一–从理论知识到实际问题的推导"></a>举三反一–从理论知识到实际问题的推导</h1><blockquote>
<p>怎么样才能获取举三反一的秘籍， 普通人为什么要案例来深化对理论知识的理解。</p>
</blockquote>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举三反一，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理解理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>对于费曼（参考费曼学习法）这样的聪明人就是很容易看到一个理论知识就能理解这个理论知识背后的本质。</p>
<p>肯定知识效率最牛逼，但是拥有这种能力的人毕竟非常少。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快地掌握一个新知识。剩下的绝大部分只能拼时间(刷题)+方法+总结等也能掌握一些知识</p>
<p>但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，即使灰色地带也行啊。</p>
<p>接下来看看TCP状态中的CLOSE_WAIT状态的含义</p>
<h2 id="先看TCP连接状态图"><a href="#先看TCP连接状态图" class="headerlink" title="先看TCP连接状态图"></a>先看TCP连接状态图</h2><p>这是网络、书本上凡是描述TCP状态一定会出现的状态图，理论上看这个图能解决任何TCP状态问题。</p>
<p><img src="/images/oss/b3d075782450b0c8d2615c5d2b75d923.png" alt="image.png"></p>
<p>反复看这个图的右下部分的CLOSE_WAIT ，从这个图里可以得到如下结论：</p>
<p><strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong></p>
<p>基本上这一结论要能帮助解决所有CLOSE_WAIT相关的问题，如果不能说明对这个知识点理解的不够。</p>
<h2 id="server端大量close-wait案例"><a href="#server端大量close-wait案例" class="headerlink" title="server端大量close_wait案例"></a>server端大量close_wait案例</h2><p>用实际案例来检查自己对CLOSE_WAIT 理论（<strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong>）的掌握 – 能不能用这个结论来解决实际问题。同时也可以看看自己从知识到问题的推理能力（跟前面的知识效率呼应一下）。</p>
<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><blockquote>
<p>服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
</blockquote>
<p>根据这个描述先不要往下看，自己推理分析下可能的原因。</p>
<p>我的推理如下：</p>
<p>从这里看起来，client跟server成功建立了somaxconn个连接（somaxconn小于backlog，所以accept queue只有这么大），但是应用没有accept这个连接，导致这些连接一直在accept queue中。但是这些连接的状态已经是ESTABLISHED了，也就是client可以发送数据了，数据发送到server后OS ack了，并放在os的tcp buffer中，应用一直没有accept也就没法读取数据。client于是发送fin（可能是超时、也可能是简单发送数据任务完成了得结束连接），这时Server上这个连接变成了CLOSE_WAIT .</p>
<p>也就是从开始到结束这些连接都在accept queue中，没有被应用accept，很快他们又因为client 发送 fin 包变成了CLOSE_WAIT ，所以始终看到的是服务端出现大量CLOSE_WAIT 并且个数正好等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）。</p>
<p>如下图所示，在连接进入accept queue后状态就是ESTABLISED了，也就是可以正常收发数据和fin了。client是感知不到server是否accept()了，只是发了数据后server的os代为保存在OS的TCP buffer中，因为应用没来取自然在CLOSE_WAIT 后应用也没有close()，所以一直维持CLOSE_WAIT 。</p>
<p>得检查server 应用为什么没有accept。</p>
<p><img src="/images/951413iMgBlog/20190706093602331.png" alt="Recv-Q和Send-Q"></p>
<p>如上是老司机的思路靠经验缺省了一些理论推理，缺省还是对理论理解不够， 这个分析抓住了 大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）但是没有抓住 CLOSE_WAIT 背后的核心原因</p>
<h3 id="更简单的推理"><a href="#更简单的推理" class="headerlink" title="更简单的推理"></a>更简单的推理</h3><p>如果没有任何实战经验，只看上面的状态图的学霸应该是这样推理的：</p>
<p>看到server上有大量的CLOSE_WAIT说明client主动断开了连接，server的OS收到client 发的fin，并回复了ack，这个过程不需要应用感知，进而连接从ESTABLISHED进入CLOSE_WAIT，此时在等待server上的应用调用close连关闭连接（处理完所有收发数据后才会调close()） —- 结论：server上的应用一直卡着没有调close().</p>
<p>同时这里很奇怪的现象： 服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn，进而可以猜测是不是连接建立后很快accept队列满了（应用也没有accept() ), 导致 大量CLOSE_WAIT 个数正好 等于somaxconn —- 结论： server 上的应用不但没有调close(), 连close() 前面必须调用 accept() 都一直卡着没调 （这个结论需要有accept()队列的理论知识)</p>
<p><strong>从上面两个结论可以清楚地看到 server的应用卡住了</strong></p>
<h3 id="实际结论："><a href="#实际结论：" class="headerlink" title="实际结论："></a>实际结论：</h3><blockquote>
<p>这个case的最终原因是因为<strong>OS的open files设置的是1024,达到了上限</strong>，进而导致server不能accept，但这个时候的tcp连接状态已经是ESTABLISHED了（这个状态变换是取决于内核收发包，跟应用是否accept()无关）。</p>
<p>同时从这里可以推断 netstat 即使看到一个tcp连接状态是ESTABLISHED也不能代表占用了 open files句柄。此时client可以正常发送数据了，只是应用服务在accept之前没法receive数据和close连接。</p>
</blockquote>
<p>这个结论的图解如下：</p>
<p><img src="/images/oss/bcf463efeb677d5749d8d7571274ee79.png" alt="image.png"></p>
<p>假如全连接队列满了，握手第三步后对于client端来说是无法感知的，client端只需要回复ack后这个连接对于client端就是ESTABLISHED了，这时client是可以发送数据的。但是Server会扔掉收到的ack，回复syn+ack给client。</p>
<p>如果全连接队列没满，但是fd不够，那么在Server端这个Socket也是ESTABLISHED，但是只是暂存在全连接队列中，等待应用来accept，这个时候client端同样无法感知这个连接没有被accept，client是可以发送数据的，这个数据会保存在tcp receive memory buffer中，等到accept后再给应用。</p>
<p>如果自己无法得到上面的分析，那么再来看看如果把 CLOSE_WAIT 状态更细化地分析下(类似有老师帮你把知识点揉开跟实际案例联系下—-未必是上面的案例)，看完后再来分析下上面的案例。</p>
<h2 id="CLOSE-WAIT-状态拆解"><a href="#CLOSE-WAIT-状态拆解" class="headerlink" title="CLOSE_WAIT 状态拆解"></a>CLOSE_WAIT 状态拆解</h2><p>通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：</p>
<ul>
<li><strong>程序问题</strong>：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。</li>
<li>响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。</li>
<li>BACKLOG 太大：此处的 backlog 不是 syn backlog，而是 accept 的 backlog，如果 backlog 太大的话，设想突然遭遇大访问量的话，即便响应速度不慢，也可能出现来不及消费的情况，导致多余的请求还在<a href="http://jaseywang.me/2014/07/20/tcp-queue-的一些问题/" target="_blank" rel="external">队列</a>里就被对方关闭了。</li>
</ul>
<p>如果你通过「netstat -ant」或者「ss -ant」命令发现了很多 CLOSE_WAIT 连接，请注意结果中的「Recv-Q」和「Local Address」字段，通常「Recv-Q」会不为空，它表示应用还没来得及接收数据，而「Local Address」表示哪个地址和端口有问题，我们可以通过「lsof -i:<port>」来确认端口对应运行的是什么程序以及它的进程号是多少。</port></p>
<p>如果是我们自己写的一些程序，比如用 HttpClient 自定义的蜘蛛，那么八九不离十是程序问题，如果是一些使用广泛的程序，比如 Tomcat 之类的，那么更可能是响应速度太慢或者 timeout 设置太小或者 BACKLOG 设置过大导致的故障。</p>
<p>看完这段 CLOSE_WAIT 更具体深入点的分析后再来分析上面的案例看看，能否推导得到正确的结论。</p>
<h2 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h2><h3 id="连接都没有被accept-client端就能发送数据了？"><a href="#连接都没有被accept-client端就能发送数据了？" class="headerlink" title="连接都没有被accept(), client端就能发送数据了？"></a>连接都没有被accept(), client端就能发送数据了？</h3><p>答：是的。只要这个连接在OS看来是ESTABLISHED的了就可以，因为握手、接收数据都是由内核完成的，内核收到数据后会先将数据放在内核的tcp buffer中，然后os回复ack。另外三次握手之后client端是没法知道server端是否accept()了。</p>
<h3 id="CLOSE-WAIT与accept-queue有关系吗？"><a href="#CLOSE-WAIT与accept-queue有关系吗？" class="headerlink" title="CLOSE_WAIT与accept queue有关系吗？"></a>CLOSE_WAIT与accept queue有关系吗？</h3><p>答：没有关系。只是本案例中因为open files不够了，影响了应用accept(), 导致accept queue满了，同时因为即使应用不accept（三次握手后，server端是否accept client端无法感知），client也能发送数据和发 fin断连接，这些响应都是os来负责，跟上层应用没关系，连接从握手到ESTABLISHED再到CLOSE_WAIT都不需要fd，也不需要应用参与。CLOSE_WAIT只跟应用不调 close() 有关系。 </p>
<h3 id="CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？"><a href="#CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？" class="headerlink" title="CLOSE_WAIT与accept queue为什么刚好一致并且联动了？"></a>CLOSE_WAIT与accept queue为什么刚好一致并且联动了？</h3><p>答：这里他们的数量刚好一致是因为所有新建连接都没有accept，堵在queue中。同时client发现问题后把所有连接都fin了，也就是所有queue中的连接从来没有被accept过，但是他们都是ESTABLISHED，过一阵子之后client端发了fin所以所有accept queue中的连接又变成了 CLOSE_WAIT, 所以二者刚好一致并且联动了</p>
<h3 id="openfiles和accept-的关系是？"><a href="#openfiles和accept-的关系是？" class="headerlink" title="openfiles和accept()的关系是？"></a>openfiles和accept()的关系是？</h3><p>答：accept()的时候才会创建文件句柄，消耗openfiles</p>
<h3 id="一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"><a href="#一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？" class="headerlink" title="一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"></a>一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？</h3><p>答：是</p>
<h3 id="如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？"><a href="#如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？" class="headerlink" title="如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？"></a>如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？</h3><p>答：对</p>
<h3 id="如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？"><a href="#如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？" class="headerlink" title="如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？"></a>如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？</h3><p>答： 能，这个时候在client和server两边看到的连接状态都是 ESTABLISHED，只是Server上的全连接队列占用加1。连接握手并切换到ESTABLISHED状态都是由OS来负责的，应用不参与，ESTABLISHED后应用才能accept，进而收发数据。也就是能放入到全连接队列里面的连接肯定都是 ESTABLISHED 状态的了</p>
<h3 id="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"><a href="#接着上面的问题，如果新连接继续连接进而全连接队列满了呢？" class="headerlink" title="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"></a>接着上面的问题，如果新连接继续连接进而全连接队列满了呢？</h3><p>答：那就连不上了，server端的OS因为全连接队列满了直接扔掉第一个syn握手包，这个时候连接在client端是SYN_SENT，Server端没有这个连接，这是因为syn到server端就直接被OS drop 了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">//如下图，本机测试，只有一个client端发起的syn_send, 3306的server端没有任何连接</div><div class="line">$netstat -antp  |grep -i 127.0.0.1:3306</div><div class="line">tcp     0   1 127.0.0.1:61106      127.0.0.1:3306    SYN_SENT    21352/telnet</div></pre></td></tr></table></figure>
<p>能进入到accept queue中的连接都是 ESTABLISHED，不管用户态有没有accept，用户态accept后队列大小减1</p>
<h3 id="如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？"><a href="#如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？" class="headerlink" title="如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？"></a>如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？</h3><p>答： 如果此时收到对方的RESET了，那么OS会释放这个连接。但是内核认为所有 listen 到的连接, 必须要 accept 走, 因为用户有权利知道有过这么一个连接存在过。所以OS不会到全连接队列拿掉这个连接，全连接队列数量也不会减1，直到应用accept这个连接，然后read/write才发现这个连接断开了，报communication failure异常</p>
<h3 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h3><p>三次握手成功就变成 ESTABLISHED 了，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>这是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</div><div class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </div><div class="line">LISTEN      73     1024            *:terabase                 *:*</div></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;  </div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</div><div class="line"># ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </div><div class="line">LISTEN      1025   1024             *:terabase              *:*</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="twitter @plantegg" />
          <p class="site-author-name" itemprop="name">twitter @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">147</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">244</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
